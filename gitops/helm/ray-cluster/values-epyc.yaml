# EPYC-optimized values for ray-cluster
# AMD EPYC 7702 specific optimizations

global:
  xorb:
    environment: "production"
    version: "2.0.0"

# Ray Head node - EPYC optimized
rayHead:
  replicaCount: 1
  
  resources:
    requests:
      cpu: "4"
      memory: "16Gi"
    limits:
      cpu: "16"
      memory: "32Gi"
  
  # EPYC-specific node selection
  nodeSelector:
    kubernetes.io/arch: amd64
    node.kubernetes.io/cpu-family: "EPYC"
    xorb.ai/optimized-for: epyc-7702
  
  # CPU affinity for NUMA optimization
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node.kubernetes.io/cpu-family
            operator: In
            values: ["EPYC"]
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/component
              operator: In
              values: ["ray-head"]
          topologyKey: kubernetes.io/hostname
  
  # EPYC-optimized environment variables
  env:
    - name: RAY_DISABLE_IMPORT_WARNING
      value: "1"
    - name: RAY_scheduler_placement_group_strict_packing
      value: "0"
    - name: PYTHONPATH
      value: "/workspace"
    # EPYC CPU optimizations
    - name: RAY_CPU_COUNT
      value: "16"
    - name: RAY_MEMORY_LIMIT
      value: "32GB"
    - name: OMP_NUM_THREADS
      value: "8"  # Half of CPU count for hyperthreading
    - name: OPENBLAS_NUM_THREADS
      value: "8"
    - name: MKL_NUM_THREADS
      value: "8"
    # NUMA optimizations
    - name: NUMA_MEMBIND
      value: "0,1"
    - name: OMP_PROC_BIND
      value: "close"
    - name: OMP_PLACES
      value: "cores"
    # Ray specific optimizations
    - name: RAY_max_io_workers
      value: "4"
    - name: RAY_object_store_memory
      value: "8000000000"  # 8GB object store
  
  rayStartParams:
    dashboard-host: "0.0.0.0"
    dashboard-port: "8265"
    port: "6379"
    redis-password: ""
    block: true
    # EPYC-specific parameters
    num-cpus: "16"
    memory: "32000000000"  # 32GB in bytes
    object-store-memory: "8000000000"  # 8GB object store

# Ray Workers - EPYC optimized
rayWorkers:
  replicaCount: 4  # More workers for EPYC capacity
  
  resources:
    requests:
      cpu: "8"
      memory: "32Gi"
    limits:
      cpu: "32"
      memory: "64Gi"
  
  # EPYC-specific node selection
  nodeSelector:
    kubernetes.io/arch: amd64
    node.kubernetes.io/cpu-family: "EPYC"
    xorb.ai/optimized-for: epyc-7702
  
  # CPU affinity for NUMA optimization
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node.kubernetes.io/cpu-family
            operator: In
            values: ["EPYC"]
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/component
              operator: In
              values: ["ray-worker"]
          topologyKey: kubernetes.io/hostname
  
  # EPYC-optimized environment variables
  env:
    - name: RAY_DISABLE_IMPORT_WARNING
      value: "1"
    - name: PYTHONPATH
      value: "/workspace"
    # EPYC CPU optimizations
    - name: RAY_CPU_COUNT
      value: "32"
    - name: RAY_MEMORY_LIMIT
      value: "64GB"
    - name: OMP_NUM_THREADS
      value: "16"  # Half of CPU count for hyperthreading
    - name: OPENBLAS_NUM_THREADS
      value: "16"
    - name: MKL_NUM_THREADS
      value: "16"
    # NUMA optimizations
    - name: NUMA_MEMBIND
      value: "0,1"
    - name: OMP_PROC_BIND
      value: "close"
    - name: OMP_PLACES
      value: "cores"
    # PyTorch optimizations for EPYC
    - name: TORCH_NUM_THREADS
      value: "16"
    - name: TORCH_CUDA_ARCH_LIST
      value: ""  # CPU-only
    # Ray specific optimizations
    - name: RAY_max_io_workers
      value: "8"
    - name: RAY_object_store_memory
      value: "16000000000"  # 16GB object store
  
  rayStartParams:
    block: true
    # EPYC-specific parameters
    num-cpus: "32"
    memory: "64000000000"  # 64GB in bytes
    object-store-memory: "16000000000"  # 16GB object store
  
  # Aggressive autoscaling for EPYC capacity
  autoscaler:
    enabled: true
    minReplicas: 2
    maxReplicas: 12  # Scale up to utilize EPYC capacity
    targetCPUUtilizationPercentage: 60  # Lower threshold for better performance
    targetMemoryUtilizationPercentage: 70
    scaleDownDelaySecondsAfterAdd: 180  # Faster scaling
    scaleDownDelaySecondsAfterDelete: 30
    
    # Custom metrics for ML workloads
    behavior:
      scaleUp:
        stabilizationWindowSeconds: 60
        policies:
        - type: Percent
          value: 100  # Double capacity quickly
          periodSeconds: 60
        - type: Pods
          value: 2
          periodSeconds: 60
      scaleDown:
        stabilizationWindowSeconds: 300
        policies:
        - type: Percent
          value: 50
          periodSeconds: 60

# Custom configuration for EPYC
config:
  ray.yaml: |
    ray:
      runtime_env:
        working_dir: "/workspace"
        pip:
          - ray[default]==2.8.0
          - torch>=2.0.0+cpu
          - stable-baselines3>=2.2.0
          - optuna>=3.4.0
          - mlflow>=2.8.0
          - scikit-learn>=1.3.0
          - numpy>=1.24.0
          - vowpal-wabbit>=9.8.0
        env_vars:
          OMP_NUM_THREADS: "16"
          MKL_NUM_THREADS: "16"
          OPENBLAS_NUM_THREADS: "16"
          TORCH_NUM_THREADS: "16"
      
      # EPYC-specific Ray configuration
      cluster:
        head:
          resources:
            CPU: 16
            memory: 32000000000
            object_store_memory: 8000000000
        worker:
          resources:
            CPU: 32
            memory: 64000000000
            object_store_memory: 16000000000
      
      # Training configuration optimized for EPYC
      tune:
        cluster:
          resources:
            CPU: 32
            memory: 64000000000
        
        # Optimize for EPYC's high core count
        trainable:
          num_workers: 16
          num_cpus_per_worker: 2
          num_gpus_per_worker: 0

# Enhanced monitoring for production
serviceMonitor:
  enabled: true
  interval: "15s"  # More frequent monitoring
  scrapeTimeout: "15s"
  labels:
    xorb.ai/monitoring: "enabled"
    xorb.ai/environment: "production"

# Production-grade probes with EPYC considerations
probes:
  startup:
    enabled: true
    initialDelaySeconds: 60  # Longer startup for EPYC optimization
    periodSeconds: 10
    timeoutSeconds: 10
    failureThreshold: 20
    successThreshold: 1
  
  liveness:
    enabled: true
    initialDelaySeconds: 120
    periodSeconds: 30
    timeoutSeconds: 15
    failureThreshold: 3
    successThreshold: 1
  
  readiness:
    enabled: true
    initialDelaySeconds: 60
    periodSeconds: 15
    timeoutSeconds: 10
    failureThreshold: 3
    successThreshold: 1

# Production labels for EPYC deployment
commonLabels:
  app.kubernetes.io/name: ray-cluster
  app.kubernetes.io/component: learning-infrastructure
  app.kubernetes.io/part-of: xorb
  xorb.ai/layer: learning
  xorb.ai/optimized-for: epyc-7702
  xorb.ai/environment: production

commonAnnotations:
  xorb.ai/managed-by: helm
  xorb.ai/version: "2.0.0"
  xorb.ai/cpu-family: "EPYC"
  xorb.ai/optimization-profile: "epyc-7702"