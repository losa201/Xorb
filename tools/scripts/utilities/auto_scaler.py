#!/usr/bin/env python3
"""
XORB Automated Scaling and Load Balancing System
Enterprise-grade auto-scaling with intelligent load distribution
"""

import asyncio
import json
import time
from datetime import datetime
from typing import Dict, List, Optional

import aiohttp
import docker
from fastapi import FastAPI, BackgroundTasks
import uvicorn

app = FastAPI(
    title="XORB Auto Scaler",
    description="Automated Scaling and Load Balancing for XORB Platform",
    version="1.0.0"
)

class XORBAutoScaler:
    """Intelligent auto-scaling system for XORB services"""

    def __init__(self):
        self.docker_client = docker.from_env()
        self.services = {
            "neural_orchestrator": {
                "image": "xorb-neural-orchestrator",
                "base_port": 8003,
                "min_instances": 1,
                "max_instances": 5,
                "current_instances": 1,
                "target_cpu": 70,
                "target_response_time": 5000  # ms
            },
            "learning_service": {
                "image": "xorb-learning-service",
                "base_port": 8004,
                "min_instances": 1,
                "max_instances": 3,
                "current_instances": 1,
                "target_cpu": 80,
                "target_response_time": 5000
            },
            "threat_detection": {
                "image": "xorb-threat-detection",
                "base_port": 8005,
                "min_instances": 1,
                "max_instances": 4,
                "current_instances": 1,
                "target_cpu": 75,
                "target_response_time": 3000
            }
        }
        self.metrics_history = {}
        self.scaling_decisions = []

    async def collect_service_metrics(self, service_name: str, port: int) -> Dict:
        """Collect performance metrics from a service"""
        try:
            async with aiohttp.ClientSession() as session:
                # Health check with timing
                start_time = time.time()
                async with session.get(f"http://localhost:{port}/health", timeout=aiohttp.ClientTimeout(total=5)) as response:
                    response_time = (time.time() - start_time) * 1000

                    if response.status == 200:
                        health_data = await response.json()

                        # Get container stats
                        try:
                            container = self.docker_client.containers.get(f"xorb_{service_name.replace('_', '-')}")
                            stats = container.stats(stream=False)

                            # Calculate CPU usage
                            cpu_usage = self.calculate_cpu_usage(stats)

                            # Memory usage
                            memory_usage = stats['memory_stats']['usage'] / stats['memory_stats']['limit'] * 100

                            return {
                                "service": service_name,
                                "status": "healthy",
                                "response_time_ms": response_time,
                                "cpu_usage_percent": cpu_usage,
                                "memory_usage_percent": memory_usage,
                                "timestamp": datetime.now().isoformat()
                            }
                        except Exception as e:
                            # Fallback if container stats not available
                            return {
                                "service": service_name,
                                "status": "healthy",
                                "response_time_ms": response_time,
                                "cpu_usage_percent": 50,  # Estimated
                                "memory_usage_percent": 30,  # Estimated
                                "timestamp": datetime.now().isoformat(),
                                "note": f"Container stats unavailable: {str(e)}"
                            }
                    else:
                        return {
                            "service": service_name,
                            "status": "unhealthy",
                            "response_time_ms": 0,
                            "timestamp": datetime.now().isoformat()
                        }

        except Exception as e:
            return {
                "service": service_name,
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def calculate_cpu_usage(self, stats: Dict) -> float:
        """Calculate CPU usage percentage from Docker stats"""
        try:
            cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - stats['precpu_stats']['cpu_usage']['total_usage']
            system_delta = stats['cpu_stats']['system_cpu_usage'] - stats['precpu_stats']['system_cpu_usage']

            if system_delta > 0:
                cpu_usage = (cpu_delta / system_delta) * len(stats['cpu_stats']['cpu_usage']['percpu_usage']) * 100
                return min(cpu_usage, 100)  # Cap at 100%

            return 0
        except (KeyError, ZeroDivisionError):
            return 0

    async def make_scaling_decision(self, service_name: str, metrics: Dict) -> str:
        """Make intelligent scaling decision based on metrics"""
        service_config = self.services[service_name]
        current_instances = service_config["current_instances"]

        # Get recent metrics history
        if service_name not in self.metrics_history:
            self.metrics_history[service_name] = []

        self.metrics_history[service_name].append(metrics)

        # Keep only last 10 metrics points
        if len(self.metrics_history[service_name]) > 10:
            self.metrics_history[service_name] = self.metrics_history[service_name][-10:]

        # Need at least 3 data points for scaling decisions
        if len(self.metrics_history[service_name]) < 3:
            return "no_action"

        recent_metrics = self.metrics_history[service_name][-3:]

        # Calculate averages
        avg_cpu = sum(m.get("cpu_usage_percent", 0) for m in recent_metrics) / len(recent_metrics)
        avg_response_time = sum(m.get("response_time_ms", 0) for m in recent_metrics) / len(recent_metrics)

        # Scaling up conditions
        scale_up_conditions = [
            avg_cpu > service_config["target_cpu"],
            avg_response_time > service_config["target_response_time"],
            current_instances < service_config["max_instances"]
        ]

        # Scaling down conditions
        scale_down_conditions = [
            avg_cpu < service_config["target_cpu"] * 0.3,  # 30% of target
            avg_response_time < service_config["target_response_time"] * 0.5,  # 50% of target
            current_instances > service_config["min_instances"]
        ]

        if all(scale_up_conditions):
            decision = {
                "action": "scale_up",
                "service": service_name,
                "current_instances": current_instances,
                "target_instances": current_instances + 1,
                "reason": f"High load: CPU={avg_cpu:.1f}%, Response={avg_response_time:.1f}ms",
                "timestamp": datetime.now().isoformat()
            }
            self.scaling_decisions.append(decision)
            return "scale_up"

        elif all(scale_down_conditions):
            decision = {
                "action": "scale_down",
                "service": service_name,
                "current_instances": current_instances,
                "target_instances": current_instances - 1,
                "reason": f"Low load: CPU={avg_cpu:.1f}%, Response={avg_response_time:.1f}ms",
                "timestamp": datetime.now().isoformat()
            }
            self.scaling_decisions.append(decision)
            return "scale_down"

        return "no_action"

    async def scale_service(self, service_name: str, action: str) -> bool:
        """Execute scaling action"""
        try:
            service_config = self.services[service_name]

            if action == "scale_up":
                new_instance = service_config["current_instances"] + 1
                new_port = service_config["base_port"] + new_instance - 1

                # In a real implementation, this would start a new container
                print(f"üîÑ Scaling UP {service_name}: Starting instance {new_instance} on port {new_port}")

                # Simulate container creation (would be actual Docker container start)
                service_config["current_instances"] += 1

                return True

            elif action == "scale_down":
                if service_config["current_instances"] > service_config["min_instances"]:
                    instance_to_remove = service_config["current_instances"]

                    print(f"üîÑ Scaling DOWN {service_name}: Stopping instance {instance_to_remove}")

                    # Simulate container removal
                    service_config["current_instances"] -= 1

                    return True

            return False

        except Exception as e:
            print(f"‚ùå Scaling error for {service_name}: {e}")
            return False

    async def monitoring_loop(self):
        """Main monitoring and scaling loop"""
        print("üöÄ Starting XORB Auto-Scaling Monitoring Loop")

        while True:
            try:
                for service_name, config in self.services.items():
                    # Collect metrics from all instances
                    for instance in range(1, config["current_instances"] + 1):
                        port = config["base_port"] + instance - 1
                        metrics = await self.collect_service_metrics(service_name, port)

                        # Make scaling decision
                        decision = await self.make_scaling_decision(service_name, metrics)

                        # Execute scaling if needed
                        if decision in ["scale_up", "scale_down"]:
                            success = await self.scale_service(service_name, decision)
                            if success:
                                print(f"‚úÖ Successfully executed {decision} for {service_name}")
                            else:
                                print(f"‚ùå Failed to execute {decision} for {service_name}")

                # Wait before next monitoring cycle
                await asyncio.sleep(30)  # Monitor every 30 seconds

            except Exception as e:
                print(f"‚ùå Monitoring loop error: {e}")
                await asyncio.sleep(10)

    def get_scaling_status(self) -> Dict:
        """Get current scaling status"""
        status = {
            "timestamp": datetime.now().isoformat(),
            "services": {},
            "total_instances": 0,
            "recent_decisions": self.scaling_decisions[-10:] if self.scaling_decisions else []
        }

        for service_name, config in self.services.items():
            status["services"][service_name] = {
                "current_instances": config["current_instances"],
                "min_instances": config["min_instances"],
                "max_instances": config["max_instances"],
                "target_cpu": config["target_cpu"],
                "target_response_time": config["target_response_time"],
                "recent_metrics": self.metrics_history.get(service_name, [])[-3:]
            }
            status["total_instances"] += config["current_instances"]

        return status

# Initialize auto scaler
auto_scaler = XORBAutoScaler()

@app.on_event("startup")
async def startup_event():
    """Start monitoring loop on startup"""
    # Start monitoring in background
    asyncio.create_task(auto_scaler.monitoring_loop())

@app.get("/scaling/status")
async def get_scaling_status():
    """Get current auto-scaling status"""
    return auto_scaler.get_scaling_status()

@app.get("/scaling/metrics/{service_name}")
async def get_service_metrics(service_name: str):
    """Get metrics for specific service"""
    if service_name not in auto_scaler.services:
        return {"error": f"Service {service_name} not found"}

    config = auto_scaler.services[service_name]
    port = config["base_port"]

    metrics = await auto_scaler.collect_service_metrics(service_name, port)
    return metrics

@app.post("/scaling/manual/{service_name}/{action}")
async def manual_scaling(service_name: str, action: str):
    """Manual scaling trigger"""
    if service_name not in auto_scaler.services:
        return {"error": f"Service {service_name} not found"}

    if action not in ["scale_up", "scale_down"]:
        return {"error": "Action must be 'scale_up' or 'scale_down'"}

    success = await auto_scaler.scale_service(service_name, action)

    return {
        "service": service_name,
        "action": action,
        "success": success,
        "new_instance_count": auto_scaler.services[service_name]["current_instances"]
    }

@app.get("/scaling/decisions")
async def get_scaling_decisions():
    """Get recent scaling decisions"""
    return {
        "total_decisions": len(auto_scaler.scaling_decisions),
        "recent_decisions": auto_scaler.scaling_decisions[-20:] if auto_scaler.scaling_decisions else []
    }

@app.get("/health")
async def health_check():
    """Auto scaler health check"""
    return {
        "status": "healthy",
        "service": "xorb_auto_scaler",
        "version": "1.0.0",
        "features": [
            "Automatic Scaling",
            "Load Monitoring",
            "Performance Metrics",
            "Manual Scaling",
            "Decision Tracking"
        ],
        "monitoring_active": True
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=9001)
