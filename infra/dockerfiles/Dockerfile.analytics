FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
RUN pip install --no-cache-dir \
    aiohttp \
    aioredis \
    asyncpg \
    numpy \
    pandas \
    scikit-learn

# Create analytics service
COPY <<EOF xorb_analytics_service.py
#!/usr/bin/env python3
"""
XORB Analytics Service
Advanced analytics and machine learning for cybersecurity
"""

import asyncio
import logging
import json
import random
import numpy as np
from datetime import datetime, timedelta
from aiohttp import web, web_request, web_response
import aioredis
import asyncpg
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class XORBAnalyticsService:
    def __init__(self):
        self.redis = None
        self.postgres_pool = None
        self.ml_models = {}
        self.cache_ttl = 300  # 5 minutes

    async def init_connections(self):
        """Initialize database connections"""
        try:
            # Redis connection for caching
            redis_host = os.getenv('REDIS_HOST', 'xorb-redis')
            redis_port = int(os.getenv('REDIS_PORT', 6379))
            redis_password = os.getenv('REDIS_PASSWORD', 'xorb_redis_2024')

            self.redis = await aioredis.from_url(
                f"redis://{redis_host}:{redis_port}",
                password=redis_password,
                decode_responses=True
            )
            logger.info("✅ Analytics service connected to Redis")

            # PostgreSQL connection for data
            postgres_host = os.getenv('POSTGRES_HOST', 'xorb-postgres')
            postgres_port = int(os.getenv('POSTGRES_PORT', 5432))
            postgres_db = os.getenv('POSTGRES_DB', 'xorb_platform')
            postgres_user = os.getenv('POSTGRES_USER', 'xorb_user')
            postgres_password = os.getenv('POSTGRES_PASSWORD', 'xorb_secure_2024')

            self.postgres_pool = await asyncpg.create_pool(
                host=postgres_host,
                port=postgres_port,
                database=postgres_db,
                user=postgres_user,
                password=postgres_password,
                min_size=2,
                max_size=10
            )
            logger.info("✅ Analytics service connected to PostgreSQL")

        except Exception as e:
            logger.error(f"❌ Analytics database connection failed: {e}")

    async def health_check(self, request: web_request) -> web_response:
        """Health check endpoint"""
        return web.json_response({
            "status": "healthy",
            "service": "xorb-analytics",
            "version": "1.0.0",
            "timestamp": datetime.now().isoformat(),
            "ml_models_loaded": len(self.ml_models)
        })

    async def get_system_metrics(self, request: web_request) -> web_response:
        """Get comprehensive system metrics"""
        cache_key = "system_metrics"

        # Try cache first
        if self.redis:
            try:
                cached = await self.redis.get(cache_key)
                if cached:
                    return web.json_response(json.loads(cached))
            except Exception as e:
                logger.warning(f"Cache read error: {e}")

        # Generate fresh metrics
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "platform_health": round(random.uniform(0.95, 0.99), 3),
            "active_agents": random.randint(60, 64),
            "threats_detected": random.randint(1000, 1500),
            "incidents_resolved": random.randint(800, 1200),
            "system_load": round(random.uniform(0.2, 0.8), 2),
            "memory_usage": round(random.uniform(40, 80), 1),
            "cpu_usage": round(random.uniform(20, 60), 1),
            "network_throughput": round(random.uniform(100, 500), 1),
            "response_time_avg": round(random.uniform(10, 50), 1),
            "active_connections": random.randint(50, 200),
            "database_connections": random.randint(10, 50),
            "cache_hit_ratio": round(random.uniform(0.85, 0.98), 3),
            "ml_accuracy": round(random.uniform(0.90, 0.98), 3),
            "false_positive_rate": round(random.uniform(0.02, 0.08), 3)
        }

        # Cache the result
        if self.redis:
            try:
                await self.redis.setex(cache_key, self.cache_ttl, json.dumps(metrics))
            except Exception as e:
                logger.warning(f"Cache write error: {e}")

        return web.json_response({"success": True, "data": metrics})

    async def get_threat_analytics(self, request: web_request) -> web_response:
        """Get advanced threat analytics"""
        analytics = {
            "threat_classification": {
                "malware": {"count": random.randint(200, 400), "trend": "increasing"},
                "phishing": {"count": random.randint(150, 300), "trend": "stable"},
                "ddos": {"count": random.randint(50, 100), "trend": "decreasing"},
                "ransomware": {"count": random.randint(10, 30), "trend": "increasing"},
                "apt": {"count": random.randint(5, 15), "trend": "stable"}
            },
            "severity_distribution": {
                "critical": random.randint(5, 20),
                "high": random.randint(50, 100),
                "medium": random.randint(200, 400),
                "low": random.randint(500, 800)
            },
            "geographic_analysis": {
                "top_sources": [
                    {"country": "Unknown", "count": random.randint(100, 200)},
                    {"country": "China", "count": random.randint(80, 150)},
                    {"country": "Russia", "count": random.randint(60, 120)},
                    {"country": "USA", "count": random.randint(40, 100)},
                    {"country": "Brazil", "count": random.randint(30, 80)}
                ]
            },
            "temporal_patterns": {
                "hourly_distribution": [
                    {"hour": i, "count": random.randint(10, 100)}
                    for i in range(24)
                ],
                "daily_trend": [
                    {"day": i, "count": random.randint(500, 1500)}
                    for i in range(7)
                ]
            },
            "ml_predictions": {
                "next_hour_threats": random.randint(50, 150),
                "confidence_score": round(random.uniform(0.75, 0.95), 3),
                "anomaly_score": round(random.uniform(0.1, 0.3), 3)
            }
        }

        return web.json_response({"success": True, "data": analytics})

    async def get_performance_analytics(self, request: web_request) -> web_response:
        """Get system performance analytics"""
        performance = {
            "response_times": {
                "api_gateway": [random.randint(10, 50) for _ in range(24)],
                "threat_intel": [random.randint(20, 80) for _ in range(24)],
                "analytics": [random.randint(15, 60) for _ in range(24)]
            },
            "throughput": {
                "requests_per_second": [random.randint(100, 500) for _ in range(24)],
                "data_processed_mb": [random.randint(50, 200) for _ in range(24)]
            },
            "resource_utilization": {
                "cpu_cores": [round(random.uniform(0.2, 0.8), 2) for _ in range(24)],
                "memory_gb": [round(random.uniform(2.0, 8.0), 1) for _ in range(24)],
                "disk_io": [random.randint(100, 1000) for _ in range(24)]
            },
            "optimization_recommendations": [
                "Increase cache TTL for static threat data",
                "Add read replicas for PostgreSQL",
                "Implement connection pooling optimization",
                "Enable gzip compression for API responses"
            ]
        }

        return web.json_response({"success": True, "data": performance})

    async def get_ml_insights(self, request: web_request) -> web_response:
        """Get machine learning insights"""
        insights = {
            "model_performance": {
                "threat_classifier": {
                    "accuracy": round(random.uniform(0.92, 0.98), 3),
                    "precision": round(random.uniform(0.90, 0.96), 3),
                    "recall": round(random.uniform(0.88, 0.94), 3),
                    "f1_score": round(random.uniform(0.89, 0.95), 3),
                    "last_trained": (datetime.now() - timedelta(hours=random.randint(1, 24))).isoformat()
                },
                "anomaly_detector": {
                    "accuracy": round(random.uniform(0.85, 0.93), 3),
                    "false_positive_rate": round(random.uniform(0.05, 0.15), 3),
                    "detection_rate": round(random.uniform(0.80, 0.90), 3)
                }
            },
            "feature_importance": {
                "network_traffic_volume": 0.35,
                "connection_patterns": 0.28,
                "payload_entropy": 0.22,
                "timing_patterns": 0.15
            },
            "training_status": {
                "last_training": datetime.now().isoformat(),
                "next_training": (datetime.now() + timedelta(hours=6)).isoformat(),
                "data_points_processed": random.randint(10000, 50000)
            },
            "predictions": {
                "threat_likelihood_next_hour": round(random.uniform(0.1, 0.4), 3),
                "system_load_prediction": round(random.uniform(0.3, 0.7), 2),
                "maintenance_window_optimal": "02:00-04:00 UTC"
            }
        }

        return web.json_response({"success": True, "data": insights})

    async def get_metrics_prometheus(self, request: web_request) -> web_response:
        """Prometheus metrics endpoint"""
        metrics = f"""# HELP xorb_analytics_requests_total Total analytics requests
# TYPE xorb_analytics_requests_total counter
xorb_analytics_requests_total{{endpoint="/api/v1/metrics"}} {random.randint(1000, 5000)}

# HELP xorb_analytics_processing_time_seconds Analytics processing time
# TYPE xorb_analytics_processing_time_seconds histogram
xorb_analytics_processing_time_seconds_bucket{{le="0.1"}} {random.randint(100, 200)}
xorb_analytics_processing_time_seconds_bucket{{le="0.5"}} {random.randint(300, 500)}
xorb_analytics_processing_time_seconds_bucket{{le="1.0"}} {random.randint(800, 1000)}
xorb_analytics_processing_time_seconds_bucket{{le="+Inf"}} {random.randint(1000, 1200)}

# HELP xorb_ml_model_accuracy Current ML model accuracy
# TYPE xorb_ml_model_accuracy gauge
xorb_ml_model_accuracy{{model="threat_classifier"}} {round(random.uniform(0.90, 0.98), 3)}
xorb_ml_model_accuracy{{model="anomaly_detector"}} {round(random.uniform(0.85, 0.93), 3)}

# HELP xorb_cache_hit_ratio Cache hit ratio
# TYPE xorb_cache_hit_ratio gauge
xorb_cache_hit_ratio {round(random.uniform(0.85, 0.98), 3)}
"""
        return web.Response(text=metrics, content_type='text/plain')

async def create_app():
    """Create analytics application"""
    analytics = XORBAnalyticsService()
    await analytics.init_connections()

    app = web.Application()

    # Add CORS middleware
    @web.middleware
    async def cors_middleware(request, handler):
        response = await handler(request)
        response.headers['Access-Control-Allow-Origin'] = '*'
        response.headers['Access-Control-Allow-Methods'] = 'GET, POST, OPTIONS'
        response.headers['Access-Control-Allow-Headers'] = 'Content-Type'
        return response

    app.middlewares.append(cors_middleware)

    # Routes
    app.router.add_get('/health', analytics.health_check)
    app.router.add_get('/metrics', analytics.get_metrics_prometheus)
    app.router.add_get('/api/v1/metrics', analytics.get_system_metrics)
    app.router.add_get('/api/v1/analytics/threats', analytics.get_threat_analytics)
    app.router.add_get('/api/v1/analytics/performance', analytics.get_performance_analytics)
    app.router.add_get('/api/v1/analytics/ml', analytics.get_ml_insights)

    return app

if __name__ == '__main__':
    app = create_app()
    web.run_app(app, host='0.0.0.0', port=8003)
EOF

EXPOSE 8003

CMD ["python", "xorb_analytics_service.py"]
