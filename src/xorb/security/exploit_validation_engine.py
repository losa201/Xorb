#!/usr/bin/env python3
"""
Exploit Validation Engine
Comprehensive validation and safety controls for exploit execution

This module provides enterprise-grade exploit validation including:
- Multi-layered safety validation
- Real-time threat assessment
- Controlled environment enforcement
- Compliance validation
- Automated safety monitoring
- Emergency intervention capabilities
"""

import asyncio
import logging
import json
import uuid
from typing import Dict, List, Optional, Any, Set, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
import hashlib
import re
import structlog

from ..common.security_framework import SecurityFramework, SecurityLevel, ThreatCategory
from ..common.audit_logger import AuditLogger, AuditEvent, EventSeverity

logger = structlog.get_logger(__name__)


class ValidationLevel(Enum):
    """Validation strictness levels"""
    STRICT = "strict"        # Maximum validation, simulation only
    MODERATE = "moderate"    # Controlled validation with safety checks
    PERMISSIVE = "permissive"  # Minimal validation for authorized environments
    DISABLED = "disabled"    # No validation (testing only)


class ThreatAssessment(Enum):
    """Threat level assessment"""
    BENIGN = "benign"
    LOW_RISK = "low_risk"
    MEDIUM_RISK = "medium_risk"
    HIGH_RISK = "high_risk"
    CRITICAL_RISK = "critical_risk"


class ValidationResult(Enum):
    """Validation decision results"""
    APPROVED = "approved"
    REJECTED = "rejected"
    CONDITIONAL = "conditional"
    REQUIRES_REVIEW = "requires_review"


@dataclass
class ExploitSignature:
    """Exploit signature for detection and validation"""
    signature_id: str
    name: str
    description: str
    exploit_patterns: List[str]
    payload_patterns: List[str]
    behavior_indicators: List[str]
    severity: ThreatAssessment
    mitre_techniques: List[str]
    created_at: datetime
    updated_at: datetime


@dataclass
class SafetyConstraint:
    """Safety constraint definition"""
    constraint_id: str
    name: str
    description: str
    constraint_type: str  # environment, target, technique, payload
    validation_rules: List[Dict[str, Any]]
    enforcement_level: ValidationLevel
    exemptions: List[str]
    created_by: str
    created_at: datetime


@dataclass
class ValidationRequest:
    """Comprehensive validation request"""
    request_id: str
    operation_id: str
    user_id: str
    exploit_data: Dict[str, Any]
    target_environment: Dict[str, Any]
    technique_details: Dict[str, Any]
    payload_info: Dict[str, Any]
    safety_constraints: List[str]
    compliance_requirements: List[str]
    validation_level: ValidationLevel
    timestamp: datetime


@dataclass
class ValidationResponse:
    """Detailed validation response"""
    request_id: str
    result: ValidationResult
    threat_assessment: ThreatAssessment
    risk_score: float  # 0.0 - 1.0
    safety_violations: List[str]
    compliance_issues: List[str]
    recommendations: List[str]
    conditions: List[str]
    approved_by: Optional[str] = None
    valid_until: Optional[datetime] = None
    monitoring_required: bool = True
    emergency_stop_enabled: bool = True
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class SignatureDatabase:
    """Database of exploit signatures for validation"""
    
    def __init__(self):
        self.signatures: Dict[str, ExploitSignature] = {}
        self._initialize_signatures()
    
    def _initialize_signatures(self):
        """Initialize database with known exploit signatures"""
        signatures = [
            ExploitSignature(
                signature_id="SIG_001",
                name="Buffer Overflow Exploit",
                description="Classic buffer overflow exploitation patterns",
                exploit_patterns=[
                    r"A{100,}",  # Long strings of A's
                    r"\x41{50,}",  # Hex pattern of A's
                    r"BBBB",  # Return address overwrite
                    r"\x42{4,}"  # Hex B's
                ],
                payload_patterns=[
                    r"\x90{10,}",  # NOP sled
                    r"\xcc+",  # INT3 instructions
                    r"\x48\x31\xc0"  # x64 shellcode patterns
                ],
                behavior_indicators=[
                    "stack_corruption",
                    "return_address_overwrite",
                    "shellcode_execution"
                ],
                severity=ThreatAssessment.HIGH_RISK,
                mitre_techniques=["T1068"],
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            ),
            
            ExploitSignature(
                signature_id="SIG_002", 
                name="SQL Injection Attack",
                description="SQL injection exploitation patterns",
                exploit_patterns=[
                    r"'.*OR.*1=1",
                    r"UNION.*SELECT",
                    r"DROP.*TABLE",
                    r"';.*--",
                    r"'.*WAITFOR.*DELAY"
                ],
                payload_patterns=[
                    r"xp_cmdshell",
                    r"INTO.*OUTFILE",
                    r"LOAD_FILE\(",
                    r"@@version"
                ],
                behavior_indicators=[
                    "database_access",
                    "command_execution",
                    "data_extraction"
                ],
                severity=ThreatAssessment.HIGH_RISK,
                mitre_techniques=["T1190"],
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            ),
            
            ExploitSignature(
                signature_id="SIG_003",
                name="Command Injection",
                description="Operating system command injection patterns",
                exploit_patterns=[
                    r";\s*cat\s+/etc/passwd",
                    r"\|\s*nc\s+",
                    r"&\s*whoami",
                    r"`.*`",
                    r"\$\(.*\)"
                ],
                payload_patterns=[
                    r"/bin/sh",
                    r"cmd\.exe",
                    r"powershell",
                    r"bash\s+-[ic]"
                ],
                behavior_indicators=[
                    "command_execution",
                    "shell_access",
                    "system_interaction"
                ],
                severity=ThreatAssessment.CRITICAL_RISK,
                mitre_techniques=["T1059"],
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
        ]
        
        for sig in signatures:
            self.signatures[sig.signature_id] = sig
        
        logger.info("Signature database initialized", signatures_count=len(self.signatures))
    
    def scan_for_signatures(self, data: str) -> List[ExploitSignature]:
        """Scan data for known exploit signatures"""
        detected_signatures = []
        
        for signature in self.signatures.values():
            # Check exploit patterns
            for pattern in signature.exploit_patterns:
                if re.search(pattern, data, re.IGNORECASE):
                    detected_signatures.append(signature)
                    break
            
            # Check payload patterns if not already detected
            if signature not in detected_signatures:
                for pattern in signature.payload_patterns:
                    if re.search(pattern, data, re.IGNORECASE):
                        detected_signatures.append(signature)
                        break
        
        return detected_signatures


class EnvironmentValidator:
    """Validates target environment for safety"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.authorized_environments = set(config.get("authorized_environments", []))
        self.production_indicators = config.get("production_indicators", [
            "prod", "production", "live", "critical", "customer"
        ])
    
    async def validate_environment(self, environment: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """Validate target environment for safety"""
        violations = []
        
        try:
            # Check if environment is authorized
            env_id = environment.get("environment_id", "unknown")
            if self.authorized_environments and env_id not in self.authorized_environments:
                violations.append(f"Environment {env_id} not in authorized list")
            
            # Check for production environment indicators
            env_name = environment.get("name", "").lower()
            env_type = environment.get("type", "").lower()
            
            for indicator in self.production_indicators:
                if indicator in env_name or indicator in env_type:
                    violations.append(f"Production environment detected: {indicator}")
            
            # Check network restrictions
            network_ranges = environment.get("network_ranges", [])
            for network in network_ranges:
                if self._is_restricted_network(network):
                    violations.append(f"Restricted network range detected: {network}")
            
            # Check for critical systems
            systems = environment.get("critical_systems", [])
            if systems:
                violations.append(f"Critical systems present: {len(systems)} systems")
            
            # Check for customer data
            if environment.get("contains_customer_data", False):
                violations.append("Environment contains customer data")
            
            # Check compliance requirements
            compliance = environment.get("compliance_requirements", [])
            restricted_compliance = ["pci-dss", "hipaa", "sox"]
            for req in compliance:
                if req.lower() in restricted_compliance:
                    violations.append(f"Restricted compliance environment: {req}")
            
            is_valid = len(violations) == 0
            
            return is_valid, violations
            
        except Exception as e:
            logger.error("Environment validation failed", error=str(e))
            return False, [f"Validation error: {str(e)}"]
    
    def _is_restricted_network(self, network: str) -> bool:
        """Check if network range is restricted"""
        restricted_ranges = [
            "10.0.0.0/8",      # Private networks (if production)
            "172.16.0.0/12",
            "192.168.0.0/16"
        ]
        
        # In production, implement proper CIDR matching
        return any(restricted in network for restricted in restricted_ranges)


class PayloadAnalyzer:
    """Analyzes payloads for safety and compliance"""
    
    def __init__(self):
        self.dangerous_patterns = [
            # Destructive operations
            r"rm\s+-rf\s+/",
            r"del\s+/[fqs]\s+",
            r"format\s+c:",
            r"DROP\s+DATABASE",
            r"TRUNCATE\s+TABLE",
            
            # Network attacks
            r"ping\s+-[tf]\s+",
            r"nc\s+-l\s+",
            r"nmap\s+.*-sS",
            
            # Privilege escalation
            r"sudo\s+su\s*-",
            r"runas\s+/user:",
            r"UAC\s*bypass",
            
            # Data exfiltration
            r"wget\s+.*\|\s*bash",
            r"curl\s+.*\|\s*sh",
            r"scp\s+.*\*",
            
            # Persistence mechanisms
            r"crontab\s+-",
            r"schtasks\s+/create",
            r"reg\s+add.*Run"
        ]
    
    async def analyze_payload(self, payload_data: bytes, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze payload for safety and threat assessment"""
        try:
            analysis_result = {
                "size": len(payload_data),
                "entropy": self._calculate_entropy(payload_data),
                "dangerous_patterns": [],
                "threat_level": ThreatAssessment.BENIGN,
                "encrypted": False,
                "obfuscated": False,
                "executable_content": False,
                "network_activity": False,
                "file_operations": False,
                "registry_operations": False,
                "recommendations": []
            }
            
            # Convert to string for pattern matching
            try:
                payload_str = payload_data.decode('utf-8', errors='ignore')
            except:
                payload_str = str(payload_data)
            
            # Check for dangerous patterns
            for pattern in self.dangerous_patterns:
                if re.search(pattern, payload_str, re.IGNORECASE):
                    analysis_result["dangerous_patterns"].append(pattern)
            
            # Assess threat level based on patterns found
            if analysis_result["dangerous_patterns"]:
                pattern_count = len(analysis_result["dangerous_patterns"])
                if pattern_count >= 3:
                    analysis_result["threat_level"] = ThreatAssessment.CRITICAL_RISK
                elif pattern_count >= 2:
                    analysis_result["threat_level"] = ThreatAssessment.HIGH_RISK
                else:
                    analysis_result["threat_level"] = ThreatAssessment.MEDIUM_RISK
            
            # Check for obfuscation/encryption
            if analysis_result["entropy"] > 7.0:
                analysis_result["encrypted"] = True
                analysis_result["threat_level"] = ThreatAssessment.HIGH_RISK
            elif analysis_result["entropy"] > 6.0:
                analysis_result["obfuscated"] = True
            
            # Check for executable content
            if self._contains_executable_content(payload_data):
                analysis_result["executable_content"] = True
                analysis_result["threat_level"] = ThreatAssessment.HIGH_RISK
            
            # Generate recommendations
            if analysis_result["dangerous_patterns"]:
                analysis_result["recommendations"].append("Payload contains dangerous patterns - review required")
            
            if analysis_result["encrypted"]:
                analysis_result["recommendations"].append("Encrypted payload detected - manual analysis required")
            
            if analysis_result["threat_level"] in [ThreatAssessment.HIGH_RISK, ThreatAssessment.CRITICAL_RISK]:
                analysis_result["recommendations"].append("High-risk payload - restrict to isolated environment only")
            
            return analysis_result
            
        except Exception as e:
            logger.error("Payload analysis failed", error=str(e))
            return {
                "error": str(e),
                "threat_level": ThreatAssessment.CRITICAL_RISK,
                "recommendations": ["Analysis failed - manual review required"]
            }
    
    def _calculate_entropy(self, data: bytes) -> float:
        """Calculate Shannon entropy of data"""
        if not data:
            return 0.0
        
        # Count byte frequencies
        frequencies = [0] * 256
        for byte in data:
            frequencies[byte] += 1
        
        # Calculate entropy
        entropy = 0.0
        length = len(data)
        
        for freq in frequencies:
            if freq > 0:
                p = freq / length
                entropy -= p * (p.bit_length() - 1)
        
        return entropy
    
    def _contains_executable_content(self, data: bytes) -> bool:
        """Check if data contains executable content"""
        # Check for common executable signatures
        exe_signatures = [
            b"MZ",              # DOS/Windows PE
            b"\x7fELF",         # Linux ELF
            b"\xca\xfe\xba\xbe", # Mach-O (macOS)
            b"\xfe\xed\xfa\xce", # Mach-O 32-bit
            b"\xfe\xed\xfa\xcf", # Mach-O 64-bit
        ]
        
        for sig in exe_signatures:
            if data.startswith(sig):
                return True
        
        return False


class ExploitValidationEngine:
    """Main exploit validation and safety engine"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.engine_id = str(uuid.uuid4())
        
        # Core components
        self.security_framework: Optional[SecurityFramework] = None
        self.audit_logger: Optional[AuditLogger] = None
        
        # Validation components
        self.signature_database = SignatureDatabase()
        self.environment_validator = EnvironmentValidator(config.get("environment", {}))
        self.payload_analyzer = PayloadAnalyzer()
        
        # Safety constraints
        self.safety_constraints: Dict[str, SafetyConstraint] = {}
        self.validation_level = ValidationLevel(config.get("validation_level", "strict"))
        
        # Validation cache
        self.validation_cache: Dict[str, ValidationResponse] = {}
        self.cache_ttl = timedelta(minutes=config.get("cache_ttl_minutes", 15))
        
        # Metrics
        self.metrics = {
            "validations_performed": 0,
            "validations_approved": 0,
            "validations_rejected": 0,
            "validations_conditional": 0,
            "safety_violations_detected": 0,
            "threats_detected": 0
        }
    
    async def initialize(self):
        """Initialize exploit validation engine"""
        try:
            logger.info("Initializing Exploit Validation Engine", engine_id=self.engine_id)
            
            # Initialize safety constraints
            await self._initialize_safety_constraints()
            
            # Start cache cleanup task
            asyncio.create_task(self._cleanup_validation_cache())
            
            logger.info("Exploit Validation Engine initialized successfully")
            
        except Exception as e:
            logger.error("Exploit Validation Engine initialization failed", error=str(e))
            raise
    
    async def validate_exploit(self, request: ValidationRequest) -> ValidationResponse:
        """Comprehensive exploit validation"""
        try:
            logger.info("Starting exploit validation", request_id=request.request_id)
            
            # Check cache first
            cache_key = self._generate_cache_key(request)
            cached_response = self.validation_cache.get(cache_key)
            if cached_response and self._is_cache_valid(cached_response):
                logger.debug("Returning cached validation result", request_id=request.request_id)
                return cached_response
            
            # Perform comprehensive validation
            validation_response = ValidationResponse(
                request_id=request.request_id,
                result=ValidationResult.APPROVED,  # Default, will be updated
                threat_assessment=ThreatAssessment.BENIGN,
                risk_score=0.0,
                safety_violations=[],
                compliance_issues=[],
                recommendations=[],
                conditions=[]
            )
            
            # 1. Environment validation
            env_valid, env_violations = await self.environment_validator.validate_environment(
                request.target_environment
            )
            if not env_valid:
                validation_response.safety_violations.extend(env_violations)
            
            # 2. Payload analysis
            if request.payload_info.get("payload_data"):
                payload_analysis = await self.payload_analyzer.analyze_payload(
                    request.payload_info["payload_data"],
                    request.payload_info
                )
                
                if payload_analysis["threat_level"] != ThreatAssessment.BENIGN:
                    validation_response.threat_assessment = payload_analysis["threat_level"]
                    validation_response.recommendations.extend(payload_analysis["recommendations"])
            
            # 3. Signature scanning
            exploit_data_str = json.dumps(request.exploit_data, default=str)
            detected_signatures = self.signature_database.scan_for_signatures(exploit_data_str)
            
            if detected_signatures:
                self.metrics["threats_detected"] += 1
                for sig in detected_signatures:
                    validation_response.safety_violations.append(
                        f"Detected exploit signature: {sig.name} ({sig.signature_id})"
                    )
                    # Update threat assessment to highest detected
                    if sig.severity.value > validation_response.threat_assessment.value:
                        validation_response.threat_assessment = sig.severity
            
            # 4. Safety constraint validation
            constraint_violations = await self._validate_safety_constraints(request)
            validation_response.safety_violations.extend(constraint_violations)
            
            # 5. Compliance validation
            compliance_issues = await self._validate_compliance_requirements(request)
            validation_response.compliance_issues.extend(compliance_issues)
            
            # 6. Calculate overall risk score
            validation_response.risk_score = await self._calculate_risk_score(
                validation_response, request
            )
            
            # 7. Make final validation decision
            validation_response.result = await self._make_validation_decision(
                validation_response, request
            )
            
            # 8. Add conditions based on validation level
            if validation_response.result == ValidationResult.CONDITIONAL:
                validation_response.conditions = await self._generate_conditions(
                    validation_response, request
                )
            
            # Update metrics
            self.metrics["validations_performed"] += 1
            if validation_response.safety_violations:
                self.metrics["safety_violations_detected"] += 1
            
            if validation_response.result == ValidationResult.APPROVED:
                self.metrics["validations_approved"] += 1
            elif validation_response.result == ValidationResult.REJECTED:
                self.metrics["validations_rejected"] += 1
            elif validation_response.result == ValidationResult.CONDITIONAL:
                self.metrics["validations_conditional"] += 1
            
            # Cache result
            self.validation_cache[cache_key] = validation_response
            
            # Log validation result
            await self._log_validation_event(request, validation_response)
            
            logger.info("Exploit validation completed",
                       request_id=request.request_id,
                       result=validation_response.result.value,
                       risk_score=validation_response.risk_score)
            
            return validation_response
            
        except Exception as e:
            logger.error("Exploit validation failed", request_id=request.request_id, error=str(e))
            
            # Return rejection on error
            error_response = ValidationResponse(
                request_id=request.request_id,
                result=ValidationResult.REJECTED,
                threat_assessment=ThreatAssessment.CRITICAL_RISK,
                risk_score=1.0,
                safety_violations=[f"Validation error: {str(e)}"],
                compliance_issues=[],
                recommendations=["Manual review required due to validation error"],
                conditions=[]
            )
            
            await self._log_validation_event(request, error_response)
            return error_response
    
    async def _validate_safety_constraints(self, request: ValidationRequest) -> List[str]:
        """Validate against safety constraints"""
        violations = []
        
        try:
            for constraint_id in request.safety_constraints:
                constraint = self.safety_constraints.get(constraint_id)
                if not constraint:
                    violations.append(f"Unknown safety constraint: {constraint_id}")
                    continue
                
                # Evaluate constraint rules
                for rule in constraint.validation_rules:
                    violation = await self._evaluate_constraint_rule(rule, request)
                    if violation:
                        violations.append(f"{constraint.name}: {violation}")
            
            return violations
            
        except Exception as e:
            logger.error("Safety constraint validation failed", error=str(e))
            return [f"Safety constraint validation error: {str(e)}"]
    
    async def _evaluate_constraint_rule(self, rule: Dict[str, Any], 
                                      request: ValidationRequest) -> Optional[str]:
        """Evaluate individual constraint rule"""
        try:
            rule_type = rule.get("type")
            
            if rule_type == "technique_restriction":
                # Check if technique is restricted
                restricted_techniques = rule.get("restricted_techniques", [])
                technique_id = request.technique_details.get("technique_id")
                if technique_id in restricted_techniques:
                    return f"Technique {technique_id} is restricted"
            
            elif rule_type == "payload_size_limit":
                # Check payload size limits
                max_size = rule.get("max_size", 0)
                payload_size = len(request.payload_info.get("payload_data", b""))
                if payload_size > max_size:
                    return f"Payload size {payload_size} exceeds limit {max_size}"
            
            elif rule_type == "environment_restriction":
                # Check environment restrictions
                allowed_environments = rule.get("allowed_environments", [])
                env_type = request.target_environment.get("type")
                if allowed_environments and env_type not in allowed_environments:
                    return f"Environment type {env_type} not allowed"
            
            elif rule_type == "time_restriction":
                # Check time-based restrictions
                allowed_hours = rule.get("allowed_hours", [])
                if allowed_hours:
                    current_hour = datetime.utcnow().hour
                    if current_hour not in allowed_hours:
                        return f"Operation not allowed at hour {current_hour}"
            
            return None
            
        except Exception as e:
            logger.error("Constraint rule evaluation failed", error=str(e))
            return f"Rule evaluation error: {str(e)}"
    
    async def _calculate_risk_score(self, response: ValidationResponse, 
                                  request: ValidationRequest) -> float:
        """Calculate overall risk score"""
        try:
            risk_factors = []
            
            # Threat assessment weight
            threat_weights = {
                ThreatAssessment.BENIGN: 0.0,
                ThreatAssessment.LOW_RISK: 0.2,
                ThreatAssessment.MEDIUM_RISK: 0.4,
                ThreatAssessment.HIGH_RISK: 0.7,
                ThreatAssessment.CRITICAL_RISK: 1.0
            }
            risk_factors.append(threat_weights.get(response.threat_assessment, 0.5))
            
            # Safety violations weight
            if response.safety_violations:
                violation_risk = min(1.0, len(response.safety_violations) * 0.2)
                risk_factors.append(violation_risk)
            
            # Compliance issues weight
            if response.compliance_issues:
                compliance_risk = min(1.0, len(response.compliance_issues) * 0.15)
                risk_factors.append(compliance_risk)
            
            # Environment risk
            env_type = request.target_environment.get("type", "unknown")
            env_risk = {
                "production": 1.0,
                "staging": 0.6,
                "testing": 0.3,
                "development": 0.1,
                "isolated": 0.0
            }.get(env_type, 0.5)
            risk_factors.append(env_risk)
            
            # Calculate weighted average
            if risk_factors:
                risk_score = sum(risk_factors) / len(risk_factors)
            else:
                risk_score = 0.0
            
            return min(1.0, risk_score)
            
        except Exception as e:
            logger.error("Risk score calculation failed", error=str(e))
            return 1.0  # Maximum risk on error
    
    async def _make_validation_decision(self, response: ValidationResponse,
                                      request: ValidationRequest) -> ValidationResult:
        """Make final validation decision"""
        try:
            # Strict validation level
            if self.validation_level == ValidationLevel.STRICT:
                if (response.safety_violations or 
                    response.compliance_issues or 
                    response.threat_assessment != ThreatAssessment.BENIGN):
                    return ValidationResult.REJECTED
                return ValidationResult.APPROVED
            
            # Moderate validation level
            elif self.validation_level == ValidationLevel.MODERATE:
                if response.risk_score >= 0.8:
                    return ValidationResult.REJECTED
                elif response.risk_score >= 0.5:
                    return ValidationResult.CONDITIONAL
                return ValidationResult.APPROVED
            
            # Permissive validation level
            elif self.validation_level == ValidationLevel.PERMISSIVE:
                if response.risk_score >= 0.9:
                    return ValidationResult.REJECTED
                elif response.risk_score >= 0.7:
                    return ValidationResult.REQUIRES_REVIEW
                return ValidationResult.APPROVED
            
            # Disabled validation
            else:
                return ValidationResult.APPROVED
            
        except Exception as e:
            logger.error("Validation decision failed", error=str(e))
            return ValidationResult.REJECTED
    
    async def _initialize_safety_constraints(self):
        """Initialize default safety constraints"""
        try:
            # Add default safety constraints
            default_constraints = [
                SafetyConstraint(
                    constraint_id="env_prod_restriction",
                    name="Production Environment Restriction",
                    description="Prevent operations in production environments",
                    constraint_type="environment",
                    validation_rules=[{
                        "type": "environment_restriction",
                        "allowed_environments": ["testing", "development", "isolated"]
                    }],
                    enforcement_level=ValidationLevel.STRICT,
                    exemptions=[],
                    created_by="system",
                    created_at=datetime.utcnow()
                ),
                SafetyConstraint(
                    constraint_id="payload_size_limit",
                    name="Payload Size Limitation",
                    description="Limit maximum payload size to prevent resource exhaustion",
                    constraint_type="payload",
                    validation_rules=[{
                        "type": "payload_size_limit",
                        "max_size": 1024 * 1024  # 1MB
                    }],
                    enforcement_level=ValidationLevel.MODERATE,
                    exemptions=[],
                    created_by="system",
                    created_at=datetime.utcnow()
                )
            ]
            
            for constraint in default_constraints:
                self.safety_constraints[constraint.constraint_id] = constraint
            
            logger.info("Safety constraints initialized", count=len(self.safety_constraints))
            
        except Exception as e:
            logger.error("Failed to initialize safety constraints", error=str(e))
            raise
    
    async def _cleanup_validation_cache(self):
        """Periodic cleanup of validation cache"""
        while True:
            try:
                current_time = datetime.utcnow()
                expired_keys = []
                
                for key, response in self.validation_cache.items():
                    if (response.valid_until and 
                        current_time > response.valid_until):
                        expired_keys.append(key)
                
                for key in expired_keys:
                    del self.validation_cache[key]
                
                if expired_keys:
                    logger.debug("Cleaned up expired cache entries", count=len(expired_keys))
                
                # Run cleanup every 5 minutes
                await asyncio.sleep(300)
                
            except Exception as e:
                logger.error("Cache cleanup failed", error=str(e))
                await asyncio.sleep(60)  # Retry in 1 minute on error
    
    def _generate_cache_key(self, request: ValidationRequest) -> str:
        """Generate cache key for validation request"""
        key_data = {
            "exploit_data": request.exploit_data,
            "target_environment": request.target_environment,
            "technique_details": request.technique_details,
            "payload_info": request.payload_info,
            "safety_constraints": sorted(request.safety_constraints),
            "compliance_requirements": sorted(request.compliance_requirements),
            "validation_level": request.validation_level.value
        }
        
        key_str = json.dumps(key_data, sort_keys=True, default=str)
        return hashlib.sha256(key_str.encode()).hexdigest()
    
    def _is_cache_valid(self, response: ValidationResponse) -> bool:
        """Check if cached response is still valid"""
        if not response.valid_until:
            return True  # No expiration set
        
        return datetime.utcnow() < response.valid_until
    
    async def _validate_compliance_requirements(self, request: ValidationRequest) -> List[str]:
        """Validate compliance requirements"""
        issues = []
        
        try:
            for requirement in request.compliance_requirements:
                if requirement.lower() == "pci-dss":
                    # Check PCI-DSS specific requirements
                    if request.target_environment.get("contains_cardholder_data", False):
                        issues.append("PCI-DSS: Testing on cardholder data environment requires additional approvals")
                
                elif requirement.lower() == "hipaa":
                    # Check HIPAA specific requirements
                    if request.target_environment.get("contains_phi", False):
                        issues.append("HIPAA: Testing on PHI environment requires compliance officer approval")
                
                elif requirement.lower() == "sox":
                    # Check SOX specific requirements
                    if request.target_environment.get("financial_systems", False):
                        issues.append("SOX: Testing on financial systems requires audit committee approval")
            
            return issues
            
        except Exception as e:
            logger.error("Compliance validation failed", error=str(e))
            return [f"Compliance validation error: {str(e)}"]
    
    async def _generate_conditions(self, response: ValidationResponse, 
                                 request: ValidationRequest) -> List[str]:
        """Generate conditions for conditional approval"""
        conditions = []
        
        try:
            # Environment-based conditions
            if response.risk_score >= 0.5:
                conditions.append("Execution must be performed in isolated environment")
                conditions.append("Real-time monitoring required during execution")
                conditions.append("Security team notification required before execution")
            
            # Payload-based conditions
            if request.payload_info.get("payload_data"):
                payload_size = len(request.payload_info["payload_data"])
                if payload_size > 512 * 1024:  # 512KB
                    conditions.append("Large payload detected - staged delivery recommended")
            
            # Technique-based conditions
            technique_id = request.technique_details.get("technique_id")
            if technique_id in ["T1059", "T1068", "T1190"]:  # High-risk techniques
                conditions.append("High-risk technique - requires security team oversight")
                conditions.append("Execution time limited to 30 minutes")
            
            # Time-based conditions
            current_hour = datetime.utcnow().hour
            if current_hour < 8 or current_hour > 18:  # Outside business hours
                conditions.append("Out-of-hours execution - emergency contact required")
            
            return conditions
            
        except Exception as e:
            logger.error("Failed to generate conditions", error=str(e))
            return ["Condition generation failed - manual review required"]
    
    async def _log_validation_event(self, request: ValidationRequest, 
                                  response: ValidationResponse):
        """Log validation event for audit trail"""
        try:
            if self.audit_logger:
                event = AuditEvent(
                    event_type="exploit_validation",
                    user_id=request.user_id,
                    resource_id=request.operation_id,
                    action="validate_exploit",
                    severity=EventSeverity.HIGH if response.result == ValidationResult.REJECTED else EventSeverity.MEDIUM,
                    details={
                        "request_id": request.request_id,
                        "validation_result": response.result.value,
                        "risk_score": response.risk_score,
                        "threat_assessment": response.threat_assessment.value,
                        "safety_violations": len(response.safety_violations),
                        "compliance_issues": len(response.compliance_issues)
                    },
                    timestamp=datetime.utcnow()
                )
                
                await self.audit_logger.log_event(event)
            
            # Also log to structured logger
            logger.info("Validation event logged",
                       request_id=request.request_id,
                       user_id=request.user_id,
                       operation_id=request.operation_id,
                       result=response.result.value,
                       risk_score=response.risk_score)
            
        except Exception as e:
            logger.error("Failed to log validation event", error=str(e))
    
    async def get_validation_metrics(self) -> Dict[str, Any]:
        """Get validation engine metrics"""
        return {
            "engine_id": self.engine_id,
            "validation_level": self.validation_level.value,
            "metrics": self.metrics.copy(),
            "cache_size": len(self.validation_cache),
            "safety_constraints": len(self.safety_constraints),
            "signatures_loaded": len(self.signature_database.signatures)
        }


# Export main classes
__all__ = [
    "ExploitValidationEngine",
    "ValidationLevel",
    "ThreatAssessment", 
    "ValidationResult",
    "ValidationRequest",
    "ValidationResponse",
    "ExploitSignature",
    "SafetyConstraint"
]