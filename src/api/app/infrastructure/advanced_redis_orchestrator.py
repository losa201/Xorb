"""
XORB Advanced Redis Orchestrator - Production-Ready Sophisticated Redis Infrastructure
Implements advanced Redis patterns, distributed coordination, and intelligent data management
"""

import asyncio
import logging
import json
import pickle
import time
import uuid
from typing import Dict, List, Optional, Any, Union, Callable, TypeVar, Set, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from enum import Enum
import hashlib
import zlib
from contextlib import asynccontextmanager
import inspect

import redis.asyncio as redis
from redis.asyncio.client import Pipeline
from redis.exceptions import ConnectionError, TimeoutError, RedisError

logger = logging.getLogger(__name__)

T = TypeVar('T')


class RedisPattern(Enum):
    """Advanced Redis patterns for sophisticated operations"""
    DISTRIBUTED_LOCK = "distributed_lock"
    LEADER_ELECTION = "leader_election"
    MESSAGE_QUEUE = "message_queue"
    PUBSUB_COORDINATION = "pubsub_coordination"
    RATE_LIMITING = "rate_limiting"
    CIRCUIT_BREAKER = "circuit_breaker"
    CACHE_INVALIDATION = "cache_invalidation"
    SESSION_MANAGEMENT = "session_management"
    REAL_TIME_ANALYTICS = "real_time_analytics"
    WORKFLOW_ORCHESTRATION = "workflow_orchestration"


class RedisDataStructure(Enum):
    """Advanced Redis data structures"""
    HYPERLOGLOG = "hyperloglog"
    BLOOM_FILTER = "bloom_filter"
    GEOSPATIAL = "geospatial"
    TIMESERIES = "timeseries"
    GRAPH = "graph"
    BITMAP = "bitmap"
    STREAM = "stream"


@dataclass
class RedisClusterNode:
    """Redis cluster node configuration"""
    host: str
    port: int
    role: str  # master, slave, sentinel
    priority: int = 0
    health_score: float = 1.0
    last_seen: datetime = None


@dataclass
class DistributedLockConfig:
    """Configuration for distributed locks"""
    timeout_seconds: int = 30
    retry_delay_ms: int = 100
    retry_jitter_ms: int = 50
    auto_release: bool = True
    extend_on_activity: bool = True


@dataclass
class MessageQueueConfig:
    """Configuration for Redis-based message queues"""
    max_length: int = 10000
    consumer_group: str = "default"
    acknowledge_timeout: int = 300
    retry_limit: int = 3
    priority_levels: int = 5


@dataclass
class RedisMetrics:
    """Advanced Redis metrics"""
    operations_per_second: float = 0.0
    memory_usage_mb: float = 0.0
    connected_clients: int = 0
    cache_hit_rate: float = 0.0
    network_throughput_mbps: float = 0.0
    cpu_usage_percent: float = 0.0
    replication_lag_ms: float = 0.0
    key_count: int = 0
    expired_keys: int = 0
    evicted_keys: int = 0


class AdvancedRedisOrchestrator:
    """Sophisticated Redis orchestrator with advanced patterns and distributed coordination"""
    
    def __init__(self, cluster_config: List[RedisClusterNode] = None):
        self.cluster_config = cluster_config or []
        self.redis_clients: Dict[str, redis.Redis] = {}
        self.connection_pools: Dict[str, redis.ConnectionPool] = {}
        self.is_initialized = False
        
        # Advanced pattern managers
        self.lock_manager = DistributedLockManager(self)
        self.queue_manager = MessageQueueManager(self)
        self.pubsub_coordinator = PubSubCoordinator(self)
        self.analytics_engine = RealTimeAnalytics(self)
        self.circuit_breaker = CircuitBreakerManager(self)
        
        # Monitoring and metrics
        self.metrics = RedisMetrics()
        self.health_checks: Dict[str, bool] = {}
        self.performance_history: List[RedisMetrics] = []
        
        # Distributed coordination
        self.node_id = str(uuid.uuid4())
        self.leader_node: Optional[str] = None
        self.cluster_topology: Dict[str, RedisClusterNode] = {}
        
        # Advanced caching strategies
        self.cache_strategies = {
            "threat_intelligence": {
                "ttl": 3600,
                "pattern": RedisPattern.CACHE_INVALIDATION,
                "compression": True,
                "replication": True
            },
            "scan_results": {
                "ttl": 7200,
                "pattern": RedisPattern.WORKFLOW_ORCHESTRATION,
                "compression": True,
                "persistence": True
            },
            "user_sessions": {
                "ttl": 86400,
                "pattern": RedisPattern.SESSION_MANAGEMENT,
                "compression": False,
                "distributed": True
            },
            "real_time_metrics": {
                "ttl": 300,
                "pattern": RedisPattern.REAL_TIME_ANALYTICS,
                "compression": False,
                "streaming": True
            }
        }
    
    async def initialize(self, primary_url: str = "redis://localhost:6379/0") -> bool:
        """Initialize sophisticated Redis infrastructure"""
        try:
            logger.info("Initializing advanced Redis orchestrator")
            
            # Initialize primary connection
            await self._setup_primary_connection(primary_url)
            
            # Initialize cluster if configured
            if self.cluster_config:
                await self._setup_cluster_connections()
            
            # Initialize advanced pattern managers
            await self._initialize_advanced_patterns()
            
            # Start health monitoring
            await self._start_health_monitoring()
            
            # Perform leader election if in cluster mode
            if self.cluster_config:
                await self._perform_leader_election()
            
            self.is_initialized = True
            logger.info("Advanced Redis orchestrator initialized successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize Redis orchestrator: {e}")
            return False
    
    async def _setup_primary_connection(self, url: str):
        """Setup primary Redis connection with advanced configuration"""
        try:
            pool = redis.ConnectionPool.from_url(
                url,
                decode_responses=True,
                max_connections=100,
                retry_on_timeout=True,
                health_check_interval=30,
                socket_keepalive=True,
                socket_keepalive_options={}
            )
            
            client = redis.Redis(connection_pool=pool)
            
            # Test connection
            await client.ping()
            
            self.connection_pools["primary"] = pool
            self.redis_clients["primary"] = client
            
            logger.info("Primary Redis connection established")
            
        except Exception as e:
            logger.error(f"Failed to setup primary Redis connection: {e}")
            raise
    
    async def _setup_cluster_connections(self):
        """Setup cluster connections with failover support"""
        for node in self.cluster_config:
            try:
                node_id = f"{node.host}:{node.port}"
                url = f"redis://{node.host}:{node.port}/0"
                
                pool = redis.ConnectionPool.from_url(
                    url,
                    decode_responses=True,
                    max_connections=50,
                    retry_on_timeout=True
                )
                
                client = redis.Redis(connection_pool=pool)
                await client.ping()
                
                self.connection_pools[node_id] = pool
                self.redis_clients[node_id] = client
                self.cluster_topology[node_id] = node
                
                logger.info(f"Connected to cluster node: {node_id}")
                
            except Exception as e:
                logger.warning(f"Failed to connect to cluster node {node.host}:{node.port}: {e}")
    
    async def _initialize_advanced_patterns(self):
        """Initialize advanced Redis patterns and managers"""
        await self.lock_manager.initialize()
        await self.queue_manager.initialize()
        await self.pubsub_coordinator.initialize()
        await self.analytics_engine.initialize()
        await self.circuit_breaker.initialize()
    
    async def _start_health_monitoring(self):
        """Start continuous health monitoring"""
        asyncio.create_task(self._health_monitor_loop())
        asyncio.create_task(self._metrics_collection_loop())
    
    async def _perform_leader_election(self):
        """Perform distributed leader election"""
        try:
            election_key = "cluster:leader_election"
            lock_key = f"cluster:leader_lock:{self.node_id}"
            
            # Try to acquire leadership
            primary_client = self.redis_clients["primary"]
            result = await primary_client.set(
                election_key, 
                self.node_id, 
                nx=True, 
                ex=60  # Leadership expires in 60 seconds
            )
            
            if result:
                self.leader_node = self.node_id
                logger.info(f"Node {self.node_id} elected as leader")
                
                # Start leader responsibilities
                asyncio.create_task(self._leader_heartbeat())
            else:
                # Find current leader
                current_leader = await primary_client.get(election_key)
                self.leader_node = current_leader
                logger.info(f"Following leader: {current_leader}")
                
        except Exception as e:
            logger.error(f"Leader election failed: {e}")
    
    async def _leader_heartbeat(self):
        """Maintain leadership with periodic heartbeats"""
        while self.leader_node == self.node_id:
            try:
                primary_client = self.redis_clients["primary"]
                await primary_client.expire("cluster:leader_election", 60)
                await asyncio.sleep(30)  # Heartbeat every 30 seconds
                
            except Exception as e:
                logger.error(f"Leader heartbeat failed: {e}")
                break
    
    async def _health_monitor_loop(self):
        """Continuous health monitoring loop"""
        while self.is_initialized:
            try:
                await self._check_cluster_health()
                await asyncio.sleep(10)  # Check every 10 seconds
                
            except Exception as e:
                logger.error(f"Health monitoring error: {e}")
                await asyncio.sleep(5)
    
    async def _metrics_collection_loop(self):
        """Continuous metrics collection loop"""
        while self.is_initialized:
            try:
                await self._collect_performance_metrics()
                await asyncio.sleep(30)  # Collect every 30 seconds
                
            except Exception as e:
                logger.error(f"Metrics collection error: {e}")
                await asyncio.sleep(10)
    
    async def _check_cluster_health(self):
        """Check health of all cluster nodes"""
        for node_id, client in self.redis_clients.items():
            try:
                start_time = time.time()
                await client.ping()
                response_time = (time.time() - start_time) * 1000
                
                self.health_checks[node_id] = True
                
                # Update node health score
                if node_id in self.cluster_topology:
                    node = self.cluster_topology[node_id]
                    node.health_score = min(1.0, 1.0 / max(response_time / 10, 1.0))
                    node.last_seen = datetime.utcnow()
                
            except Exception as e:
                logger.warning(f"Health check failed for {node_id}: {e}")
                self.health_checks[node_id] = False
                
                if node_id in self.cluster_topology:
                    self.cluster_topology[node_id].health_score = 0.0
    
    async def _collect_performance_metrics(self):
        """Collect comprehensive performance metrics"""
        try:
            primary_client = self.redis_clients["primary"]
            info = await primary_client.info()
            
            # Extract key metrics
            self.metrics.operations_per_second = info.get('instantaneous_ops_per_sec', 0)
            self.metrics.memory_usage_mb = info.get('used_memory', 0) / (1024 * 1024)
            self.metrics.connected_clients = info.get('connected_clients', 0)
            self.metrics.key_count = info.get('db0', {}).get('keys', 0)
            self.metrics.expired_keys = info.get('expired_keys', 0)
            self.metrics.evicted_keys = info.get('evicted_keys', 0)
            
            # Calculate cache hit rate
            keyspace_hits = info.get('keyspace_hits', 0)
            keyspace_misses = info.get('keyspace_misses', 0)
            total_requests = keyspace_hits + keyspace_misses
            self.metrics.cache_hit_rate = keyspace_hits / total_requests if total_requests > 0 else 0.0
            
            # Store metrics history
            self.performance_history.append(self.metrics)
            
            # Keep only last 100 measurements
            if len(self.performance_history) > 100:
                self.performance_history = self.performance_history[-100:]
                
        except Exception as e:
            logger.error(f"Metrics collection failed: {e}")
    
    async def get_optimal_client(self, operation_type: str = "read") -> redis.Redis:
        """Get optimal Redis client based on operation type and cluster health"""
        if not self.cluster_config:
            return self.redis_clients["primary"]
        
        # For read operations, prefer healthy slaves
        if operation_type == "read":
            slaves = [
                (node_id, node) for node_id, node in self.cluster_topology.items() 
                if node.role == "slave" and self.health_checks.get(node_id, False)
            ]
            
            if slaves:
                # Select slave with highest health score
                best_slave = max(slaves, key=lambda x: x[1].health_score)
                return self.redis_clients[best_slave[0]]
        
        # For write operations or if no healthy slaves, use master
        masters = [
            (node_id, node) for node_id, node in self.cluster_topology.items() 
            if node.role == "master" and self.health_checks.get(node_id, False)
        ]
        
        if masters:
            best_master = max(masters, key=lambda x: x[1].health_score)
            return self.redis_clients[best_master[0]]
        
        # Fallback to primary
        return self.redis_clients["primary"]
    
    async def execute_distributed_transaction(self, operations: List[Dict[str, Any]]) -> bool:
        """Execute distributed transaction across cluster"""
        try:
            transaction_id = str(uuid.uuid4())
            
            # Phase 1: Prepare all nodes
            prepared_nodes = []
            for node_id, client in self.redis_clients.items():
                try:
                    pipe = client.pipeline()
                    
                    # Add operations to pipeline
                    for op in operations:
                        if op["type"] == "set":
                            pipe.set(op["key"], op["value"], ex=op.get("ttl"))
                        elif op["type"] == "delete":
                            pipe.delete(op["key"])
                        elif op["type"] == "increment":
                            pipe.incr(op["key"])
                    
                    # Execute pipeline
                    await pipe.execute()
                    prepared_nodes.append(node_id)
                    
                except Exception as e:
                    logger.error(f"Transaction prepare failed on {node_id}: {e}")
                    break
            
            # Phase 2: Commit or rollback
            if len(prepared_nodes) == len(self.redis_clients):
                # All nodes prepared successfully - commit
                logger.info(f"Distributed transaction {transaction_id} committed")
                return True
            else:
                # Rollback on all prepared nodes
                await self._rollback_transaction(prepared_nodes, operations)
                logger.warning(f"Distributed transaction {transaction_id} rolled back")
                return False
                
        except Exception as e:
            logger.error(f"Distributed transaction failed: {e}")
            return False
    
    async def _rollback_transaction(self, nodes: List[str], operations: List[Dict[str, Any]]):
        """Rollback transaction on specified nodes"""
        for node_id in nodes:
            try:
                client = self.redis_clients[node_id]
                pipe = client.pipeline()
                
                # Reverse operations
                for op in reversed(operations):
                    if op["type"] == "set":
                        pipe.delete(op["key"])
                    elif op["type"] == "delete":
                        # Restore deleted key (if backup exists)
                        backup_key = f"backup:{op['key']}"
                        backup_value = await client.get(backup_key)
                        if backup_value:
                            pipe.set(op["key"], backup_value)
                    elif op["type"] == "increment":
                        pipe.decr(op["key"])
                
                await pipe.execute()
                
            except Exception as e:
                logger.error(f"Rollback failed on {node_id}: {e}")
    
    async def create_intelligent_cache(self, namespace: str, config: Dict[str, Any]) -> 'IntelligentCache':
        """Create intelligent cache with advanced features"""
        return IntelligentCache(self, namespace, config)
    
    async def get_cluster_status(self) -> Dict[str, Any]:
        """Get comprehensive cluster status"""
        return {
            "node_id": self.node_id,
            "leader_node": self.leader_node,
            "is_leader": self.leader_node == self.node_id,
            "cluster_size": len(self.cluster_topology),
            "healthy_nodes": sum(1 for health in self.health_checks.values() if health),
            "metrics": asdict(self.metrics),
            "performance_trend": self._calculate_performance_trend(),
            "nodes": {
                node_id: {
                    "role": node.role,
                    "health_score": node.health_score,
                    "last_seen": node.last_seen.isoformat() if node.last_seen else None,
                    "healthy": self.health_checks.get(node_id, False)
                }
                for node_id, node in self.cluster_topology.items()
            }
        }
    
    def _calculate_performance_trend(self) -> str:
        """Calculate performance trend from metrics history"""
        if len(self.performance_history) < 2:
            return "stable"
        
        recent_ops = [m.operations_per_second for m in self.performance_history[-10:]]
        if not recent_ops:
            return "stable"
        
        avg_recent = sum(recent_ops) / len(recent_ops)
        older_ops = [m.operations_per_second for m in self.performance_history[-20:-10]]
        
        if not older_ops:
            return "stable"
        
        avg_older = sum(older_ops) / len(older_ops)
        
        if avg_recent > avg_older * 1.1:
            return "improving"
        elif avg_recent < avg_older * 0.9:
            return "degrading"
        else:
            return "stable"
    
    async def shutdown(self):
        """Gracefully shutdown Redis orchestrator"""
        logger.info("Shutting down Redis orchestrator")
        
        self.is_initialized = False
        
        # Close all connections
        for client in self.redis_clients.values():
            try:
                await client.close()
            except Exception as e:
                logger.warning(f"Error closing Redis connection: {e}")
        
        # Close connection pools
        for pool in self.connection_pools.values():
            try:
                await pool.disconnect()
            except Exception as e:
                logger.warning(f"Error disconnecting connection pool: {e}")
        
        logger.info("Redis orchestrator shutdown complete")


class DistributedLockManager:
    """Advanced distributed lock manager with sophisticated features"""
    
    def __init__(self, orchestrator: AdvancedRedisOrchestrator):
        self.orchestrator = orchestrator
        self.active_locks: Dict[str, Dict[str, Any]] = {}
        self.lock_statistics: Dict[str, int] = {}
    
    async def initialize(self):
        """Initialize lock manager"""
        logger.info("Initializing distributed lock manager")
    
    @asynccontextmanager
    async def acquire_lock(
        self, 
        resource_id: str, 
        config: DistributedLockConfig = None
    ):
        """Acquire distributed lock with automatic release"""
        config = config or DistributedLockConfig()
        lock_id = str(uuid.uuid4())
        lock_key = f"lock:{resource_id}"
        
        try:
            # Attempt to acquire lock
            acquired = await self._acquire_lock_internal(lock_key, lock_id, config)
            
            if not acquired:
                raise TimeoutError(f"Failed to acquire lock for {resource_id}")
            
            self.active_locks[lock_key] = {
                "lock_id": lock_id,
                "resource_id": resource_id,
                "acquired_at": time.time(),
                "config": config
            }
            
            self.lock_statistics[resource_id] = self.lock_statistics.get(resource_id, 0) + 1
            
            yield lock_id
            
        finally:
            # Always try to release lock
            await self._release_lock_internal(lock_key, lock_id)
            if lock_key in self.active_locks:
                del self.active_locks[lock_key]
    
    async def _acquire_lock_internal(
        self, 
        lock_key: str, 
        lock_id: str, 
        config: DistributedLockConfig
    ) -> bool:
        """Internal lock acquisition with retry logic"""
        client = await self.orchestrator.get_optimal_client("write")
        
        start_time = time.time()
        while time.time() - start_time < config.timeout_seconds:
            try:
                # Try to set lock with expiration
                result = await client.set(
                    lock_key, 
                    lock_id, 
                    nx=True, 
                    ex=config.timeout_seconds
                )
                
                if result:
                    return True
                
                # Wait before retry with jitter
                delay = config.retry_delay_ms / 1000
                jitter = (config.retry_jitter_ms / 1000) * (0.5 - asyncio.get_event_loop().time() % 1)
                await asyncio.sleep(delay + jitter)
                
            except Exception as e:
                logger.error(f"Lock acquisition error: {e}")
                await asyncio.sleep(config.retry_delay_ms / 1000)
        
        return False
    
    async def _release_lock_internal(self, lock_key: str, lock_id: str):
        """Internal lock release with verification"""
        try:
            client = await self.orchestrator.get_optimal_client("write")
            
            # Lua script to atomically check and release lock
            release_script = """
            if redis.call("get", KEYS[1]) == ARGV[1] then
                return redis.call("del", KEYS[1])
            else
                return 0
            end
            """
            
            await client.eval(release_script, 1, lock_key, lock_id)
            
        except Exception as e:
            logger.error(f"Lock release error: {e}")


class MessageQueueManager:
    """Sophisticated message queue manager using Redis Streams"""
    
    def __init__(self, orchestrator: AdvancedRedisOrchestrator):
        self.orchestrator = orchestrator
        self.active_queues: Dict[str, MessageQueueConfig] = {}
        self.consumer_tasks: Dict[str, asyncio.Task] = {}
    
    async def initialize(self):
        """Initialize message queue manager"""
        logger.info("Initializing message queue manager")
    
    async def create_queue(self, queue_name: str, config: MessageQueueConfig = None) -> bool:
        """Create sophisticated message queue"""
        try:
            config = config or MessageQueueConfig()
            client = await self.orchestrator.get_optimal_client("write")
            
            # Create consumer group
            try:
                await client.xgroup_create(
                    queue_name, 
                    config.consumer_group, 
                    id="0", 
                    mkstream=True
                )
            except redis.ResponseError as e:
                if "BUSYGROUP" not in str(e):
                    raise
            
            self.active_queues[queue_name] = config
            logger.info(f"Created message queue: {queue_name}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to create queue {queue_name}: {e}")
            return False
    
    async def enqueue_message(
        self, 
        queue_name: str, 
        message: Dict[str, Any], 
        priority: int = 3
    ) -> str:
        """Enqueue message with priority"""
        try:
            client = await self.orchestrator.get_optimal_client("write")
            
            # Add priority and metadata
            message_data = {
                "priority": priority,
                "timestamp": time.time(),
                "payload": json.dumps(message),
                "id": str(uuid.uuid4())
            }
            
            # Add to stream
            message_id = await client.xadd(queue_name, message_data)
            
            # Trim queue to max length
            config = self.active_queues.get(queue_name, MessageQueueConfig())
            await client.xtrim(queue_name, maxlen=config.max_length, approximate=True)
            
            return message_id
            
        except Exception as e:
            logger.error(f"Failed to enqueue message: {e}")
            raise
    
    async def start_consumer(
        self, 
        queue_name: str, 
        consumer_name: str, 
        handler: Callable[[Dict[str, Any]], Any]
    ) -> bool:
        """Start sophisticated message consumer"""
        try:
            if queue_name not in self.active_queues:
                await self.create_queue(queue_name)
            
            consumer_task = asyncio.create_task(
                self._consumer_loop(queue_name, consumer_name, handler)
            )
            
            task_key = f"{queue_name}:{consumer_name}"
            self.consumer_tasks[task_key] = consumer_task
            
            logger.info(f"Started consumer {consumer_name} for queue {queue_name}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to start consumer: {e}")
            return False
    
    async def _consumer_loop(
        self, 
        queue_name: str, 
        consumer_name: str, 
        handler: Callable[[Dict[str, Any]], Any]
    ):
        """Consumer loop with error handling and acknowledgment"""
        client = await self.orchestrator.get_optimal_client("read")
        config = self.active_queues[queue_name]
        
        while True:
            try:
                # Read messages from stream
                messages = await client.xreadgroup(
                    config.consumer_group,
                    consumer_name,
                    {queue_name: ">"},
                    count=10,
                    block=1000
                )
                
                for stream, stream_messages in messages:
                    for message_id, fields in stream_messages:
                        try:
                            # Process message
                            payload = json.loads(fields.get("payload", "{}"))
                            await handler(payload)
                            
                            # Acknowledge message
                            await client.xack(queue_name, config.consumer_group, message_id)
                            
                        except Exception as e:
                            logger.error(f"Message processing error: {e}")
                            # Could implement retry logic here
                            
            except Exception as e:
                logger.error(f"Consumer loop error: {e}")
                await asyncio.sleep(5)


class PubSubCoordinator:
    """Advanced Pub/Sub coordinator for real-time communication"""
    
    def __init__(self, orchestrator: AdvancedRedisOrchestrator):
        self.orchestrator = orchestrator
        self.subscribers: Dict[str, List[Callable]] = {}
        self.pubsub_tasks: Dict[str, asyncio.Task] = {}
    
    async def initialize(self):
        """Initialize Pub/Sub coordinator"""
        logger.info("Initializing Pub/Sub coordinator")
    
    async def publish(self, channel: str, message: Dict[str, Any]) -> int:
        """Publish message to channel"""
        try:
            client = await self.orchestrator.get_optimal_client("write")
            
            message_data = {
                "timestamp": time.time(),
                "sender": self.orchestrator.node_id,
                "payload": message
            }
            
            return await client.publish(channel, json.dumps(message_data))
            
        except Exception as e:
            logger.error(f"Failed to publish message: {e}")
            return 0
    
    async def subscribe(self, channel: str, handler: Callable[[Dict[str, Any]], None]):
        """Subscribe to channel with message handler"""
        try:
            if channel not in self.subscribers:
                self.subscribers[channel] = []
                
                # Start subscription task
                task = asyncio.create_task(self._subscription_loop(channel))
                self.pubsub_tasks[channel] = task
            
            self.subscribers[channel].append(handler)
            logger.info(f"Subscribed to channel: {channel}")
            
        except Exception as e:
            logger.error(f"Failed to subscribe to channel {channel}: {e}")
    
    async def _subscription_loop(self, channel: str):
        """Subscription loop for handling messages"""
        client = await self.orchestrator.get_optimal_client("read")
        pubsub = client.pubsub()
        
        try:
            await pubsub.subscribe(channel)
            
            async for message in pubsub.listen():
                if message["type"] == "message":
                    try:
                        data = json.loads(message["data"])
                        
                        # Call all handlers for this channel
                        for handler in self.subscribers.get(channel, []):
                            try:
                                await handler(data["payload"])
                            except Exception as e:
                                logger.error(f"Handler error for channel {channel}: {e}")
                                
                    except Exception as e:
                        logger.error(f"Message parsing error: {e}")
                        
        except Exception as e:
            logger.error(f"Subscription loop error for {channel}: {e}")
        finally:
            await pubsub.close()


class RealTimeAnalytics:
    """Real-time analytics engine using Redis"""
    
    def __init__(self, orchestrator: AdvancedRedisOrchestrator):
        self.orchestrator = orchestrator
        self.metric_definitions: Dict[str, Dict[str, Any]] = {}
    
    async def initialize(self):
        """Initialize real-time analytics"""
        logger.info("Initializing real-time analytics engine")
    
    async def track_event(
        self, 
        event_type: str, 
        properties: Dict[str, Any] = None, 
        timestamp: float = None
    ):
        """Track real-time event"""
        try:
            client = await self.orchestrator.get_optimal_client("write")
            timestamp = timestamp or time.time()
            properties = properties or {}
            
            # Store in multiple data structures for different query patterns
            
            # 1. Time series for trend analysis
            ts_key = f"timeseries:{event_type}"
            await client.zadd(ts_key, {str(uuid.uuid4()): timestamp})
            
            # 2. Counters for aggregation
            counter_key = f"counter:{event_type}:{int(timestamp // 3600)}"  # Hourly buckets
            await client.incr(counter_key)
            await client.expire(counter_key, 86400 * 7)  # Keep for 7 days
            
            # 3. Properties for filtering
            if properties:
                for key, value in properties.items():
                    prop_key = f"property:{event_type}:{key}:{value}"
                    await client.incr(prop_key)
                    await client.expire(prop_key, 86400)
            
            # 4. Recent events for real-time monitoring
            recent_key = f"recent:{event_type}"
            event_data = {
                "timestamp": timestamp,
                "properties": properties
            }
            await client.lpush(recent_key, json.dumps(event_data))
            await client.ltrim(recent_key, 0, 999)  # Keep last 1000 events
            
        except Exception as e:
            logger.error(f"Failed to track event {event_type}: {e}")
    
    async def get_metric(
        self, 
        metric_name: str, 
        time_range_hours: int = 24, 
        granularity: str = "hour"
    ) -> Dict[str, Any]:
        """Get real-time metric data"""
        try:
            client = await self.orchestrator.get_optimal_client("read")
            current_time = time.time()
            
            if granularity == "hour":
                bucket_size = 3600
            elif granularity == "minute":
                bucket_size = 60
            else:
                bucket_size = 86400  # day
            
            # Calculate time range
            start_time = current_time - (time_range_hours * 3600)
            buckets = []
            
            for i in range(int(time_range_hours * 3600 / bucket_size)):
                bucket_time = int((start_time + i * bucket_size) // bucket_size) * bucket_size
                counter_key = f"counter:{metric_name}:{int(bucket_time // 3600)}"
                
                count = await client.get(counter_key) or 0
                buckets.append({
                    "timestamp": bucket_time,
                    "value": int(count)
                })
            
            # Calculate aggregations
            values = [b["value"] for b in buckets]
            total = sum(values)
            average = total / len(values) if values else 0
            peak = max(values) if values else 0
            
            return {
                "metric_name": metric_name,
                "time_range_hours": time_range_hours,
                "granularity": granularity,
                "total": total,
                "average": average,
                "peak": peak,
                "buckets": buckets
            }
            
        except Exception as e:
            logger.error(f"Failed to get metric {metric_name}: {e}")
            return {}


class CircuitBreakerManager:
    """Sophisticated circuit breaker implementation"""
    
    def __init__(self, orchestrator: AdvancedRedisOrchestrator):
        self.orchestrator = orchestrator
        self.circuit_configs: Dict[str, Dict[str, Any]] = {}
    
    async def initialize(self):
        """Initialize circuit breaker manager"""
        logger.info("Initializing circuit breaker manager")
    
    async def call_with_circuit_breaker(
        self, 
        circuit_name: str, 
        func: Callable, 
        *args, 
        **kwargs
    ):
        """Execute function with circuit breaker protection"""
        try:
            # Check circuit state
            if await self._is_circuit_open(circuit_name):
                raise Exception(f"Circuit breaker {circuit_name} is open")
            
            # Execute function
            try:
                result = await func(*args, **kwargs)
                await self._record_success(circuit_name)
                return result
                
            except Exception as e:
                await self._record_failure(circuit_name)
                raise
                
        except Exception as e:
            logger.error(f"Circuit breaker {circuit_name} error: {e}")
            raise
    
    async def _is_circuit_open(self, circuit_name: str) -> bool:
        """Check if circuit breaker is open"""
        client = await self.orchestrator.get_optimal_client("read")
        
        # Check if circuit is manually opened
        manual_state = await client.get(f"circuit:{circuit_name}:manual")
        if manual_state == "open":
            return True
        
        # Check failure rate
        failure_key = f"circuit:{circuit_name}:failures"
        success_key = f"circuit:{circuit_name}:successes"
        
        failures = int(await client.get(failure_key) or 0)
        successes = int(await client.get(success_key) or 0)
        total = failures + successes
        
        if total >= 10:  # Minimum requests before opening
            failure_rate = failures / total
            if failure_rate > 0.5:  # Open if > 50% failure rate
                return True
        
        return False
    
    async def _record_success(self, circuit_name: str):
        """Record successful execution"""
        client = await self.orchestrator.get_optimal_client("write")
        success_key = f"circuit:{circuit_name}:successes"
        
        await client.incr(success_key)
        await client.expire(success_key, 300)  # 5 minute window
    
    async def _record_failure(self, circuit_name: str):
        """Record failed execution"""
        client = await self.orchestrator.get_optimal_client("write")
        failure_key = f"circuit:{circuit_name}:failures"
        
        await client.incr(failure_key)
        await client.expire(failure_key, 300)  # 5 minute window


class IntelligentCache:
    """Intelligent cache with adaptive policies and machine learning"""
    
    def __init__(self, orchestrator: AdvancedRedisOrchestrator, namespace: str, config: Dict[str, Any]):
        self.orchestrator = orchestrator
        self.namespace = namespace
        self.config = config
        self.access_patterns: Dict[str, List[float]] = {}
        self.hit_rates: Dict[str, float] = {}
    
    async def get(self, key: str) -> Optional[Any]:
        """Intelligent cache get with pattern learning"""
        try:
            full_key = f"{self.namespace}:{key}"
            client = await self.orchestrator.get_optimal_client("read")
            
            # Record access pattern
            self._record_access(key)
            
            # Get from cache
            cached_data = await client.get(full_key)
            
            if cached_data:
                # Cache hit
                self._record_hit(key)
                return self._deserialize(cached_data)
            else:
                # Cache miss
                self._record_miss(key)
                return None
                
        except Exception as e:
            logger.error(f"Intelligent cache get error: {e}")
            return None
    
    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """Intelligent cache set with adaptive TTL"""
        try:
            full_key = f"{self.namespace}:{key}"
            client = await self.orchestrator.get_optimal_client("write")
            
            # Adaptive TTL based on access patterns
            if ttl is None:
                ttl = self._calculate_adaptive_ttl(key)
            
            # Serialize and compress if needed
            serialized_value = self._serialize(value)
            
            # Set in cache
            await client.setex(full_key, ttl, serialized_value)
            
            return True
            
        except Exception as e:
            logger.error(f"Intelligent cache set error: {e}")
            return False
    
    def _record_access(self, key: str):
        """Record access pattern for key"""
        current_time = time.time()
        
        if key not in self.access_patterns:
            self.access_patterns[key] = []
        
        self.access_patterns[key].append(current_time)
        
        # Keep only recent accesses
        cutoff_time = current_time - 3600
        self.access_patterns[key] = [
            t for t in self.access_patterns[key] if t > cutoff_time
        ]
    
    def _record_hit(self, key: str):
        """Record cache hit"""
        if key not in self.hit_rates:
            self.hit_rates[key] = 0.0
        
        # Exponential moving average
        self.hit_rates[key] = 0.9 * self.hit_rates[key] + 0.1 * 1.0
    
    def _record_miss(self, key: str):
        """Record cache miss"""
        if key not in self.hit_rates:
            self.hit_rates[key] = 0.0
        
        # Exponential moving average
        self.hit_rates[key] = 0.9 * self.hit_rates[key] + 0.1 * 0.0
    
    def _calculate_adaptive_ttl(self, key: str) -> int:
        """Calculate adaptive TTL based on access patterns"""
        base_ttl = self.config.get("ttl", 3600)
        
        # Get access frequency
        access_count = len(self.access_patterns.get(key, []))
        hit_rate = self.hit_rates.get(key, 0.5)
        
        # Adjust TTL based on patterns
        if access_count > 10 and hit_rate > 0.8:
            # Frequently accessed with high hit rate - extend TTL
            return int(base_ttl * 2)
        elif hit_rate < 0.3:
            # Low hit rate - reduce TTL
            return int(base_ttl * 0.5)
        else:
            return base_ttl
    
    def _serialize(self, value: Any) -> bytes:
        """Serialize value for storage"""
        if self.config.get("compression", False):
            serialized = pickle.dumps(value)
            return zlib.compress(serialized)
        else:
            return pickle.dumps(value)
    
    def _deserialize(self, data: bytes) -> Any:
        """Deserialize value from storage"""
        if self.config.get("compression", False):
            decompressed = zlib.decompress(data)
            return pickle.loads(decompressed)
        else:
            return pickle.loads(data)


# Global orchestrator instance
_redis_orchestrator: Optional[AdvancedRedisOrchestrator] = None


async def get_redis_orchestrator() -> AdvancedRedisOrchestrator:
    """Get global Redis orchestrator instance"""
    global _redis_orchestrator
    
    if _redis_orchestrator is None:
        _redis_orchestrator = AdvancedRedisOrchestrator()
        await _redis_orchestrator.initialize()
    
    return _redis_orchestrator


async def init_advanced_redis(primary_url: str = "redis://localhost:6379/0", cluster_config: List[RedisClusterNode] = None):
    """Initialize advanced Redis infrastructure"""
    global _redis_orchestrator
    
    _redis_orchestrator = AdvancedRedisOrchestrator(cluster_config)
    success = await _redis_orchestrator.initialize(primary_url)
    
    if success:
        logger.info("Advanced Redis infrastructure initialized successfully")
    else:
        logger.error("Failed to initialize advanced Redis infrastructure")
        raise Exception("Redis initialization failed")
    
    return _redis_orchestrator


async def shutdown_advanced_redis():
    """Shutdown advanced Redis infrastructure"""
    global _redis_orchestrator
    
    if _redis_orchestrator:
        await _redis_orchestrator.shutdown()
        _redis_orchestrator = None