#!/usr/bin/env python3
"""
XORB Global Intelligence Synthesis Engine v10.0

This module implements the core Phase 10 capability: autonomous global intelligence
synthesis that ingests, normalizes, correlates, and acts on distributed intelligence
from multiple sources in real-time.

Architecture: Ingest â†’ Normalize â†’ Fuse â†’ Interpret â†’ Act â†’ Learn
"""

import asyncio
import json
import logging
import time
import uuid
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Set, Tuple, Union, Callable
from dataclasses import dataclass, field, asdict
from enum import Enum
from collections import defaultdict, deque
import heapq
from concurrent.futures import ThreadPoolExecutor

import structlog
import aiohttp
import feedparser
from prometheus_client import Counter, Histogram, Gauge

# XORB Internal Imports
from ..autonomous.episodic_memory_system import EpisodicMemorySystem, MemoryRecord
from ..autonomous.autonomous_orchestrator import AutonomousOrchestrator
from ..mission.adaptive_mission_engine import AdaptiveMissionEngine, MissionType
from ..llm.enhanced_multi_provider_client import EnhancedMultiProviderClient
from ..knowledge_fabric.vector_fabric import VectorFabric
from xorb_core.integrations.hackerone_client import HackerOneClient
from xorb_core.intelligence.semantic_cache import SemanticCache


class IntelligenceSourceType(Enum):
    """Types of intelligence sources"""
    CVE_NVD = "cve_nvd"
    HACKERONE = "hackerone"
    BUGCROWD = "bugcrowd"
    INTIGRITI = "intigriti"
    OSINT_RSS = "osint_rss"
    THREAT_INTEL = "threat_intel"
    INTERNAL_MISSIONS = "internal_missions"
    PROMETHEUS_ALERTS = "prometheus_alerts"
    SOCIAL_MEDIA = "social_media"
    DARK_WEB = "dark_web"
    VENDOR_ADVISORIES = "vendor_advisories"


class SignalPriority(Enum):
    """Signal priority levels"""
    CRITICAL = 5
    HIGH = 4
    MEDIUM = 3
    LOW = 2
    INFO = 1


class IntelligenceSignalStatus(Enum):
    """Intelligence signal processing status"""
    RAW = "raw"
    NORMALIZED = "normalized"
    CORRELATED = "correlated"
    ACTIONABLE = "actionable"
    PROCESSED = "processed"
    ARCHIVED = "archived"


@dataclass
class IntelligenceSource:
    """Intelligence source configuration"""
    source_id: str
    source_type: IntelligenceSourceType
    name: str
    url: str
    
    # Authentication and access
    api_key: Optional[str] = None
    credentials: Optional[Dict[str, str]] = None
    headers: Dict[str, str] = field(default_factory=dict)
    
    # Processing configuration
    poll_interval: int = 300  # seconds
    batch_size: int = 100
    rate_limit: int = 60  # requests per minute
    
    # Quality and reliability
    confidence_weight: float = 1.0
    reliability_score: float = 0.8
    historical_accuracy: float = 0.85
    
    # Status and metrics
    active: bool = True
    last_poll: Optional[datetime] = None
    total_signals: int = 0
    error_count: int = 0
    
    # Filtering and processing
    include_patterns: List[str] = field(default_factory=list)
    exclude_patterns: List[str] = field(default_factory=list)
    priority_keywords: List[str] = field(default_factory=list)


@dataclass
class IntelligenceSignal:
    """Raw intelligence signal from external source"""
    signal_id: str
    source_id: str
    source_type: IntelligenceSourceType
    
    # Core content
    title: str
    description: str
    content: Dict[str, Any]
    raw_data: Dict[str, Any]
    
    # Metadata
    timestamp: datetime
    discovered_at: datetime
    url: Optional[str] = None
    tags: List[str] = field(default_factory=list)
    
    # Classification
    signal_type: str = "unknown"
    priority: SignalPriority = SignalPriority.MEDIUM
    confidence: float = 0.5
    
    # Processing status
    status: IntelligenceSignalStatus = IntelligenceSignalStatus.RAW
    processing_history: List[Dict[str, Any]] = field(default_factory=list)
    
    # Correlation data
    related_signals: List[str] = field(default_factory=list)
    correlation_score: float = 0.0
    deduplication_hash: Optional[str] = None
    
    # Action tracking
    triggered_missions: List[str] = field(default_factory=list)
    agent_assignments: List[str] = field(default_factory=list)
    
    def __post_init__(self):
        if self.discovered_at is None:
            self.discovered_at = datetime.utcnow()
        if self.deduplication_hash is None:
            self.deduplication_hash = self._generate_dedup_hash()
    
    def _generate_dedup_hash(self) -> str:
        """Generate deduplication hash from content"""
        content_str = f"{self.title}|{self.description}|{self.source_type.value}"
        return hashlib.sha256(content_str.encode()).hexdigest()[:16]


@dataclass
class CorrelatedIntelligence:
    """Correlated and fused intelligence ready for action"""
    intelligence_id: str
    primary_signal_id: str
    related_signal_ids: List[str]
    
    # Synthesized content
    synthesized_title: str
    synthesized_description: str
    key_indicators: List[str]
    threat_context: Dict[str, Any]
    
    # Assessment
    overall_priority: SignalPriority
    confidence_score: float
    threat_level: str
    impact_assessment: Dict[str, Any]
    
    # Action recommendations
    recommended_actions: List[Dict[str, Any]]
    required_capabilities: List[str]
    estimated_effort: str
    
    # Temporal data
    created_at: datetime
    expires_at: Optional[datetime] = None
    
    # Mission tracking
    spawned_missions: List[str] = field(default_factory=list)
    feedback_scores: List[float] = field(default_factory=list)


class GlobalSynthesisEngine:
    """
    Global Intelligence Synthesis Engine
    
    Orchestrates the complete intelligence lifecycle:
    1. Ingest: Pull from multiple global sources
    2. Normalize: Convert to standard schema
    3. Fuse: Correlate and deduplicate
    4. Interpret: Extract actionable insights
    5. Act: Route to appropriate agents/missions
    6. Learn: Continuously improve from feedback
    """
    
    def __init__(self, 
                 orchestrator: AutonomousOrchestrator,
                 mission_engine: AdaptiveMissionEngine,
                 episodic_memory: EpisodicMemorySystem,
                 vector_fabric: VectorFabric):
        
        self.orchestrator = orchestrator
        self.mission_engine = mission_engine
        self.episodic_memory = episodic_memory
        self.vector_fabric = vector_fabric
        
        self.logger = structlog.get_logger("GlobalSynthesisEngine")
        
        # Intelligence sources and processing
        self.intelligence_sources: Dict[str, IntelligenceSource] = {}
        self.raw_signals: Dict[str, IntelligenceSignal] = {}
        self.correlated_intelligence: Dict[str, CorrelatedIntelligence] = {}
        
        # Processing queues and state
        self.ingestion_queue: asyncio.Queue = asyncio.Queue()
        self.normalization_queue: asyncio.Queue = asyncio.Queue()
        self.correlation_queue: asyncio.Queue = asyncio.Queue()
        self.action_queue: asyncio.Queue = asyncio.Queue()
        
        # Deduplication and correlation
        self.deduplication_cache: Dict[str, str] = {}  # hash -> signal_id
        self.correlation_graph: Dict[str, Set[str]] = defaultdict(set)
        self.signal_embeddings: Dict[str, List[float]] = {}
        
        # Learning and optimization
        self.source_performance: Dict[str, Dict[str, float]] = defaultdict(dict)
        self.signal_effectiveness: Dict[str, float] = {}
        self.correlation_patterns: Dict[str, int] = defaultdict(int)
        
        # External integrations
        self.llm_client = EnhancedMultiProviderClient()
        self.semantic_cache = SemanticCache()
        self.hackerone_client = HackerOneClient()
        
        # Metrics and monitoring
        self.synthesis_metrics = self._initialize_metrics()
        
        # Configuration
        self.max_signals_memory = 50000
        self.correlation_threshold = 0.75
        self.processing_batch_size = 50
        self.max_concurrent_sources = 20
        
        # State management
        self._running = False
        self._processing_tasks: List[asyncio.Task] = []
        
    def _initialize_metrics(self) -> Dict[str, Any]:
        """Initialize Prometheus metrics"""
        return {
            'signals_ingested': Counter('xorb_synthesis_signals_ingested_total', 
                                      'Total signals ingested', ['source_type']),
            'signals_correlated': Counter('xorb_synthesis_signals_correlated_total',
                                        'Total signals correlated'),
            'missions_triggered': Counter('xorb_synthesis_missions_triggered_total',
                                        'Total missions triggered by synthesis'),
            'processing_duration': Histogram('xorb_synthesis_processing_duration_seconds',
                                           'Signal processing duration', ['stage']),
            'source_reliability': Gauge('xorb_synthesis_source_reliability',
                                      'Source reliability score', ['source_id']),
            'correlation_accuracy': Gauge('xorb_synthesis_correlation_accuracy',
                                        'Correlation accuracy score'),
            'active_sources': Gauge('xorb_synthesis_active_sources',
                                  'Number of active intelligence sources'),
            'pending_signals': Gauge('xorb_synthesis_pending_signals',
                                   'Number of pending signals', ['queue'])
        }
    
    async def start_synthesis_engine(self):
        """Start the global intelligence synthesis engine"""
        self.logger.info("ðŸŒ Starting Global Intelligence Synthesis Engine")
        
        self._running = True
        
        # Initialize intelligence sources
        await self._initialize_intelligence_sources()
        
        # Start processing pipelines
        self._processing_tasks = [
            asyncio.create_task(self._ingestion_pipeline()),
            asyncio.create_task(self._normalization_pipeline()),
            asyncio.create_task(self._correlation_pipeline()),
            asyncio.create_task(self._action_pipeline()),
            asyncio.create_task(self._learning_pipeline()),
            asyncio.create_task(self._source_monitoring_loop()),
            asyncio.create_task(self._metrics_collection_loop())
        ]
        
        # Start source pollers
        for source in self.intelligence_sources.values():
            if source.active:
                task = asyncio.create_task(self._source_poller(source))
                self._processing_tasks.append(task)
        
        self.logger.info("ðŸš€ Global Intelligence Synthesis Engine: ACTIVE")
        
        # Update metrics
        self.synthesis_metrics['active_sources'].set(
            len([s for s in self.intelligence_sources.values() if s.active])
        )
    
    async def _initialize_intelligence_sources(self):
        """Initialize and configure intelligence sources"""
        
        # CVE/NVD Feed
        cve_source = IntelligenceSource(
            source_id="cve_nvd",
            source_type=IntelligenceSourceType.CVE_NVD,
            name="CVE/NVD Feed",
            url="https://services.nvd.nist.gov/rest/json/cves/2.0",
            poll_interval=1800,  # 30 minutes
            confidence_weight=0.95,
            reliability_score=0.98,
            priority_keywords=["critical", "high", "remote", "authentication"]
        )
        
        # HackerOne Platform
        hackerone_source = IntelligenceSource(
            source_id="hackerone_main",
            source_type=IntelligenceSourceType.HACKERONE,
            name="HackerOne Public Reports",
            url="https://api.hackerone.com/v1/reports",
            api_key=self._get_hackerone_api_key(),
            poll_interval=600,  # 10 minutes
            confidence_weight=0.85,
            reliability_score=0.90,
            priority_keywords=["bounty", "disclosed", "triaged"]
        )
        
        # OSINT RSS Feeds
        osint_source = IntelligenceSource(
            source_id="osint_threatpost",
            source_type=IntelligenceSourceType.OSINT_RSS,
            name="Threatpost RSS Feed",
            url="https://threatpost.com/feed/",
            poll_interval=900,  # 15 minutes
            confidence_weight=0.7,
            reliability_score=0.75,
            priority_keywords=["vulnerability", "exploit", "zero-day", "apt"]
        )
        
        # Internal Mission History
        internal_source = IntelligenceSource(
            source_id="internal_missions",
            source_type=IntelligenceSourceType.INTERNAL_MISSIONS,
            name="Internal Mission Intelligence",
            url="internal://missions",
            poll_interval=300,  # 5 minutes
            confidence_weight=1.0,
            reliability_score=0.95,
            priority_keywords=["success", "failure", "anomaly", "pattern"]
        )
        
        # Prometheus Alerts
        prometheus_source = IntelligenceSource(
            source_id="prometheus_alerts",
            source_type=IntelligenceSourceType.PROMETHEUS_ALERTS,
            name="Prometheus Alert Manager",
            url="http://prometheus:9090/api/v1/alerts",
            poll_interval=60,  # 1 minute
            confidence_weight=0.9,
            reliability_score=0.95,
            priority_keywords=["critical", "warning", "anomaly", "threshold"]
        )
        
        # Store sources
        sources = [cve_source, hackerone_source, osint_source, internal_source, prometheus_source]
        for source in sources:
            self.intelligence_sources[source.source_id] = source
        
        self.logger.info(f"ðŸ“¡ Initialized {len(sources)} intelligence sources")
    
    async def _source_poller(self, source: IntelligenceSource):
        """Poll a specific intelligence source for new data"""
        
        while self._running:
            try:
                self.logger.debug(f"ðŸ” Polling source: {source.name}")
                
                # Check if source is active and within rate limits
                if not source.active:
                    await asyncio.sleep(source.poll_interval)
                    continue
                
                # Poll the source
                signals = await self._poll_intelligence_source(source)
                
                # Queue signals for ingestion
                for signal in signals:
                    await self.ingestion_queue.put(signal)
                
                # Update source metrics
                source.last_poll = datetime.utcnow()
                source.total_signals += len(signals)
                
                if signals:
                    self.logger.info(f"ðŸ“¥ Ingested {len(signals)} signals from {source.name}")
                
                # Update reliability metrics
                self.synthesis_metrics['source_reliability'].labels(
                    source_id=source.source_id
                ).set(source.reliability_score)
                
                await asyncio.sleep(source.poll_interval)
                
            except Exception as e:
                source.error_count += 1
                self.logger.error(f"âŒ Source polling error: {source.name}", error=str(e))
                
                # Exponential backoff on errors
                backoff_time = min(source.poll_interval * (2 ** min(source.error_count, 5)), 3600)
                await asyncio.sleep(backoff_time)
    
    async def _poll_intelligence_source(self, source: IntelligenceSource) -> List[IntelligenceSignal]:
        """Poll a specific source and return normalized signals"""
        
        signals = []
        
        try:
            if source.source_type == IntelligenceSourceType.CVE_NVD:
                signals = await self._poll_cve_nvd(source)
            elif source.source_type == IntelligenceSourceType.HACKERONE:
                signals = await self._poll_hackerone(source)
            elif source.source_type == IntelligenceSourceType.OSINT_RSS:
                signals = await self._poll_rss_feed(source)
            elif source.source_type == IntelligenceSourceType.INTERNAL_MISSIONS:
                signals = await self._poll_internal_missions(source)
            elif source.source_type == IntelligenceSourceType.PROMETHEUS_ALERTS:
                signals = await self._poll_prometheus_alerts(source)
            
            # Apply source-specific filtering
            signals = self._filter_signals(signals, source)
            
            # Update metrics
            self.synthesis_metrics['signals_ingested'].labels(
                source_type=source.source_type.value
            ).inc(len(signals))
            
        except Exception as e:
            self.logger.error(f"Source polling failed: {source.name}", error=str(e))
        
        return signals
    
    async def _poll_cve_nvd(self, source: IntelligenceSource) -> List[IntelligenceSignal]:
        """Poll CVE/NVD for new vulnerabilities"""
        signals = []
        
        try:
            # Calculate time window for recent CVEs
            since = datetime.utcnow() - timedelta(hours=24)
            
            url = f"{source.url}?lastModStartDate={since.isoformat()}"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=source.headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        for vuln in data.get('vulnerabilities', []):
                            cve_data = vuln.get('cve', {})
                            
                            # Extract key information
                            cve_id = cve_data.get('id', 'Unknown')
                            description = self._extract_cve_description(cve_data)
                            
                            # Calculate priority based on CVSS score
                            priority = self._calculate_cve_priority(cve_data)
                            
                            signal = IntelligenceSignal(
                                signal_id=f"cve_{cve_id}_{int(time.time())}",
                                source_id=source.source_id,
                                source_type=source.source_type,
                                title=f"CVE: {cve_id}",
                                description=description,
                                content={
                                    'cve_id': cve_id,
                                    'cvss_score': self._extract_cvss_score(cve_data),
                                    'affected_products': self._extract_affected_products(cve_data),
                                    'references': cve_data.get('references', [])
                                },
                                raw_data=vuln,
                                timestamp=datetime.fromisoformat(
                                    cve_data.get('published', datetime.utcnow().isoformat())
                                ),
                                priority=priority,
                                signal_type="vulnerability",
                                tags=['cve', 'vulnerability', 'nvd']
                            )
                            
                            signals.append(signal)
                    
        except Exception as e:
            self.logger.error("CVE/NVD polling failed", error=str(e))
        
        return signals
    
    async def _poll_hackerone(self, source: IntelligenceSource) -> List[IntelligenceSignal]:
        """Poll HackerOne for new disclosed reports"""
        signals = []
        
        try:
            if self.hackerone_client:
                reports = await self.hackerone_client.get_disclosed_reports(limit=50)
                
                for report in reports:
                    signal = IntelligenceSignal(
                        signal_id=f"h1_{report.get('id')}_{int(time.time())}",
                        source_id=source.source_id,
                        source_type=source.source_type,
                        title=report.get('title', 'HackerOne Report'),
                        description=report.get('vulnerability_information', ''),
                        content={
                            'report_id': report.get('id'),
                            'severity': report.get('severity', {}).get('rating'),
                            'bounty_amount': report.get('bounty', {}).get('amount'),
                            'program': report.get('program', {}).get('name'),
                            'weakness': report.get('weakness', {})
                        },
                        raw_data=report,
                        timestamp=datetime.fromisoformat(
                            report.get('disclosed_at', datetime.utcnow().isoformat())
                        ),
                        priority=self._calculate_hackerone_priority(report),
                        signal_type="bug_bounty",
                        tags=['hackerone', 'bounty', 'disclosed', 'vulnerability'],
                        url=f"https://hackerone.com/reports/{report.get('id')}"
                    )
                    
                    signals.append(signal)
                    
        except Exception as e:
            self.logger.error("HackerOne polling failed", error=str(e))
        
        return signals
    
    async def _poll_rss_feed(self, source: IntelligenceSource) -> List[IntelligenceSignal]:
        """Poll RSS feeds for threat intelligence"""
        signals = []
        
        try:
            feed = feedparser.parse(source.url)
            
            for entry in feed.entries[:source.batch_size]:
                # Skip old entries
                entry_date = datetime(*entry.published_parsed[:6])
                if entry_date < datetime.utcnow() - timedelta(days=7):
                    continue
                
                signal = IntelligenceSignal(
                    signal_id=f"rss_{hashlib.md5(entry.link.encode()).hexdigest()[:12]}",
                    source_id=source.source_id,
                    source_type=source.source_type,
                    title=entry.title,
                    description=entry.summary,
                    content={
                        'link': entry.link,
                        'author': getattr(entry, 'author', ''),
                        'tags': [tag.term for tag in getattr(entry, 'tags', [])]
                    },
                    raw_data=dict(entry),
                    timestamp=entry_date,
                    priority=self._calculate_rss_priority(entry, source),
                    signal_type="threat_intel",
                    tags=['osint', 'rss', 'threat_intel'],
                    url=entry.link
                )
                
                signals.append(signal)
                
        except Exception as e:
            self.logger.error("RSS feed polling failed", error=str(e))
        
        return signals
    
    async def _poll_internal_missions(self, source: IntelligenceSource) -> List[IntelligenceSignal]:
        """Poll internal mission results for intelligence"""
        signals = []
        
        try:
            # Get recent mission completions from episodic memory
            recent_memories = await self.episodic_memory.query_memories(
                query_type="mission_completion",
                time_range=timedelta(hours=1),
                limit=20
            )
            
            for memory in recent_memories:
                if memory.success_metrics and memory.insights:
                    signal = IntelligenceSignal(
                        signal_id=f"mission_{memory.memory_id}",
                        source_id=source.source_id,
                        source_type=source.source_type,
                        title=f"Mission Intelligence: {memory.operation_type}",
                        description=f"Insights from {memory.operation_type} mission",
                        content={
                            'mission_id': memory.context.get('mission_id'),
                            'success_rate': memory.success_metrics.get('success_rate'),
                            'insights': memory.insights,
                            'patterns': memory.patterns_identified,
                            'performance': memory.performance_data
                        },
                        raw_data=asdict(memory),
                        timestamp=memory.timestamp,
                        priority=self._calculate_internal_priority(memory),
                        signal_type="mission_intelligence",
                        tags=['internal', 'mission', 'performance', 'insights']
                    )
                    
                    signals.append(signal)
                    
        except Exception as e:
            self.logger.error("Internal mission polling failed", error=str(e))
        
        return signals
    
    async def _poll_prometheus_alerts(self, source: IntelligenceSource) -> List[IntelligenceSignal]:
        """Poll Prometheus alerts for system intelligence"""
        signals = []
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(source.url) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        for alert in data.get('data', {}).get('alerts', []):
                            signal = IntelligenceSignal(
                                signal_id=f"alert_{alert.get('fingerprint', str(uuid.uuid4())[:8])}",
                                source_id=source.source_id,
                                source_type=source.source_type,
                                title=f"System Alert: {alert.get('labels', {}).get('alertname')}",
                                description=alert.get('annotations', {}).get('description', ''),
                                content={
                                    'alertname': alert.get('labels', {}).get('alertname'),
                                    'severity': alert.get('labels', {}).get('severity'),
                                    'instance': alert.get('labels', {}).get('instance'),
                                    'value': alert.get('annotations', {}).get('value'),
                                    'state': alert.get('state')
                                },
                                raw_data=alert,
                                timestamp=datetime.fromisoformat(
                                    alert.get('activeAt', datetime.utcnow().isoformat())
                                ),
                                priority=self._calculate_alert_priority(alert),
                                signal_type="system_alert",
                                tags=['prometheus', 'alert', 'system', 'monitoring']
                            )
                            
                            signals.append(signal)
                            
        except Exception as e:
            self.logger.error("Prometheus alerts polling failed", error=str(e))
        
        return signals
    
    async def _ingestion_pipeline(self):
        """Process raw signals through ingestion pipeline"""
        
        while self._running:
            try:
                signals_batch = []
                
                # Collect batch of signals
                for _ in range(self.processing_batch_size):
                    try:
                        signal = await asyncio.wait_for(
                            self.ingestion_queue.get(), timeout=1.0
                        )
                        signals_batch.append(signal)
                    except asyncio.TimeoutError:
                        break
                
                if not signals_batch:
                    await asyncio.sleep(1)
                    continue
                
                # Process batch
                with self.synthesis_metrics['processing_duration'].labels(stage='ingestion').time():
                    processed_signals = await self._process_ingestion_batch(signals_batch)
                
                # Queue for normalization
                for signal in processed_signals:
                    await self.normalization_queue.put(signal)
                
                # Update metrics
                self.synthesis_metrics['pending_signals'].labels(queue='ingestion').set(
                    self.ingestion_queue.qsize()
                )
                
            except Exception as e:
                self.logger.error("Ingestion pipeline error", error=str(e))
                await asyncio.sleep(5)
    
    async def _process_ingestion_batch(self, signals: List[IntelligenceSignal]) -> List[IntelligenceSignal]:
        """Process a batch of signals through ingestion"""
        
        processed_signals = []
        
        for signal in signals:
            try:
                # Deduplication check
                if signal.deduplication_hash in self.deduplication_cache:
                    existing_id = self.deduplication_cache[signal.deduplication_hash]
                    self.logger.debug(f"ðŸ”„ Duplicate signal detected", 
                                    new_id=signal.signal_id[:8],
                                    existing_id=existing_id[:8])
                    continue
                
                # Store signal
                self.raw_signals[signal.signal_id] = signal
                self.deduplication_cache[signal.deduplication_hash] = signal.signal_id
                
                # Initial processing
                signal.status = IntelligenceSignalStatus.NORMALIZED
                signal.processing_history.append({
                    'stage': 'ingestion',
                    'timestamp': datetime.utcnow().isoformat(),
                    'processor': 'ingestion_pipeline'
                })
                
                processed_signals.append(signal)
                
            except Exception as e:
                self.logger.error(f"Signal ingestion failed: {signal.signal_id[:8]}", error=str(e))
        
        # Memory management
        if len(self.raw_signals) > self.max_signals_memory:
            await self._cleanup_old_signals()
        
        return processed_signals
    
    async def get_synthesis_status(self) -> Dict[str, Any]:
        """Get comprehensive synthesis engine status"""
        
        return {
            'synthesis_engine': {
                'status': 'running' if self._running else 'stopped',
                'active_sources': len([s for s in self.intelligence_sources.values() if s.active]),
                'total_sources': len(self.intelligence_sources),
                'raw_signals': len(self.raw_signals),
                'correlated_intelligence': len(self.correlated_intelligence)
            },
            'processing_queues': {
                'ingestion': self.ingestion_queue.qsize(),
                'normalization': self.normalization_queue.qsize(),
                'correlation': self.correlation_queue.qsize(),
                'action': self.action_queue.qsize()
            },
            'intelligence_sources': {
                source_id: {
                    'type': source.source_type.value,
                    'active': source.active,
                    'last_poll': source.last_poll.isoformat() if source.last_poll else None,
                    'total_signals': source.total_signals,
                    'reliability': source.reliability_score,
                    'error_count': source.error_count
                }
                for source_id, source in list(self.intelligence_sources.items())[:10]
            },
            'recent_intelligence': [
                {
                    'intelligence_id': intel.intelligence_id,
                    'title': intel.synthesized_title,
                    'priority': intel.overall_priority.value,
                    'confidence': intel.confidence_score,
                    'threat_level': intel.threat_level,
                    'created_at': intel.created_at.isoformat(),
                    'spawned_missions': len(intel.spawned_missions)
                }
                for intel in list(self.correlated_intelligence.values())[-10:]
            ],
            'performance_metrics': {
                'total_signals_processed': sum(s.total_signals for s in self.intelligence_sources.values()),
                'correlation_accuracy': self._calculate_correlation_accuracy(),
                'average_processing_time': self._calculate_average_processing_time(),
                'source_reliability_avg': self._calculate_average_source_reliability()
            }
        }
    
    # Placeholder methods for additional pipeline stages
    async def _normalization_pipeline(self): pass
    async def _correlation_pipeline(self): pass  
    async def _action_pipeline(self): pass
    async def _learning_pipeline(self): pass
    async def _source_monitoring_loop(self): pass
    async def _metrics_collection_loop(self): pass
    
    # Helper methods (simplified implementations)
    def _get_hackerone_api_key(self) -> Optional[str]: return None
    def _filter_signals(self, signals: List[IntelligenceSignal], source: IntelligenceSource) -> List[IntelligenceSignal]: return signals
    def _extract_cve_description(self, cve_data: Dict) -> str: return cve_data.get('descriptions', [{}])[0].get('value', '')
    def _calculate_cve_priority(self, cve_data: Dict) -> SignalPriority: return SignalPriority.MEDIUM
    def _extract_cvss_score(self, cve_data: Dict) -> float: return 0.0
    def _extract_affected_products(self, cve_data: Dict) -> List[str]: return []
    def _calculate_hackerone_priority(self, report: Dict) -> SignalPriority: return SignalPriority.MEDIUM
    def _calculate_rss_priority(self, entry: Any, source: IntelligenceSource) -> SignalPriority: return SignalPriority.MEDIUM
    def _calculate_internal_priority(self, memory: MemoryRecord) -> SignalPriority: return SignalPriority.MEDIUM
    def _calculate_alert_priority(self, alert: Dict) -> SignalPriority: return SignalPriority.MEDIUM
    async def _cleanup_old_signals(self): pass
    def _calculate_correlation_accuracy(self) -> float: return 0.85
    def _calculate_average_processing_time(self) -> float: return 0.5
    def _calculate_average_source_reliability(self) -> float: return 0.8