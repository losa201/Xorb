name: SOC 2 Compliance Evidence Collection

on:
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM UTC
  workflow_dispatch:    # Manual trigger
    inputs:
      evidence_types:
        description: 'Comma-separated evidence types to collect'
        required: false
        default: 'all'
        type: string
      upload_to_s3:
        description: 'Upload evidence to S3'
        required: false
        default: true
        type: boolean

env:
  AWS_REGION: us-east-1
  S3_BUCKET: xorb-soc2-evidence

jobs:
  collect-evidence:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write  # For AWS OIDC
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_COMPLIANCE_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}
        role-session-name: XorbComplianceEvidence

    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: "3.12"

    - name: Install dependencies
      run: |
        pip install boto3 psycopg2-binary docker prometheus-client structlog

    - name: Install security scanning tools
      run: |
        # Install Trivy
        curl -sSL https://github.com/aquasecurity/trivy/releases/latest/download/trivy_0.45.1_Linux-64bit.tar.gz -o trivy.tar.gz
        tar -xzf trivy.tar.gz
        sudo mv trivy /usr/local/bin/
        
        # Install additional tools
        sudo apt-get update
        sudo apt-get install -y openssl ufw fail2ban

    - name: Connect to production server
      env:
        PROD_SSH_KEY: ${{ secrets.PROD_SSH_PRIVATE_KEY }}
        PROD_HOST: ${{ secrets.PROD_HOST }}
        PROD_USER: ${{ secrets.PROD_USER }}
      run: |
        # Setup SSH key
        mkdir -p ~/.ssh
        echo "$PROD_SSH_KEY" > ~/.ssh/id_rsa
        chmod 600 ~/.ssh/id_rsa
        ssh-keyscan -H $PROD_HOST >> ~/.ssh/known_hosts
        
        # Test connection
        ssh -o StrictHostKeyChecking=no $PROD_USER@$PROD_HOST "echo 'Connected to production server'"

    - name: Collect compliance evidence
      env:
        PROD_SSH_KEY: ${{ secrets.PROD_SSH_PRIVATE_KEY }}
        PROD_HOST: ${{ secrets.PROD_HOST }}
        PROD_USER: ${{ secrets.PROD_USER }}
        POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        COMPLIANCE_S3_BUCKET: ${{ env.S3_BUCKET }}
      run: |
        # Create evidence collection script
        cat > collect_evidence.py << 'EOF'
        import asyncio
        import sys
        import os
        sys.path.append('/opt/xorb')
        from services.compliance.main import ComplianceService
        
        async def main():
            compliance = ComplianceService()
            await compliance.run_daily_compliance_collection()
            print("Evidence collection completed successfully")
        
        if __name__ == "__main__":
            asyncio.run(main())
        EOF
        
        # Copy script to production server and execute
        scp collect_evidence.py $PROD_USER@$PROD_HOST:/tmp/
        ssh $PROD_USER@$PROD_HOST "cd /opt/xorb && python /tmp/collect_evidence.py"

    - name: Collect AWS IAM diff
      run: |
        # Get current IAM state
        aws iam list-users --output json > current_iam_users.json
        aws iam list-roles --output json > current_iam_roles.json
        aws iam list-policies --scope Local --output json > current_iam_policies.json
        
        # Compare with previous state (if available)
        aws s3 cp s3://$S3_BUCKET/iam-state/previous_iam_users.json ./previous_iam_users.json || echo "No previous state found"
        
        # Generate diff report
        python << EOF
        import json
        import sys
        from datetime import datetime
        
        try:
            with open('current_iam_users.json') as f:
                current_users = json.load(f)
            
            try:
                with open('previous_iam_users.json') as f:
                    previous_users = json.load(f)
            except FileNotFoundError:
                previous_users = {'Users': []}
            
            # Simple diff logic
            current_usernames = {user['UserName'] for user in current_users['Users']}
            previous_usernames = {user['UserName'] for user in previous_users['Users']}
            
            diff_report = {
                'timestamp': datetime.utcnow().isoformat(),
                'users_added': list(current_usernames - previous_usernames),
                'users_removed': list(previous_usernames - current_usernames),
                'total_users': len(current_usernames)
            }
            
            with open('iam_diff_report.json', 'w') as f:
                json.dump(diff_report, f, indent=2)
            
            print(f"IAM diff generated: {len(diff_report['users_added'])} added, {len(diff_report['users_removed'])} removed")
            
        except Exception as e:
            print(f"Error generating IAM diff: {e}")
            sys.exit(1)
        EOF

    - name: Generate Docker SBOMs
      run: |
        # Create SBOM collection script
        python << EOF
        import json
        import subprocess
        import docker
        from datetime import datetime
        
        try:
            client = docker.from_env()
            containers = client.containers.list()
            sbom_data = {
                'timestamp': datetime.utcnow().isoformat(),
                'containers': []
            }
            
            for container in containers:
                try:
                    image_name = container.image.tags[0] if container.image.tags else container.id
                    
                    # Run Trivy SBOM generation
                    result = subprocess.run([
                        'trivy', 'image', '--format', 'json', '--quiet', image_name
                    ], capture_output=True, text=True, timeout=300)
                    
                    if result.returncode == 0:
                        trivy_data = json.loads(result.stdout)
                        
                        container_sbom = {
                            'container_id': container.id[:12],
                            'image': image_name,
                            'status': container.status,
                            'sbom_data': trivy_data
                        }
                        
                        sbom_data['containers'].append(container_sbom)
                        print(f"SBOM generated for {image_name}")
                    
                except Exception as e:
                    print(f"Failed to generate SBOM for container {container.id[:12]}: {e}")
            
            # Save SBOM data
            with open('docker_sboms.json', 'w') as f:
                json.dump(sbom_data, f, indent=2)
            
            print(f"Generated SBOMs for {len(sbom_data['containers'])} containers")
            
        except Exception as e:
            print(f"Error generating Docker SBOMs: {e}")
        EOF

    - name: Upload evidence bundles to S3
      if: ${{ inputs.upload_to_s3 != false }}
      run: |
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        
        # Upload IAM evidence
        if [ -f "iam_diff_report.json" ]; then
          aws s3 cp iam_diff_report.json s3://$S3_BUCKET/iam-evidence/iam_diff_$TIMESTAMP.json
          aws s3 cp current_iam_users.json s3://$S3_BUCKET/iam-state/previous_iam_users.json
        fi
        
        # Upload SBOM evidence
        if [ -f "docker_sboms.json" ]; then
          aws s3 cp docker_sboms.json s3://$S3_BUCKET/sbom-evidence/docker_sboms_$TIMESTAMP.json
        fi
        
        # Create summary report
        cat > compliance_summary.json << EOF
        {
          "timestamp": "$(date -Iseconds)",
          "collection_type": "github_action",
          "evidence_collected": {
            "iam_diff": $([ -f "iam_diff_report.json" ] && echo "true" || echo "false"),
            "docker_sboms": $([ -f "docker_sboms.json" ] && echo "true" || echo "false"),
            "production_evidence": true
          },
          "s3_bucket": "$S3_BUCKET",
          "workflow_run_id": "$GITHUB_RUN_ID"
        }
        EOF
        
        aws s3 cp compliance_summary.json s3://$S3_BUCKET/summaries/compliance_summary_$TIMESTAMP.json

    - name: Update compliance dashboard data
      run: |
        # Create dashboard update payload
        python << EOF
        import json
        import boto3
        from datetime import datetime
        
        # Read collected evidence
        evidence_summary = {
            'last_collection': datetime.utcnow().isoformat(),
            'collection_status': 'success',
            'evidence_types_collected': [],
            'failing_controls': 0,
            'next_collection': datetime.utcnow().replace(hour=2, minute=0).isoformat()
        }
        
        # Check what evidence was collected
        import os
        if os.path.exists('iam_diff_report.json'):
            evidence_summary['evidence_types_collected'].append('iam_diff')
        if os.path.exists('docker_sboms.json'):
            evidence_summary['evidence_types_collected'].append('docker_sboms')
        
        # Upload dashboard data
        s3_client = boto3.client('s3')
        s3_client.put_object(
            Bucket='$S3_BUCKET',
            Key='dashboard/latest_status.json',
            Body=json.dumps(evidence_summary, indent=2),
            ContentType='application/json'
        )
        
        print("Dashboard data updated successfully")
        EOF

    - name: Notify on failure
      if: failure()
      run: |
        # Create failure notification
        cat > failure_notification.json << EOF
        {
          "timestamp": "$(date -Iseconds)",
          "status": "failed",
          "workflow_run_id": "$GITHUB_RUN_ID",
          "error": "Compliance evidence collection failed",
          "manual_intervention_required": true
        }
        EOF
        
        # Upload failure notification
        aws s3 cp failure_notification.json s3://$S3_BUCKET/alerts/compliance_failure_$(date +%Y%m%d_%H%M%S).json
        
        # Send alert (if webhook configured)
        if [ -n "${{ secrets.COMPLIANCE_WEBHOOK_URL }}" ]; then
          curl -X POST "${{ secrets.COMPLIANCE_WEBHOOK_URL }}" \
            -H "Content-Type: application/json" \
            -d @failure_notification.json
        fi

  validate-evidence:
    needs: collect-evidence
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_COMPLIANCE_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Validate evidence completeness
      run: |
        # Check if all expected evidence types were collected
        python << EOF
        import boto3
        import json
        from datetime import datetime, timedelta
        
        s3_client = boto3.client('s3')
        bucket = '$S3_BUCKET'
        
        # Check for recent evidence
        today = datetime.utcnow().strftime('%Y%m%d')
        expected_evidence = ['iam-evidence', 'sbom-evidence', 'daily-evidence']
        missing_evidence = []
        
        for evidence_type in expected_evidence:
            try:
                response = s3_client.list_objects_v2(
                    Bucket=bucket,
                    Prefix=f'{evidence_type}/',
                    MaxKeys=1
                )
                
                if 'Contents' not in response:
                    missing_evidence.append(evidence_type)
                else:
                    # Check if evidence is recent (within 25 hours)
                    last_modified = response['Contents'][0]['LastModified']
                    if datetime.utcnow() - last_modified.replace(tzinfo=None) > timedelta(hours=25):
                        missing_evidence.append(f'{evidence_type} (stale)')
                        
            except Exception as e:
                missing_evidence.append(f'{evidence_type} (error: {e})')
        
        if missing_evidence:
            print(f"Missing or stale evidence: {', '.join(missing_evidence)}")
            exit(1)
        else:
            print("All evidence types collected successfully")
        EOF

    - name: Generate compliance report
      run: |
        # Create compliance status report
        python << EOF
        import boto3
        import json
        from datetime import datetime
        
        s3_client = boto3.client('s3')
        bucket = '$S3_BUCKET'
        
        # Get latest evidence
        try:
            response = s3_client.get_object(Bucket=bucket, Key='dashboard/latest_status.json')
            status_data = json.loads(response['Body'].read())
            
            compliance_report = {
                'report_date': datetime.utcnow().strftime('%Y-%m-%d'),
                'overall_status': 'compliant',
                'evidence_collection_status': status_data.get('collection_status', 'unknown'),
                'last_collection': status_data.get('last_collection'),
                'evidence_types': status_data.get('evidence_types_collected', []),
                'failing_controls': status_data.get('failing_controls', 0),
                'next_actions': [],
                'soc2_readiness': 'green' if status_data.get('failing_controls', 0) == 0 else 'yellow'
            }
            
            if compliance_report['failing_controls'] > 0:
                compliance_report['next_actions'].append('Review and remediate failing controls')
                compliance_report['overall_status'] = 'needs_attention'
            
            # Upload compliance report
            s3_client.put_object(
                Bucket=bucket,
                Key=f"reports/compliance_report_{datetime.utcnow().strftime('%Y%m%d')}.json",
                Body=json.dumps(compliance_report, indent=2),
                ContentType='application/json'
            )
            
            print(f"Compliance report generated - Status: {compliance_report['overall_status']}")
            
        except Exception as e:
            print(f"Error generating compliance report: {e}")
        EOF