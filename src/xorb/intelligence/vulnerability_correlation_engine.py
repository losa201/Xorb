"""
Real-time Vulnerability Correlation Engine
Advanced correlation and analysis of vulnerabilities across multiple sources
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Set, Tuple
from dataclasses import dataclass, field, asdict
from enum import Enum
import hashlib
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import DBSCAN
import networkx as nx
import aioredis
import aiohttp

logger = logging.getLogger(__name__)


class VulnerabilitySource(Enum):
    NVD = "nvd"
    MITRE = "mitre"
    EXPLOIT_DB = "exploit_db"
    INTERNAL_SCAN = "internal_scan"
    THREAT_FEED = "threat_feed"
    GITHUB_ADVISORY = "github_advisory"
    VENDOR_ADVISORY = "vendor_advisory"
    SECURITY_BLOG = "security_blog"


class SeverityLevel(Enum):
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"


class VulnerabilityStatus(Enum):
    NEW = "new"
    CONFIRMED = "confirmed"
    IN_PROGRESS = "in_progress"
    PATCHED = "patched"
    MITIGATED = "mitigated"
    FALSE_POSITIVE = "false_positive"
    ACCEPTED_RISK = "accepted_risk"


class ExploitAvailability(Enum):
    NOT_AVAILABLE = "not_available"
    PROOF_OF_CONCEPT = "proof_of_concept"
    FUNCTIONAL = "functional"
    WEAPONIZED = "weaponized"
    IN_THE_WILD = "in_the_wild"


@dataclass
class VulnerabilityData:
    """Core vulnerability data structure"""
    id: str
    cve_id: Optional[str]
    source: VulnerabilitySource
    title: str
    description: str
    severity: SeverityLevel
    cvss_score: float
    cvss_vector: Optional[str]
    
    # Affected systems and software
    affected_products: List[str] = field(default_factory=list)
    affected_versions: List[str] = field(default_factory=list)
    affected_platforms: List[str] = field(default_factory=list)
    
    # Vulnerability characteristics
    vulnerability_type: List[str] = field(default_factory=list)
    attack_vector: Optional[str] = None
    attack_complexity: Optional[str] = None
    privileges_required: Optional[str] = None
    user_interaction: Optional[str] = None
    scope: Optional[str] = None
    
    # Impact assessment
    confidentiality_impact: Optional[str] = None
    integrity_impact: Optional[str] = None
    availability_impact: Optional[str] = None
    
    # Temporal and environmental factors
    exploit_availability: ExploitAvailability = ExploitAvailability.NOT_AVAILABLE
    exploit_maturity: Optional[str] = None
    remediation_level: Optional[str] = None
    report_confidence: Optional[str] = None
    
    # Discovery and timeline
    discovered_date: Optional[datetime] = None
    published_date: Optional[datetime] = None
    last_modified: Optional[datetime] = None
    
    # References and metadata
    references: List[str] = field(default_factory=list)
    tags: List[str] = field(default_factory=list)
    source_data: Dict[str, Any] = field(default_factory=dict)
    
    # Internal tracking
    status: VulnerabilityStatus = VulnerabilityStatus.NEW
    assigned_to: Optional[str] = None
    priority_score: float = 0.0
    correlation_id: Optional[str] = None
    
    def get_hash(self) -> str:
        """Generate hash for vulnerability deduplication"""
        content = f"{self.title}|{self.description}|{','.join(self.affected_products)}"
        return hashlib.sha256(content.encode()).hexdigest()[:16]


@dataclass
class CorrelationCluster:
    """Group of correlated vulnerabilities"""
    id: str
    vulnerabilities: List[VulnerabilityData] = field(default_factory=list)
    cluster_type: str = "similarity"  # similarity, exploit_chain, asset_based
    confidence_score: float = 0.0
    risk_score: float = 0.0
    
    # Cluster characteristics
    primary_cve: Optional[str] = None
    affected_assets: Set[str] = field(default_factory=set)
    common_characteristics: Dict[str, Any] = field(default_factory=dict)
    
    # Threat intelligence
    exploit_chain_possible: bool = False
    active_exploitation: bool = False
    threat_actors: List[str] = field(default_factory=list)
    
    # Timeline and urgency
    created_at: datetime = field(default_factory=datetime.now)
    urgency_level: SeverityLevel = SeverityLevel.MEDIUM
    remediation_priority: int = 5  # 1-10 scale
    
    def add_vulnerability(self, vuln: VulnerabilityData):
        """Add vulnerability to cluster"""
        self.vulnerabilities.append(vuln)
        vuln.correlation_id = self.id
        self._update_cluster_metrics()
        
    def _update_cluster_metrics(self):
        """Update cluster risk and confidence metrics"""
        if not self.vulnerabilities:
            return
            
        # Calculate risk score based on highest CVSS scores
        max_cvss = max(v.cvss_score for v in self.vulnerabilities)
        exploit_factor = 1.5 if any(v.exploit_availability != ExploitAvailability.NOT_AVAILABLE 
                                  for v in self.vulnerabilities) else 1.0
        
        self.risk_score = min(max_cvss * exploit_factor, 10.0)
        
        # Set urgency based on risk score and exploit availability
        if self.risk_score >= 9.0 or self.active_exploitation:
            self.urgency_level = SeverityLevel.CRITICAL
            self.remediation_priority = 1
        elif self.risk_score >= 7.0:
            self.urgency_level = SeverityLevel.HIGH
            self.remediation_priority = 2
        elif self.risk_score >= 4.0:
            self.urgency_level = SeverityLevel.MEDIUM
            self.remediation_priority = 5
        else:
            self.urgency_level = SeverityLevel.LOW
            self.remediation_priority = 8


class VulnerabilityCorrelationEngine:
    """Real-time vulnerability correlation and analysis engine"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.redis_client = None
        self.vulnerability_store: Dict[str, VulnerabilityData] = {}
        self.correlation_clusters: Dict[str, CorrelationCluster] = {}
        
        # ML models and analyzers
        self.text_vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english',
            ngram_range=(1, 2)
        )
        self.clustering_model = DBSCAN(eps=0.3, min_samples=2)
        
        # Graph for relationship analysis
        self.vulnerability_graph = nx.Graph()
        
        # Correlation rules and patterns
        self.correlation_rules = []
        self.exploit_patterns = {}
        
    async def initialize(self):
        """Initialize the correlation engine"""
        logger.info("Initializing Vulnerability Correlation Engine...")
        
        # Initialize Redis for caching and real-time updates
        redis_url = self.config.get('redis_url', 'redis://localhost:6379')
        self.redis_client = await aioredis.from_url(redis_url)
        
        # Load correlation rules and patterns
        await self._load_correlation_rules()
        await self._load_exploit_patterns()
        
        # Initialize ML models with existing data
        await self._train_correlation_models()
        
        # Start real-time processing loop
        asyncio.create_task(self._correlation_processing_loop())
        
        logger.info("Vulnerability Correlation Engine initialized")
        
    async def _load_correlation_rules(self):
        """Load predefined correlation rules"""
        self.correlation_rules = [
            {
                "name": "same_cve_different_sources",
                "description": "Same CVE from different vulnerability sources",
                "pattern": lambda v1, v2: v1.cve_id and v1.cve_id == v2.cve_id and v1.source != v2.source,
                "confidence": 0.95,
                "action": "merge"
            },
            {
                "name": "similar_description_same_product",
                "description": "Similar vulnerabilities in same product",
                "pattern": self._check_similar_description_same_product,
                "confidence": 0.8,
                "action": "cluster"
            },
            {
                "name": "exploit_chain_vulnerability",
                "description": "Vulnerabilities that can be chained for exploitation",
                "pattern": self._check_exploit_chain_potential,
                "confidence": 0.7,
                "action": "chain"
            },
            {
                "name": "same_vendor_product_family",
                "description": "Vulnerabilities in same vendor's product family",
                "pattern": self._check_vendor_product_family,
                "confidence": 0.6,
                "action": "group"
            },
            {
                "name": "similar_attack_vector_timing",
                "description": "Similar attack vectors discovered around same time",
                "pattern": self._check_similar_attack_vector_timing,
                "confidence": 0.65,
                "action": "temporal_cluster"
            }
        ]
        
    async def _load_exploit_patterns(self):
        """Load exploit chain patterns and TTPs"""
        self.exploit_patterns = {
            "privilege_escalation_chain": {
                "description": "Local privilege escalation followed by remote code execution",
                "pattern": ["privilege_escalation", "remote_code_execution"],
                "risk_multiplier": 2.0
            },
            "authentication_bypass_chain": {
                "description": "Authentication bypass leading to data exposure",
                "pattern": ["authentication_bypass", "information_disclosure"],
                "risk_multiplier": 1.8
            },
            "injection_to_rce_chain": {
                "description": "SQL injection leading to remote code execution",
                "pattern": ["sql_injection", "remote_code_execution"],
                "risk_multiplier": 2.2
            },
            "deserialization_chain": {
                "description": "Deserialization vulnerabilities enabling code execution",
                "pattern": ["deserialization", "remote_code_execution"],
                "risk_multiplier": 2.1
            }
        }
        
    async def _train_correlation_models(self):
        """Train ML models with existing vulnerability data"""
        # Load existing vulnerabilities from storage
        existing_data = await self._load_existing_vulnerabilities()
        
        if existing_data:
            # Prepare text data for TF-IDF vectorization
            texts = [f"{v.title} {v.description}" for v in existing_data]
            self.text_vectorizer.fit(texts)
            logger.info(f"Trained text vectorizer on {len(texts)} vulnerability descriptions")
            
    async def _load_existing_vulnerabilities(self) -> List[VulnerabilityData]:
        """Load existing vulnerabilities from storage"""
        # In production, this would load from database
        return list(self.vulnerability_store.values())
        
    async def process_vulnerability(self, vuln_data: Dict[str, Any]) -> VulnerabilityData:
        """Process new vulnerability and perform correlation analysis"""
        # Create vulnerability object
        vulnerability = self._create_vulnerability_from_data(vuln_data)
        
        # Check for duplicates
        existing_vuln = await self._check_for_duplicates(vulnerability)
        if existing_vuln:
            logger.info(f"Duplicate vulnerability detected: {vulnerability.id} -> {existing_vuln.id}")
            return await self._merge_vulnerability_data(existing_vuln, vulnerability)
            
        # Store new vulnerability
        self.vulnerability_store[vulnerability.id] = vulnerability
        await self._store_vulnerability_in_cache(vulnerability)
        
        # Perform correlation analysis
        correlations = await self._find_correlations(vulnerability)
        
        # Process correlations
        for correlation in correlations:
            await self._process_correlation(vulnerability, correlation)
            
        # Update vulnerability graph
        self._update_vulnerability_graph(vulnerability)
        
        # Calculate priority score
        vulnerability.priority_score = await self._calculate_priority_score(vulnerability)
        
        # Trigger real-time notifications if high priority
        if vulnerability.priority_score >= 8.0:
            await self._trigger_high_priority_alert(vulnerability)
            
        logger.info(f"Processed vulnerability {vulnerability.id} with priority {vulnerability.priority_score:.2f}")
        return vulnerability
        
    def _create_vulnerability_from_data(self, data: Dict[str, Any]) -> VulnerabilityData:
        """Create VulnerabilityData object from raw data"""
        return VulnerabilityData(
            id=data.get('id', f"vuln_{datetime.now().timestamp()}"),
            cve_id=data.get('cve_id'),
            source=VulnerabilitySource(data.get('source', 'internal_scan')),
            title=data.get('title', ''),
            description=data.get('description', ''),
            severity=SeverityLevel(data.get('severity', 'medium')),
            cvss_score=float(data.get('cvss_score', 0.0)),
            cvss_vector=data.get('cvss_vector'),
            affected_products=data.get('affected_products', []),
            affected_versions=data.get('affected_versions', []),
            affected_platforms=data.get('affected_platforms', []),
            vulnerability_type=data.get('vulnerability_type', []),
            attack_vector=data.get('attack_vector'),
            attack_complexity=data.get('attack_complexity'),
            privileges_required=data.get('privileges_required'),
            user_interaction=data.get('user_interaction'),
            scope=data.get('scope'),
            confidentiality_impact=data.get('confidentiality_impact'),
            integrity_impact=data.get('integrity_impact'),
            availability_impact=data.get('availability_impact'),
            exploit_availability=ExploitAvailability(data.get('exploit_availability', 'not_available')),
            discovered_date=datetime.fromisoformat(data['discovered_date']) if data.get('discovered_date') else None,
            published_date=datetime.fromisoformat(data['published_date']) if data.get('published_date') else None,
            references=data.get('references', []),
            tags=data.get('tags', []),
            source_data=data.get('source_data', {})
        )
        
    async def _check_for_duplicates(self, vulnerability: VulnerabilityData) -> Optional[VulnerabilityData]:
        """Check for duplicate vulnerabilities"""
        vuln_hash = vulnerability.get_hash()
        
        # Check by CVE ID first
        if vulnerability.cve_id:
            for existing in self.vulnerability_store.values():
                if existing.cve_id == vulnerability.cve_id:
                    return existing
                    
        # Check by content hash
        for existing in self.vulnerability_store.values():
            if existing.get_hash() == vuln_hash:
                return existing
                
        # Check using ML similarity
        return await self._check_ml_similarity(vulnerability)
        
    async def _check_ml_similarity(self, vulnerability: VulnerabilityData) -> Optional[VulnerabilityData]:
        """Check for similar vulnerabilities using ML"""
        if not self.vulnerability_store:
            return None
            
        # Vectorize new vulnerability
        new_text = f"{vulnerability.title} {vulnerability.description}"
        
        try:
            # Get similarity scores with existing vulnerabilities
            existing_texts = [f"{v.title} {v.description}" for v in self.vulnerability_store.values()]
            all_texts = existing_texts + [new_text]
            
            vectors = self.text_vectorizer.transform(all_texts)
            similarity_scores = cosine_similarity(vectors[-1:], vectors[:-1])[0]
            
            # Find most similar vulnerability
            max_similarity_idx = np.argmax(similarity_scores)
            max_similarity = similarity_scores[max_similarity_idx]
            
            # If similarity is very high, consider it a duplicate
            if max_similarity > 0.95:
                existing_vulns = list(self.vulnerability_store.values())
                return existing_vulns[max_similarity_idx]
                
        except Exception as e:
            logger.error(f"ML similarity check failed: {e}")
            
        return None
        
    async def _merge_vulnerability_data(self, existing: VulnerabilityData, 
                                      new: VulnerabilityData) -> VulnerabilityData:
        """Merge data from duplicate vulnerability"""
        # Update existing vulnerability with new information
        if not existing.cve_id and new.cve_id:
            existing.cve_id = new.cve_id
            
        # Merge references
        existing.references.extend([ref for ref in new.references if ref not in existing.references])
        
        # Merge tags
        existing.tags.extend([tag for tag in new.tags if tag not in existing.tags])
        
        # Update CVSS score if higher
        if new.cvss_score > existing.cvss_score:
            existing.cvss_score = new.cvss_score
            existing.cvss_vector = new.cvss_vector
            
        # Merge affected products/versions
        existing.affected_products.extend([p for p in new.affected_products if p not in existing.affected_products])
        existing.affected_versions.extend([v for v in new.affected_versions if v not in existing.affected_versions])
        
        # Update last modified
        existing.last_modified = datetime.now()
        
        # Store merged source data
        existing.source_data[f"{new.source.value}_{datetime.now().timestamp()}"] = new.source_data
        
        await self._store_vulnerability_in_cache(existing)
        return existing
        
    async def _find_correlations(self, vulnerability: VulnerabilityData) -> List[Dict[str, Any]]:
        """Find correlations with existing vulnerabilities"""
        correlations = []
        
        for rule in self.correlation_rules:
            try:
                # Check rule against all existing vulnerabilities
                for existing_id, existing_vuln in self.vulnerability_store.items():
                    if existing_id == vulnerability.id:
                        continue
                        
                    if callable(rule["pattern"]):
                        if rule["pattern"](vulnerability, existing_vuln):
                            correlations.append({
                                "rule": rule["name"],
                                "confidence": rule["confidence"],
                                "action": rule["action"],
                                "target_vulnerability": existing_vuln,
                                "description": rule["description"]
                            })
                    elif rule["pattern"](vulnerability, existing_vuln):
                        correlations.append({
                            "rule": rule["name"],
                            "confidence": rule["confidence"],
                            "action": rule["action"],
                            "target_vulnerability": existing_vuln,
                            "description": rule["description"]
                        })
                        
            except Exception as e:
                logger.error(f"Error applying correlation rule {rule['name']}: {e}")
                
        return correlations
        
    def _check_similar_description_same_product(self, v1: VulnerabilityData, v2: VulnerabilityData) -> bool:
        """Check if vulnerabilities have similar descriptions in same product"""
        if not v1.affected_products or not v2.affected_products:
            return False
            
        # Check for common products
        common_products = set(v1.affected_products) & set(v2.affected_products)
        if not common_products:
            return False
            
        # Check description similarity
        try:
            texts = [f"{v1.title} {v1.description}", f"{v2.title} {v2.description}"]
            vectors = self.text_vectorizer.transform(texts)
            similarity = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]
            return similarity > 0.7
        except:
            return False
            
    def _check_exploit_chain_potential(self, v1: VulnerabilityData, v2: VulnerabilityData) -> bool:
        """Check if vulnerabilities can be chained for exploitation"""
        for pattern_name, pattern_info in self.exploit_patterns.items():
            pattern_types = pattern_info["pattern"]
            
            v1_types = set(v1.vulnerability_type)
            v2_types = set(v2.vulnerability_type)
            
            # Check if vulnerabilities match exploit chain pattern
            if (pattern_types[0] in v1_types and pattern_types[1] in v2_types) or \
               (pattern_types[1] in v1_types and pattern_types[0] in v2_types):
                return True
                
        return False
        
    def _check_vendor_product_family(self, v1: VulnerabilityData, v2: VulnerabilityData) -> bool:
        """Check if vulnerabilities affect same vendor's product family"""
        if not v1.affected_products or not v2.affected_products:
            return False
            
        # Simple vendor extraction (in production, use more sophisticated matching)
        def extract_vendor(products):
            vendors = set()
            for product in products:
                parts = product.lower().split()
                if parts:
                    vendors.add(parts[0])  # First word as vendor
            return vendors
            
        v1_vendors = extract_vendor(v1.affected_products)
        v2_vendors = extract_vendor(v2.affected_products)
        
        return bool(v1_vendors & v2_vendors)
        
    def _check_similar_attack_vector_timing(self, v1: VulnerabilityData, v2: VulnerabilityData) -> bool:
        """Check for similar attack vectors discovered around same time"""
        if not v1.attack_vector or not v2.attack_vector:
            return False
            
        if v1.attack_vector != v2.attack_vector:
            return False
            
        # Check timing (within 30 days)
        if v1.discovered_date and v2.discovered_date:
            time_diff = abs((v1.discovered_date - v2.discovered_date).days)
            return time_diff <= 30
            
        return False
        
    async def _process_correlation(self, vulnerability: VulnerabilityData, 
                                 correlation: Dict[str, Any]):
        """Process identified correlation"""
        action = correlation["action"]
        target_vuln = correlation["target_vulnerability"]
        confidence = correlation["confidence"]
        
        if action == "merge":
            # Merge vulnerabilities
            await self._merge_vulnerability_data(target_vuln, vulnerability)
            
        elif action == "cluster":
            # Add to existing cluster or create new one
            await self._add_to_cluster(vulnerability, target_vuln, "similarity", confidence)
            
        elif action == "chain":
            # Create exploit chain cluster
            await self._add_to_cluster(vulnerability, target_vuln, "exploit_chain", confidence)
            
        elif action == "group":
            # Create vendor/product group
            await self._add_to_cluster(vulnerability, target_vuln, "product_family", confidence)
            
        elif action == "temporal_cluster":
            # Create temporal cluster
            await self._add_to_cluster(vulnerability, target_vuln, "temporal", confidence)
            
    async def _add_to_cluster(self, vuln1: VulnerabilityData, vuln2: VulnerabilityData, 
                            cluster_type: str, confidence: float):
        """Add vulnerabilities to correlation cluster"""
        # Check if either vulnerability is already in a cluster of this type
        existing_cluster = None
        
        for cluster in self.correlation_clusters.values():
            if cluster.cluster_type == cluster_type and \
               (vuln1.correlation_id == cluster.id or vuln2.correlation_id == cluster.id):
                existing_cluster = cluster
                break
                
        if existing_cluster:
            # Add to existing cluster
            if vuln1.correlation_id != existing_cluster.id:
                existing_cluster.add_vulnerability(vuln1)
            if vuln2.correlation_id != existing_cluster.id:
                existing_cluster.add_vulnerability(vuln2)
        else:
            # Create new cluster
            cluster_id = f"cluster_{cluster_type}_{datetime.now().timestamp()}"
            new_cluster = CorrelationCluster(
                id=cluster_id,
                cluster_type=cluster_type,
                confidence_score=confidence
            )
            
            new_cluster.add_vulnerability(vuln1)
            new_cluster.add_vulnerability(vuln2)
            
            # Set special properties for exploit chains
            if cluster_type == "exploit_chain":
                new_cluster.exploit_chain_possible = True
                new_cluster.remediation_priority = max(1, new_cluster.remediation_priority - 2)
                
            self.correlation_clusters[cluster_id] = new_cluster
            
            logger.info(f"Created {cluster_type} cluster {cluster_id} with {len(new_cluster.vulnerabilities)} vulnerabilities")
            
    def _update_vulnerability_graph(self, vulnerability: VulnerabilityData):
        """Update vulnerability relationship graph"""
        # Add vulnerability as node
        self.vulnerability_graph.add_node(
            vulnerability.id,
            cve_id=vulnerability.cve_id,
            severity=vulnerability.severity.value,
            cvss_score=vulnerability.cvss_score,
            products=vulnerability.affected_products
        )
        
        # Add edges to related vulnerabilities
        if vulnerability.correlation_id:
            cluster = self.correlation_clusters.get(vulnerability.correlation_id)
            if cluster:
                for related_vuln in cluster.vulnerabilities:
                    if related_vuln.id != vulnerability.id:
                        self.vulnerability_graph.add_edge(
                            vulnerability.id,
                            related_vuln.id,
                            relationship=cluster.cluster_type,
                            confidence=cluster.confidence_score
                        )
                        
    async def _calculate_priority_score(self, vulnerability: VulnerabilityData) -> float:
        """Calculate vulnerability priority score"""
        score = vulnerability.cvss_score  # Base score
        
        # Exploit availability factor
        exploit_factors = {
            ExploitAvailability.NOT_AVAILABLE: 1.0,
            ExploitAvailability.PROOF_OF_CONCEPT: 1.2,
            ExploitAvailability.FUNCTIONAL: 1.5,
            ExploitAvailability.WEAPONIZED: 1.8,
            ExploitAvailability.IN_THE_WILD: 2.0
        }
        score *= exploit_factors.get(vulnerability.exploit_availability, 1.0)
        
        # Correlation factor
        if vulnerability.correlation_id:
            cluster = self.correlation_clusters.get(vulnerability.correlation_id)
            if cluster:
                if cluster.exploit_chain_possible:
                    score *= 1.5
                if cluster.active_exploitation:
                    score *= 1.8
                    
        # Age factor (newer vulnerabilities get higher priority)
        if vulnerability.published_date:
            days_since_published = (datetime.now() - vulnerability.published_date).days
            if days_since_published <= 7:
                score *= 1.3
            elif days_since_published <= 30:
                score *= 1.1
                
        return min(score, 10.0)
        
    async def _trigger_high_priority_alert(self, vulnerability: VulnerabilityData):
        """Trigger alert for high priority vulnerability"""
        alert_data = {
            "type": "high_priority_vulnerability",
            "vulnerability_id": vulnerability.id,
            "cve_id": vulnerability.cve_id,
            "title": vulnerability.title,
            "severity": vulnerability.severity.value,
            "cvss_score": vulnerability.cvss_score,
            "priority_score": vulnerability.priority_score,
            "affected_products": vulnerability.affected_products,
            "correlation_id": vulnerability.correlation_id,
            "timestamp": datetime.now().isoformat()
        }
        
        # Publish to Redis for real-time notifications
        await self.redis_client.publish("vulnerability_alerts", json.dumps(alert_data))
        
        logger.warning(f"HIGH PRIORITY ALERT: {vulnerability.id} - {vulnerability.title}")
        
    async def _store_vulnerability_in_cache(self, vulnerability: VulnerabilityData):
        """Store vulnerability in Redis cache"""
        key = f"vulnerability:{vulnerability.id}"
        data = json.dumps(asdict(vulnerability), default=str)
        await self.redis_client.setex(key, 86400, data)  # 24 hour expiry
        
    async def _correlation_processing_loop(self):
        """Background loop for continuous correlation processing"""
        while True:
            try:
                # Perform periodic correlation analysis
                await self._periodic_correlation_analysis()
                
                # Update exploit intelligence
                await self._update_exploit_intelligence()
                
                # Clean up old data
                await self._cleanup_old_data()
                
                await asyncio.sleep(300)  # Run every 5 minutes
                
            except Exception as e:
                logger.error(f"Error in correlation processing loop: {e}")
                await asyncio.sleep(60)
                
    async def _periodic_correlation_analysis(self):
        """Perform periodic correlation analysis on all vulnerabilities"""
        # Re-analyze clusters for new patterns
        for cluster in list(self.correlation_clusters.values()):
            await self._analyze_cluster_evolution(cluster)
            
        # Look for new temporal patterns
        await self._analyze_temporal_patterns()
        
    async def _analyze_cluster_evolution(self, cluster: CorrelationCluster):
        """Analyze how vulnerability clusters evolve over time"""
        if len(cluster.vulnerabilities) < 2:
            return
            
        # Check for new exploit developments
        has_new_exploits = any(
            v.exploit_availability != ExploitAvailability.NOT_AVAILABLE and
            v.last_modified and
            (datetime.now() - v.last_modified).days <= 7
            for v in cluster.vulnerabilities
        )
        
        if has_new_exploits and not cluster.active_exploitation:
            cluster.active_exploitation = True
            cluster.urgency_level = SeverityLevel.CRITICAL
            cluster.remediation_priority = 1
            
            # Trigger alert for cluster escalation
            await self._trigger_cluster_escalation_alert(cluster)
            
    async def _analyze_temporal_patterns(self):
        """Analyze temporal patterns in vulnerability discoveries"""
        # Group vulnerabilities by week
        weekly_groups = {}
        
        for vuln in self.vulnerability_store.values():
            if vuln.published_date:
                week_key = vuln.published_date.strftime("%Y-W%U")
                if week_key not in weekly_groups:
                    weekly_groups[week_key] = []
                weekly_groups[week_key].append(vuln)
                
        # Look for weeks with unusual clustering
        for week, vulns in weekly_groups.items():
            if len(vulns) >= 5:  # Threshold for temporal clustering
                await self._create_temporal_cluster(week, vulns)
                
    async def _create_temporal_cluster(self, time_period: str, vulnerabilities: List[VulnerabilityData]):
        """Create temporal correlation cluster"""
        cluster_id = f"temporal_{time_period}_{datetime.now().timestamp()}"
        
        cluster = CorrelationCluster(
            id=cluster_id,
            cluster_type="temporal",
            confidence_score=0.6
        )
        
        for vuln in vulnerabilities:
            cluster.add_vulnerability(vuln)
            
        self.correlation_clusters[cluster_id] = cluster
        logger.info(f"Created temporal cluster {cluster_id} for {time_period} with {len(vulnerabilities)} vulnerabilities")
        
    async def _update_exploit_intelligence(self):
        """Update exploit intelligence from external sources"""
        # In production, this would fetch from threat intelligence feeds
        pass
        
    async def _cleanup_old_data(self):
        """Clean up old vulnerability data and correlations"""
        cutoff_date = datetime.now() - timedelta(days=365)  # Keep 1 year of data
        
        # Remove old vulnerabilities
        old_vulns = [
            vid for vid, vuln in self.vulnerability_store.items()
            if vuln.published_date and vuln.published_date < cutoff_date
        ]
        
        for vid in old_vulns:
            del self.vulnerability_store[vid]
            await self.redis_client.delete(f"vulnerability:{vid}")
            
        logger.info(f"Cleaned up {len(old_vulns)} old vulnerabilities")
        
    async def _trigger_cluster_escalation_alert(self, cluster: CorrelationCluster):
        """Trigger alert for cluster threat escalation"""
        alert_data = {
            "type": "cluster_escalation",
            "cluster_id": cluster.id,
            "cluster_type": cluster.cluster_type,
            "vulnerability_count": len(cluster.vulnerabilities),
            "risk_score": cluster.risk_score,
            "urgency_level": cluster.urgency_level.value,
            "active_exploitation": cluster.active_exploitation,
            "timestamp": datetime.now().isoformat()
        }
        
        await self.redis_client.publish("vulnerability_alerts", json.dumps(alert_data))
        
    async def get_correlation_dashboard_data(self) -> Dict[str, Any]:
        """Get data for correlation dashboard"""
        total_vulns = len(self.vulnerability_store)
        total_clusters = len(self.correlation_clusters)
        
        # Severity distribution
        severity_counts = {}
        for vuln in self.vulnerability_store.values():
            severity = vuln.severity.value
            severity_counts[severity] = severity_counts.get(severity, 0) + 1
            
        # High priority vulnerabilities
        high_priority = [
            v for v in self.vulnerability_store.values()
            if v.priority_score >= 8.0
        ]
        
        # Active clusters
        active_clusters = [
            c for c in self.correlation_clusters.values()
            if c.active_exploitation or c.urgency_level == SeverityLevel.CRITICAL
        ]
        
        return {
            "total_vulnerabilities": total_vulns,
            "total_clusters": total_clusters,
            "severity_distribution": severity_counts,
            "high_priority_count": len(high_priority),
            "active_cluster_count": len(active_clusters),
            "graph_stats": {
                "nodes": self.vulnerability_graph.number_of_nodes(),
                "edges": self.vulnerability_graph.number_of_edges()
            }
        }


# Factory function
def create_vulnerability_correlation_engine(config: Dict[str, Any]) -> VulnerabilityCorrelationEngine:
    """Create and configure vulnerability correlation engine"""
    default_config = {
        "redis_url": "redis://localhost:6379",
        "ml_model_path": "./models",
        "correlation_threshold": 0.7,
        "max_cluster_size": 50
    }
    
    final_config = {**default_config, **config}
    return VulnerabilityCorrelationEngine(final_config)