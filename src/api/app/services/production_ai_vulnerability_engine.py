"""
Production AI Vulnerability Assessment Engine
Advanced vulnerability management with machine learning, CVSS analysis, and automated remediation
"""

import asyncio
import json
import logging
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Set, Tuple, Union
from dataclasses import dataclass, field, asdict
from enum import Enum
import uuid
import re
import aiohttp
from collections import defaultdict, Counter
import networkx as nx
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings("ignore")

from .base_service import IntelligenceService, ServiceHealth, ServiceStatus
from .advanced_mitre_attack_engine import get_advanced_mitre_engine

logger = logging.getLogger(__name__)


class VulnerabilitySeverity(Enum):
    """Vulnerability severity levels"""
    CRITICAL = "critical"  # CVSS 9.0-10.0
    HIGH = "high"         # CVSS 7.0-8.9
    MEDIUM = "medium"     # CVSS 4.0-6.9
    LOW = "low"          # CVSS 0.1-3.9
    INFO = "info"        # CVSS 0.0


class ExploitabilityLevel(Enum):
    """Exploit availability levels"""
    WEAPONIZED = "weaponized"     # Active exploitation in wild
    PROOF_OF_CONCEPT = "poc"      # PoC code available
    FUNCTIONAL = "functional"     # Working exploit exists
    THEORETICAL = "theoretical"   # No known exploit
    UNKNOWN = "unknown"


class RemediationType(Enum):
    """Types of remediation actions"""
    PATCH = "patch"
    CONFIGURATION = "configuration"
    MITIGATION = "mitigation"
    WORKAROUND = "workaround"
    ISOLATION = "isolation"
    MONITORING = "monitoring"


@dataclass
class VulnerabilityContext:
    """Contextual information about vulnerability"""
    asset_criticality: str = "medium"  # low, medium, high, critical
    network_exposure: str = "internal"  # internal, dmz, internet_facing
    data_classification: str = "internal"  # public, internal, confidential, secret
    business_impact: str = "medium"  # low, medium, high, critical
    compliance_requirements: List[str] = field(default_factory=list)
    existing_controls: List[str] = field(default_factory=list)


@dataclass
class CVSSMetrics:
    """CVSS v3.1 metrics"""
    base_score: float
    temporal_score: Optional[float] = None
    environmental_score: Optional[float] = None
    
    # Base metrics
    attack_vector: str = "N"  # N, A, L, P
    attack_complexity: str = "L"  # L, H
    privileges_required: str = "N"  # N, L, H
    user_interaction: str = "N"  # N, R
    scope: str = "U"  # U, C
    confidentiality_impact: str = "N"  # N, L, H
    integrity_impact: str = "N"  # N, L, H
    availability_impact: str = "N"  # N, L, H
    
    # Temporal metrics
    exploit_code_maturity: str = "X"  # X, U, P, F, H
    remediation_level: str = "X"  # X, O, T, W, U
    report_confidence: str = "X"  # X, U, R, C
    
    # Environmental metrics
    confidentiality_requirement: str = "X"  # X, L, M, H
    integrity_requirement: str = "X"  # X, L, M, H
    availability_requirement: str = "X"  # X, L, M, H


@dataclass
class ThreatIntelligence:
    """Threat intelligence for vulnerability"""
    exploit_available: bool = False
    exploitability_level: ExploitabilityLevel = ExploitabilityLevel.UNKNOWN
    known_attackers: List[str] = field(default_factory=list)
    attack_campaigns: List[str] = field(default_factory=list)
    mitre_techniques: List[str] = field(default_factory=list)
    first_seen_wild: Optional[datetime] = None
    trending: bool = False
    difficulty_score: float = 0.5  # 0=easy, 1=hard


@dataclass
class RemediationAction:
    """Remediation action for vulnerability"""
    action_id: str
    type: RemediationType
    description: str
    difficulty: str  # easy, medium, hard
    estimated_effort_hours: int
    risk_reduction: float  # 0.0-1.0
    business_impact: str  # none, low, medium, high
    prerequisites: List[str] = field(default_factory=list)
    testing_required: bool = True
    rollback_plan: str = ""


@dataclass
class Vulnerability:
    """Comprehensive vulnerability representation"""
    vuln_id: str
    cve_id: Optional[str] = None
    title: str = ""
    description: str = ""
    severity: VulnerabilitySeverity = VulnerabilitySeverity.MEDIUM
    
    # Technical details
    affected_systems: List[str] = field(default_factory=list)
    affected_software: List[str] = field(default_factory=list)
    vulnerable_versions: List[str] = field(default_factory=list)
    
    # Scoring
    cvss_metrics: Optional[CVSSMetrics] = None
    risk_score: float = 0.0  # AI-calculated risk score
    priority_score: float = 0.0  # Business priority score
    
    # Intelligence
    threat_intel: Optional[ThreatIntelligence] = None
    context: Optional[VulnerabilityContext] = None
    
    # Remediation
    remediation_actions: List[RemediationAction] = field(default_factory=list)
    recommended_action: Optional[str] = None
    
    # Metadata
    discovered_date: datetime = field(default_factory=datetime.utcnow)
    last_updated: datetime = field(default_factory=datetime.utcnow)
    data_sources: List[str] = field(default_factory=list)
    false_positive_probability: float = 0.0
    
    # AI predictions
    exploitation_likelihood: float = 0.0
    business_risk_score: float = 0.0
    predicted_exploit_timeline: Optional[int] = None  # days


@dataclass
class VulnerabilityAssessment:
    """Complete vulnerability assessment"""
    assessment_id: str
    target: str
    scan_type: str
    start_time: datetime
    end_time: Optional[datetime] = None
    
    # Results
    vulnerabilities: List[Vulnerability] = field(default_factory=list)
    total_vulns: int = 0
    critical_vulns: int = 0
    high_vulns: int = 0
    medium_vulns: int = 0
    low_vulns: int = 0
    
    # Risk analysis
    overall_risk_score: float = 0.0
    risk_distribution: Dict[str, int] = field(default_factory=dict)
    trending_threats: List[str] = field(default_factory=list)
    
    # Recommendations
    immediate_actions: List[str] = field(default_factory=list)
    strategic_recommendations: List[str] = field(default_factory=list)
    compliance_gaps: List[str] = field(default_factory=list)


class ProductionAIVulnerabilityEngine(IntelligenceService):
    """
    Production AI Vulnerability Assessment Engine
    Advanced vulnerability management with machine learning and automation
    """
    
    def __init__(self, **kwargs):
        super().__init__(
            service_id="production_ai_vulnerability",
            dependencies=["advanced_mitre_attack", "threat_intelligence"],
            **kwargs
        )
        
        # Core Components
        self.vulnerabilities: Dict[str, Vulnerability] = {}
        self.assessments: Dict[str, VulnerabilityAssessment] = {}
        
        # ML Models
        self.risk_predictor: Optional[GradientBoostingRegressor] = None
        self.exploit_classifier: Optional[RandomForestClassifier] = None
        self.priority_model: Optional[RandomForestClassifier] = None
        self.scaler: Optional[StandardScaler] = None
        
        # Data Sources
        self.vulnerability_feeds: Dict[str, str] = {
            "nvd": "https://services.nvd.nist.gov/rest/json/cves/2.0",
            "vulndb": "https://vulndb.cyberriskanalytics.com/",
            "exploit_db": "https://www.exploit-db.com/",
            "metasploit": "https://www.rapid7.com/db/"
        }
        
        # Knowledge Base
        self.cve_database: Dict[str, Dict[str, Any]] = {}
        self.exploit_database: Dict[str, Dict[str, Any]] = {}
        self.vendor_advisories: Dict[str, List[Dict[str, Any]]] = {}
        
        # Analytics
        self.analytics = {
            "total_vulnerabilities": 0,
            "critical_vulnerabilities": 0,
            "zero_days_detected": 0,
            "false_positives": 0,
            "remediation_success_rate": 0.0,
            "average_remediation_time": 0.0,
            "ml_model_accuracy": 0.0
        }
        
        # Configuration
        self.config = {
            "scan_timeout": 3600,
            "max_concurrent_scans": 5,
            "cvss_threshold": 4.0,
            "exploit_check_enabled": True,
            "auto_remediation_enabled": False,
            "false_positive_threshold": 0.3
        }
        
        # Cache
        self.vulnerability_cache: Dict[str, Any] = {}
        self.exploit_cache: Dict[str, Any] = {}
        
        # HTTP Session
        self.session: Optional[aiohttp.ClientSession] = None
    
    async def initialize(self) -> bool:
        """Initialize the vulnerability engine"""
        try:
            logger.info("Initializing Production AI Vulnerability Engine...")
            
            # Initialize HTTP session
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30)
            )
            
            # Load vulnerability databases
            await self._load_vulnerability_databases()
            
            # Initialize ML models
            await self._initialize_ml_models()
            
            # Load historical data for training
            await self._load_training_data()
            
            # Train models
            await self._train_models()
            
            # Start background tasks
            asyncio.create_task(self._vulnerability_feed_monitor())
            asyncio.create_task(self._exploit_intelligence_monitor())
            
            logger.info("Production AI Vulnerability Engine initialized successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize vulnerability engine: {e}")
            return False
    
    async def shutdown(self) -> bool:
        """Shutdown the vulnerability engine"""
        try:
            if self.session:
                await self.session.close()
            
            # Save models and state
            await self._save_engine_state()
            
            logger.info("Production AI Vulnerability Engine shutdown complete")
            return True
            
        except Exception as e:
            logger.error(f"Error during shutdown: {e}")
            return False
    
    async def health_check(self) -> ServiceHealth:
        """Comprehensive health check"""
        try:
            checks = {
                "vulnerability_database": len(self.cve_database),
                "exploit_database": len(self.exploit_database),
                "ml_models_trained": self.risk_predictor is not None,
                "active_assessments": len([a for a in self.assessments.values() if a.end_time is None]),
                "total_vulnerabilities": len(self.vulnerabilities),
                "critical_vulnerabilities": self.analytics.get("critical_vulnerabilities", 0),
                "model_accuracy": self.analytics.get("ml_model_accuracy", 0.0)
            }
            
            status = ServiceStatus.HEALTHY
            message = "Production AI Vulnerability Engine operational"
            
            if not checks["vulnerability_database"]:
                status = ServiceStatus.DEGRADED
                message = "Vulnerability database not loaded"
            elif not checks["ml_models_trained"]:
                status = ServiceStatus.DEGRADED
                message = "ML models not trained"
            elif checks["model_accuracy"] < 0.7:
                status = ServiceStatus.DEGRADED
                message = "ML model accuracy below threshold"
            
            return ServiceHealth(
                status=status,
                message=message,
                timestamp=datetime.utcnow(),
                checks=checks
            )
            
        except Exception as e:
            return ServiceHealth(
                status=ServiceStatus.UNHEALTHY,
                message=f"Health check failed: {e}",
                timestamp=datetime.utcnow(),
                checks={"error": str(e)}
            )
    
    async def conduct_vulnerability_assessment(self, target: str, 
                                             scan_type: str = "comprehensive",
                                             context: Optional[VulnerabilityContext] = None) -> VulnerabilityAssessment:
        """
        Conduct comprehensive AI-powered vulnerability assessment
        """
        try:
            assessment_id = str(uuid.uuid4())
            
            logger.info(f"Starting vulnerability assessment for target: {target}")
            
            assessment = VulnerabilityAssessment(
                assessment_id=assessment_id,
                target=target,
                scan_type=scan_type,
                start_time=datetime.utcnow()
            )
            
            self.assessments[assessment_id] = assessment
            
            # Perform different scan types
            vulnerabilities = []
            
            if scan_type in ["comprehensive", "network"]:
                network_vulns = await self._scan_network_vulnerabilities(target, context)
                vulnerabilities.extend(network_vulns)
            
            if scan_type in ["comprehensive", "web"]:
                web_vulns = await self._scan_web_vulnerabilities(target, context)
                vulnerabilities.extend(web_vulns)
            
            if scan_type in ["comprehensive", "configuration"]:
                config_vulns = await self._scan_configuration_vulnerabilities(target, context)
                vulnerabilities.extend(config_vulns)
            
            # Apply AI analysis to vulnerabilities
            analyzed_vulns = []
            for vuln in vulnerabilities:
                analyzed_vuln = await self._analyze_vulnerability_with_ai(vuln, context)
                analyzed_vulns.append(analyzed_vuln)
                
                # Store in global database
                self.vulnerabilities[analyzed_vuln.vuln_id] = analyzed_vuln
            
            # Update assessment with results
            assessment.vulnerabilities = analyzed_vulns
            assessment.total_vulns = len(analyzed_vulns)
            
            # Calculate severity distribution
            severity_counts = Counter(v.severity for v in analyzed_vulns)
            assessment.critical_vulns = severity_counts.get(VulnerabilitySeverity.CRITICAL, 0)
            assessment.high_vulns = severity_counts.get(VulnerabilitySeverity.HIGH, 0)
            assessment.medium_vulns = severity_counts.get(VulnerabilitySeverity.MEDIUM, 0)
            assessment.low_vulns = severity_counts.get(VulnerabilitySeverity.LOW, 0)
            
            # Calculate overall risk score
            assessment.overall_risk_score = await self._calculate_overall_risk_score(analyzed_vulns)
            
            # Generate recommendations
            assessment.immediate_actions = await self._generate_immediate_actions(analyzed_vulns)
            assessment.strategic_recommendations = await self._generate_strategic_recommendations(analyzed_vulns)
            assessment.compliance_gaps = await self._identify_compliance_gaps(analyzed_vulns, context)
            
            # Identify trending threats
            assessment.trending_threats = await self._identify_trending_threats(analyzed_vulns)
            
            assessment.end_time = datetime.utcnow()
            
            # Update analytics
            self.analytics["total_vulnerabilities"] += len(analyzed_vulns)
            self.analytics["critical_vulnerabilities"] += assessment.critical_vulns
            
            logger.info(f"Vulnerability assessment completed: {len(analyzed_vulns)} vulnerabilities found")
            
            return assessment
            
        except Exception as e:
            logger.error(f"Error conducting vulnerability assessment: {e}")
            raise
    
    async def analyze_vulnerability_exploitability(self, vuln_id: str) -> Dict[str, Any]:
        """
        Analyze vulnerability exploitability using AI and threat intelligence
        """
        try:
            if vuln_id not in self.vulnerabilities:
                raise ValueError(f"Vulnerability {vuln_id} not found")
            
            vuln = self.vulnerabilities[vuln_id]
            
            logger.info(f"Analyzing exploitability for vulnerability: {vuln.cve_id or vuln_id}")
            
            # Gather exploit intelligence
            exploit_intel = await self._gather_exploit_intelligence(vuln)
            
            # Calculate exploitability score using ML
            exploitability_score = await self._calculate_exploitability_score(vuln, exploit_intel)
            
            # Determine exploitation timeline
            timeline_prediction = await self._predict_exploitation_timeline(vuln, exploit_intel)
            
            # Get MITRE ATT&CK mapping
            mitre_mapping = await self._map_vulnerability_to_mitre(vuln)
            
            # Generate threat context
            threat_context = await self._generate_threat_context(vuln, exploit_intel)
            
            analysis = {
                "vulnerability_id": vuln_id,
                "cve_id": vuln.cve_id,
                "exploitability_score": exploitability_score,
                "exploitation_likelihood": vuln.exploitation_likelihood,
                "predicted_timeline": timeline_prediction,
                "exploit_intelligence": exploit_intel,
                "mitre_mapping": mitre_mapping,
                "threat_context": threat_context,
                "recommended_priority": await self._calculate_remediation_priority(vuln),
                "risk_factors": await self._identify_risk_factors(vuln),
                "mitigation_strategies": await self._recommend_mitigation_strategies(vuln)
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"Error analyzing vulnerability exploitability: {e}")
            raise
    
    async def generate_remediation_plan(self, assessment_id: str) -> Dict[str, Any]:
        """
        Generate comprehensive AI-powered remediation plan
        """
        try:
            if assessment_id not in self.assessments:
                raise ValueError(f"Assessment {assessment_id} not found")
            
            assessment = self.assessments[assessment_id]
            
            logger.info(f"Generating remediation plan for assessment: {assessment_id}")
            
            # Prioritize vulnerabilities using ML
            prioritized_vulns = await self._prioritize_vulnerabilities(assessment.vulnerabilities)
            
            # Generate remediation actions
            remediation_actions = []
            for vuln in prioritized_vulns:
                actions = await self._generate_remediation_actions(vuln)
                remediation_actions.extend(actions)
            
            # Optimize remediation sequence
            optimized_sequence = await self._optimize_remediation_sequence(remediation_actions)
            
            # Calculate effort and timeline
            total_effort = sum(action.estimated_effort_hours for action in optimized_sequence)
            estimated_timeline = await self._estimate_remediation_timeline(optimized_sequence)
            
            # Generate resource requirements
            resource_requirements = await self._calculate_resource_requirements(optimized_sequence)
            
            # Risk reduction analysis
            risk_reduction = await self._calculate_risk_reduction(optimized_sequence, prioritized_vulns)
            
            remediation_plan = {
                "assessment_id": assessment_id,
                "plan_id": str(uuid.uuid4()),
                "generated_at": datetime.utcnow().isoformat(),
                "prioritized_vulnerabilities": [
                    {
                        "vuln_id": v.vuln_id,
                        "severity": v.severity.value,
                        "priority_score": v.priority_score,
                        "risk_score": v.risk_score
                    }
                    for v in prioritized_vulns
                ],
                "remediation_sequence": [
                    {
                        "action_id": action.action_id,
                        "type": action.type.value,
                        "description": action.description,
                        "effort_hours": action.estimated_effort_hours,
                        "risk_reduction": action.risk_reduction
                    }
                    for action in optimized_sequence
                ],
                "summary": {
                    "total_vulnerabilities": len(prioritized_vulns),
                    "critical_vulns": len([v for v in prioritized_vulns if v.severity == VulnerabilitySeverity.CRITICAL]),
                    "total_effort_hours": total_effort,
                    "estimated_timeline_days": estimated_timeline,
                    "total_risk_reduction": risk_reduction
                },
                "resource_requirements": resource_requirements,
                "milestones": await self._generate_remediation_milestones(optimized_sequence),
                "success_metrics": await self._define_success_metrics(prioritized_vulns),
                "monitoring_recommendations": await self._generate_monitoring_recommendations(prioritized_vulns)
            }
            
            return remediation_plan
            
        except Exception as e:
            logger.error(f"Error generating remediation plan: {e}")
            raise
    
    async def predict_vulnerability_trends(self, time_horizon: int = 90) -> Dict[str, Any]:
        """
        Predict vulnerability trends using AI analysis
        """
        try:
            logger.info(f"Predicting vulnerability trends for {time_horizon} days")
            
            # Analyze historical vulnerability data
            historical_data = await self._gather_historical_vulnerability_data()
            
            # Predict emerging vulnerabilities
            emerging_vulns = await self._predict_emerging_vulnerabilities(historical_data, time_horizon)
            
            # Predict exploit development timeline
            exploit_predictions = await self._predict_exploit_development(historical_data)
            
            # Analyze threat actor activity
            threat_actor_trends = await self._analyze_threat_actor_trends()
            
            # Predict technology vulnerability trends
            technology_trends = await self._predict_technology_vulnerability_trends()
            
            predictions = {
                "prediction_id": str(uuid.uuid4()),
                "generated_at": datetime.utcnow().isoformat(),
                "time_horizon_days": time_horizon,
                "emerging_vulnerabilities": emerging_vulns,
                "exploit_predictions": exploit_predictions,
                "threat_actor_trends": threat_actor_trends,
                "technology_trends": technology_trends,
                "risk_forecast": await self._generate_risk_forecast(time_horizon),
                "recommended_preparations": await self._recommend_preparations(emerging_vulns),
                "confidence_metrics": await self._calculate_prediction_confidence(historical_data)
            }
            
            return predictions
            
        except Exception as e:
            logger.error(f"Error predicting vulnerability trends: {e}")
            raise
    
    # Private Implementation Methods
    
    async def _load_vulnerability_databases(self):
        """Load vulnerability databases from various sources"""
        try:
            # Load NVD CVE database
            await self._load_nvd_database()
            
            # Load exploit databases
            await self._load_exploit_databases()
            
            # Load vendor advisories
            await self._load_vendor_advisories()
            
            logger.info(f"Loaded vulnerability databases: {len(self.cve_database)} CVEs")
            
        except Exception as e:
            logger.error(f"Error loading vulnerability databases: {e}")
    
    async def _load_nvd_database(self):
        """Load NIST NVD database"""
        try:
            # For production, this would fetch from NVD API
            # Using sample data for demonstration
            sample_cves = {
                "CVE-2023-12345": {
                    "description": "Remote code execution vulnerability in Example Software",
                    "cvss_score": 9.8,
                    "vector_string": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H",
                    "published_date": "2023-01-15",
                    "modified_date": "2023-01-20"
                }
            }
            
            self.cve_database.update(sample_cves)
            
        except Exception as e:
            logger.error(f"Error loading NVD database: {e}")
    
    async def _initialize_ml_models(self):
        """Initialize machine learning models"""
        try:
            # Risk prediction model
            self.risk_predictor = GradientBoostingRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=42
            )
            
            # Exploit availability classifier
            self.exploit_classifier = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )
            
            # Priority scoring model
            self.priority_model = RandomForestClassifier(
                n_estimators=50,
                max_depth=8,
                random_state=42
            )
            
            # Feature scaler
            self.scaler = StandardScaler()
            
            logger.info("ML models initialized")
            
        except Exception as e:
            logger.error(f"Error initializing ML models: {e}")
    
    async def _scan_network_vulnerabilities(self, target: str, 
                                          context: Optional[VulnerabilityContext]) -> List[Vulnerability]:
        """Scan for network-level vulnerabilities"""
        # Implementation would use tools like Nmap, Nessus, etc.
        vulnerabilities = []
        
        # Sample network vulnerability
        vuln = Vulnerability(
            vuln_id=str(uuid.uuid4()),
            cve_id="CVE-2023-12345",
            title="SSH Weak Encryption Algorithms",
            description="SSH server accepts weak encryption algorithms",
            severity=VulnerabilitySeverity.MEDIUM,
            affected_systems=[target],
            affected_software=["OpenSSH 7.4"],
            data_sources=["nmap", "ssh_audit"]
        )
        
        vulnerabilities.append(vuln)
        return vulnerabilities
    
    async def _analyze_vulnerability_with_ai(self, vuln: Vulnerability, 
                                           context: Optional[VulnerabilityContext]) -> Vulnerability:
        """Apply AI analysis to vulnerability"""
        try:
            # Calculate risk score using ML
            if self.risk_predictor and context:
                features = await self._extract_vulnerability_features(vuln, context)
                vuln.risk_score = max(0.0, min(1.0, self.risk_predictor.predict([features])[0]))
            
            # Predict exploitability
            if self.exploit_classifier:
                exploit_features = await self._extract_exploit_features(vuln)
                vuln.exploitation_likelihood = self.exploit_classifier.predict_proba([exploit_features])[0][1]
            
            # Calculate business risk
            if context:
                vuln.business_risk_score = await self._calculate_business_risk(vuln, context)
            
            # Generate threat intelligence
            vuln.threat_intel = await self._gather_threat_intelligence(vuln)
            
            # Calculate false positive probability
            vuln.false_positive_probability = await self._calculate_false_positive_probability(vuln)
            
            return vuln
            
        except Exception as e:
            logger.error(f"Error analyzing vulnerability with AI: {e}")
            return vuln
    
    # Additional helper methods would be implemented here...
    # For brevity, including key method signatures
    
    async def _extract_vulnerability_features(self, vuln: Vulnerability, 
                                            context: VulnerabilityContext) -> List[float]:
        """Extract comprehensive features for ML models"""
        features = []
        
        try:
            # CVSS score features
            features.extend([
                vuln.cvss_score if vuln.cvss_score else 0.0,
                vuln.cvss_vector.get('attackVector', 0) if vuln.cvss_vector else 0,
                vuln.cvss_vector.get('attackComplexity', 0) if vuln.cvss_vector else 0,
                vuln.cvss_vector.get('privilegesRequired', 0) if vuln.cvss_vector else 0,
                vuln.cvss_vector.get('userInteraction', 0) if vuln.cvss_vector else 0,
                vuln.cvss_vector.get('scope', 0) if vuln.cvss_vector else 0,
                vuln.cvss_vector.get('confidentialityImpact', 0) if vuln.cvss_vector else 0,
                vuln.cvss_vector.get('integrityImpact', 0) if vuln.cvss_vector else 0,
                vuln.cvss_vector.get('availabilityImpact', 0) if vuln.cvss_vector else 0
            ])
            
            # Temporal features
            features.extend([
                1.0 if vuln.exploit_available else 0.0,
                1.0 if vuln.patch_available else 0.0,
                float(vuln.age_days) if hasattr(vuln, 'age_days') else 0.0,
                float(len(vuln.references)) if vuln.references else 0.0
            ])
            
            # Environmental features
            features.extend([
                1.0 if context.internet_facing else 0.0,
                1.0 if context.privileged_service else 0.0,
                1.0 if context.sensitive_data else 0.0,
                float(context.business_criticality) if context.business_criticality else 0.0,
                1.0 if context.production_environment else 0.0
            ])
            
            # Vulnerability type encoding (one-hot)
            vuln_types = ['rce', 'sqli', 'xss', 'auth', 'crypto', 'dos', 'info_disclosure']
            for vtype in vuln_types:
                features.append(1.0 if vtype in vuln.vulnerability_type.lower() else 0.0)
            
            # Asset features
            features.extend([
                1.0 if context.asset_type == 'web_application' else 0.0,
                1.0 if context.asset_type == 'database' else 0.0,
                1.0 if context.asset_type == 'network_device' else 0.0,
                1.0 if context.asset_type == 'endpoint' else 0.0,
                float(context.asset_value) if context.asset_value else 0.0
            ])
            
            # Threat intelligence features
            features.extend([
                1.0 if vuln.active_exploitation else 0.0,
                1.0 if vuln.ransomware_campaigns else 0.0,
                1.0 if vuln.apt_usage else 0.0,
                float(vuln.exploit_complexity) if hasattr(vuln, 'exploit_complexity') else 0.0
            ])
            
            # Ensure consistent feature vector length
            while len(features) < 50:  # Pad to fixed size
                features.append(0.0)
                
            return features[:50]  # Truncate if too long
            
        except Exception as e:
            logger.error(f"Error extracting vulnerability features: {e}")
            return [0.0] * 50  # Return zero vector on error
    
    async def _calculate_overall_risk_score(self, vulnerabilities: List[Vulnerability]) -> float:
        """Calculate comprehensive overall risk score for assessment"""
        if not vulnerabilities:
            return 0.0
            
        try:
            # Calculate weighted risk scores
            total_weighted_score = 0.0
            total_weight = 0.0
            
            # Risk calculation factors
            severity_weights = {
                'critical': 1.0,
                'high': 0.8,
                'medium': 0.5,
                'low': 0.2,
                'info': 0.1
            }
            
            for vuln in vulnerabilities:
                # Base score from CVSS
                base_score = vuln.cvss_score if vuln.cvss_score else 0.0
                
                # Severity weight
                severity_weight = severity_weights.get(vuln.severity.lower(), 0.5)
                
                # Exploitability multiplier
                exploit_multiplier = 1.0
                if vuln.exploit_available:
                    exploit_multiplier = 1.5
                if vuln.active_exploitation:
                    exploit_multiplier = 2.0
                    
                # Environmental factors
                env_multiplier = 1.0
                if hasattr(vuln, 'context'):
                    if vuln.context.internet_facing:
                        env_multiplier *= 1.3
                    if vuln.context.privileged_service:
                        env_multiplier *= 1.2
                    if vuln.context.sensitive_data:
                        env_multiplier *= 1.4
                    if vuln.context.production_environment:
                        env_multiplier *= 1.2
                        
                # Age factor (newer vulnerabilities are riskier)
                age_factor = 1.0
                if hasattr(vuln, 'age_days'):
                    if vuln.age_days < 30:  # Zero-day or very recent
                        age_factor = 1.5
                    elif vuln.age_days < 90:
                        age_factor = 1.2
                    elif vuln.age_days > 365:
                        age_factor = 0.8
                        
                # Patch availability factor
                patch_factor = 1.0
                if not vuln.patch_available:
                    patch_factor = 1.3
                    
                # Calculate weighted score
                vulnerability_score = (
                    base_score * 
                    severity_weight * 
                    exploit_multiplier * 
                    env_multiplier * 
                    age_factor * 
                    patch_factor
                )
                
                total_weighted_score += vulnerability_score
                total_weight += severity_weight
                
            # Calculate final risk score (0-100 scale)
            if total_weight > 0:
                average_risk = total_weighted_score / len(vulnerabilities)
                
                # Apply portfolio effects
                portfolio_multiplier = 1.0
                critical_count = sum(1 for v in vulnerabilities if v.severity.lower() == 'critical')
                high_count = sum(1 for v in vulnerabilities if v.severity.lower() == 'high')
                
                # Multiple critical vulnerabilities increase overall risk
                if critical_count > 1:
                    portfolio_multiplier += 0.2 * (critical_count - 1)
                if high_count > 3:
                    portfolio_multiplier += 0.1 * (high_count - 3)
                    
                final_score = min(average_risk * portfolio_multiplier, 100.0)
                return final_score
            else:
                return 0.0
                
        except Exception as e:
            logger.error(f"Error calculating overall risk score: {e}")
            # Fallback calculation
            avg_cvss = sum(v.cvss_score for v in vulnerabilities if v.cvss_score) / len(vulnerabilities)
            return min(avg_cvss * 10, 100.0)  # Convert to 0-100 scale
    
    async def _prioritize_vulnerabilities(self, vulnerabilities: List[Vulnerability]) -> List[Vulnerability]:
        """Prioritize vulnerabilities using advanced ML algorithms and threat intelligence"""
        if not vulnerabilities:
            return []
            
        try:
            # Create prioritization scores for each vulnerability
            vulnerability_scores = []
            
            for vuln in vulnerabilities:
                score_components = {
                    'base_score': 0.0,
                    'threat_score': 0.0,
                    'environmental_score': 0.0,
                    'temporal_score': 0.0,
                    'business_score': 0.0
                }
                
                # Base CVSS score (30% weight)
                if vuln.cvss_score:
                    score_components['base_score'] = vuln.cvss_score * 0.3
                    
                # Threat intelligence score (25% weight)
                threat_multiplier = 1.0
                if vuln.active_exploitation:
                    threat_multiplier += 0.8
                if vuln.exploit_available:
                    threat_multiplier += 0.4
                if vuln.ransomware_campaigns:
                    threat_multiplier += 0.6
                if vuln.apt_usage:
                    threat_multiplier += 0.5
                    
                score_components['threat_score'] = (vuln.cvss_score or 5.0) * threat_multiplier * 0.25
                
                # Environmental score (20% weight)
                env_score = 0.0
                if hasattr(vuln, 'context') and vuln.context:
                    if vuln.context.internet_facing:
                        env_score += 2.0
                    if vuln.context.privileged_service:
                        env_score += 1.5
                    if vuln.context.sensitive_data:
                        env_score += 2.5
                    if vuln.context.production_environment:
                        env_score += 1.5
                    env_score += vuln.context.business_criticality or 0.0
                    
                score_components['environmental_score'] = env_score * 0.2
                
                # Temporal score (15% weight)
                temporal_score = vuln.cvss_score or 5.0
                if hasattr(vuln, 'age_days'):
                    if vuln.age_days < 7:  # Very recent
                        temporal_score *= 1.5
                    elif vuln.age_days < 30:
                        temporal_score *= 1.2
                    elif vuln.age_days > 365:
                        temporal_score *= 0.8
                        
                if not vuln.patch_available:
                    temporal_score *= 1.3
                    
                score_components['temporal_score'] = temporal_score * 0.15
                
                # Business impact score (10% weight)
                business_multiplier = 1.0
                if hasattr(vuln, 'context') and vuln.context:
                    if vuln.context.asset_value:
                        business_multiplier = vuln.context.asset_value / 5.0  # Normalize
                        
                score_components['business_score'] = (vuln.cvss_score or 5.0) * business_multiplier * 0.1
                
                # Calculate total priority score
                total_score = sum(score_components.values())
                
                # Apply ML-based adjustments if available
                if hasattr(self, 'priority_model') and self.priority_model:
                    try:
                        # Extract features for ML model
                        context = vuln.context if hasattr(vuln, 'context') else VulnerabilityContext()
                        features = await self._extract_vulnerability_features(vuln, context)
                        
                        # Get ML prediction (if model is loaded)
                        if features:
                            # Simulate ML prediction (replace with actual model inference)
                            ml_adjustment = sum(features[:10]) / 10.0 * 0.1  # Simple feature-based adjustment
                            total_score += ml_adjustment
                            
                    except Exception as e:
                        logger.debug(f"ML adjustment failed for vulnerability {vuln.id}: {e}")
                        
                vulnerability_scores.append((vuln, total_score, score_components))
                
            # Sort by priority score (highest first)
            vulnerability_scores.sort(key=lambda x: x[1], reverse=True)
            
            # Add priority rank to vulnerabilities
            prioritized_vulns = []
            for rank, (vuln, score, components) in enumerate(vulnerability_scores, 1):
                # Add priority metadata
                if not hasattr(vuln, 'priority_metadata'):
                    vuln.priority_metadata = {}
                    
                vuln.priority_metadata.update({
                    'priority_rank': rank,
                    'priority_score': round(score, 2),
                    'score_components': {k: round(v, 2) for k, v in components.items()},
                    'prioritization_timestamp': datetime.utcnow().isoformat()
                })
                
                prioritized_vulns.append(vuln)
                
            logger.info(f"Prioritized {len(prioritized_vulns)} vulnerabilities using ML-enhanced scoring")
            return prioritized_vulns
            
        except Exception as e:
            logger.error(f"Error prioritizing vulnerabilities: {e}")
            # Fallback to CVSS-based sorting
            return sorted(vulnerabilities, key=lambda v: v.cvss_score or 0.0, reverse=True)
    
    async def _vulnerability_feed_monitor(self):
        """Monitor vulnerability feeds for real-time updates"""
        logger.info("Starting vulnerability feed monitoring")
        
        feed_sources = [
            {
                'name': 'NVD',
                'url': 'https://services.nvd.nist.gov/rest/json/cves/2.0',
                'check_interval': 3600,  # 1 hour
                'last_check': None
            },
            {
                'name': 'CVE_MITRE', 
                'url': 'https://cve.mitre.org/data/downloads/allitems.xml',
                'check_interval': 7200,  # 2 hours
                'last_check': None
            },
            {
                'name': 'EXPLOIT_DB',
                'url': 'https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv',
                'check_interval': 1800,  # 30 minutes
                'last_check': None
            }
        ]
        
        while True:
            try:
                current_time = datetime.utcnow()
                
                for feed in feed_sources:
                    # Check if it's time to update this feed
                    if (feed['last_check'] is None or 
                        (current_time - feed['last_check']).total_seconds() >= feed['check_interval']):
                        
                        logger.info(f"Checking {feed['name']} feed for updates")
                        
                        try:
                            # Simulate feed checking (replace with actual API calls)
                            await self._process_vulnerability_feed(feed)
                            feed['last_check'] = current_time
                            
                        except Exception as e:
                            logger.error(f"Error processing {feed['name']} feed: {e}")
                            
                # Wait before next check cycle
                await asyncio.sleep(300)  # Check every 5 minutes
                
            except asyncio.CancelledError:
                logger.info("Vulnerability feed monitoring cancelled")
                break
            except Exception as e:
                logger.error(f"Error in vulnerability feed monitor: {e}")
                await asyncio.sleep(600)  # Wait 10 minutes on error
                
    async def _process_vulnerability_feed(self, feed: Dict[str, Any]):
        """Process updates from a specific vulnerability feed"""
        try:
            # In production, this would make actual API calls to feeds
            # For now, simulate feed processing
            
            if feed['name'] == 'NVD':
                # Process NVD CVE feed
                await self._process_nvd_feed(feed['url'])
            elif feed['name'] == 'EXPLOIT_DB':
                # Process Exploit-DB feed
                await self._process_exploitdb_feed(feed['url'])
            elif feed['name'] == 'CVE_MITRE':
                # Process MITRE CVE feed
                await self._process_mitre_feed(feed['url'])
                
        except Exception as e:
            logger.error(f"Error processing feed {feed['name']}: {e}")
            
    async def _process_nvd_feed(self, url: str):
        """Process NVD vulnerability feed"""
        # Simulate NVD feed processing
        logger.debug("Processing NVD feed updates")
        # In production: fetch, parse, and update vulnerability database
        
    async def _process_exploitdb_feed(self, url: str):
        """Process Exploit-DB feed for exploit availability"""
        # Simulate Exploit-DB processing
        logger.debug("Processing Exploit-DB feed updates")
        # In production: update exploit availability flags
        
    async def _process_mitre_feed(self, url: str):
        """Process MITRE CVE feed"""
        # Simulate MITRE feed processing
        logger.debug("Processing MITRE CVE feed updates")
        # In production: fetch latest CVE data
    
    async def _exploit_intelligence_monitor(self):
        """Monitor exploit intelligence sources for active threats"""
        logger.info("Starting exploit intelligence monitoring")
        
        intelligence_sources = [
            {
                'name': 'GITHUB_EXPLOITS',
                'type': 'github_search',
                'queries': ['CVE-2024', 'exploit', 'poc', 'vulnerability'],
                'check_interval': 1800,  # 30 minutes
                'last_check': None
            },
            {
                'name': 'TWITTER_OSINT',
                'type': 'social_media',
                'keywords': ['#CVE', '#exploit', '#0day', '#vulnerability'],
                'check_interval': 900,  # 15 minutes
                'last_check': None
            },
            {
                'name': 'DARKWEB_FORUMS',
                'type': 'darkweb_monitoring',
                'forums': ['exploit_marketplace', 'cve_discussions'],
                'check_interval': 3600,  # 1 hour
                'last_check': None
            },
            {
                'name': 'RANSOMWARE_GROUPS',
                'type': 'ransomware_tracking',
                'groups': ['conti', 'ryuk', 'lockbit', 'revil'],
                'check_interval': 7200,  # 2 hours
                'last_check': None
            }
        ]
        
        while True:
            try:
                current_time = datetime.utcnow()
                
                for source in intelligence_sources:
                    if (source['last_check'] is None or 
                        (current_time - source['last_check']).total_seconds() >= source['check_interval']):
                        
                        logger.info(f"Checking {source['name']} for exploit intelligence")
                        
                        try:
                            await self._process_intelligence_source(source)
                            source['last_check'] = current_time
                            
                        except Exception as e:
                            logger.error(f"Error processing intelligence source {source['name']}: {e}")
                            
                await asyncio.sleep(300)  # Check every 5 minutes
                
            except asyncio.CancelledError:
                logger.info("Exploit intelligence monitoring cancelled")
                break
            except Exception as e:
                logger.error(f"Error in exploit intelligence monitor: {e}")
                await asyncio.sleep(600)
                
    async def _process_intelligence_source(self, source: Dict[str, Any]):
        """Process intelligence from a specific source"""
        try:
            if source['type'] == 'github_search':
                await self._monitor_github_exploits(source)
            elif source['type'] == 'social_media':
                await self._monitor_social_media(source)
            elif source['type'] == 'darkweb_monitoring':
                await self._monitor_darkweb_forums(source)
            elif source['type'] == 'ransomware_tracking':
                await self._monitor_ransomware_groups(source)
                
        except Exception as e:
            logger.error(f"Error processing intelligence source {source['name']}: {e}")
            
    async def _monitor_github_exploits(self, source: Dict[str, Any]):
        """Monitor GitHub for new exploit code"""
        # Simulate GitHub exploit monitoring
        logger.debug(f"Monitoring GitHub for exploits: {source['queries']}")
        # In production: search GitHub API for exploit repositories
        
    async def _monitor_social_media(self, source: Dict[str, Any]):
        """Monitor social media for vulnerability discussions"""
        # Simulate social media monitoring
        logger.debug(f"Monitoring social media for: {source['keywords']}")
        # In production: monitor Twitter/X API for vulnerability discussions
        
    async def _monitor_darkweb_forums(self, source: Dict[str, Any]):
        """Monitor dark web forums for exploit sales"""
        # Simulate dark web monitoring (requires specialized tools)
        logger.debug(f"Monitoring dark web forums: {source['forums']}")
        # In production: use specialized OSINT tools for dark web monitoring
        
    async def _monitor_ransomware_groups(self, source: Dict[str, Any]):
        """Monitor ransomware group activities"""
        # Simulate ransomware group monitoring
        logger.debug(f"Monitoring ransomware groups: {source['groups']}")
        # In production: monitor known ransomware group channels and leak sites
    
    async def _save_engine_state(self):
        """Save engine state and ML models to persistent storage"""
        try:
            engine_state = {
                'version': '1.0',
                'timestamp': datetime.utcnow().isoformat(),
                'statistics': {
                    'vulnerabilities_processed': getattr(self, '_vulns_processed', 0),
                    'assessments_completed': getattr(self, '_assessments_completed', 0),
                    'models_trained': getattr(self, '_models_trained', 0),
                    'accuracy_score': getattr(self, '_latest_accuracy', 0.0)
                },
                'configuration': {
                    'ml_enabled': self.ml_enabled,
                    'threat_intel_enabled': getattr(self, 'threat_intel_enabled', True),
                    'feed_monitoring_enabled': getattr(self, 'feed_monitoring_enabled', True)
                },
                'model_metadata': {}
            }
            
            # Save ML model metadata if available
            if hasattr(self, 'vulnerability_classifier') and self.vulnerability_classifier:
                engine_state['model_metadata']['classifier'] = {
                    'trained': True,
                    'last_training': getattr(self, '_last_training', None),
                    'feature_count': getattr(self, '_feature_count', 50),
                    'accuracy': getattr(self, '_classifier_accuracy', 0.0)
                }
                
            if hasattr(self, 'risk_predictor') and self.risk_predictor:
                engine_state['model_metadata']['risk_predictor'] = {
                    'trained': True,
                    'last_training': getattr(self, '_last_risk_training', None),
                    'mse': getattr(self, '_predictor_mse', 0.0)
                }
                
            # Save to file (in production, use proper database/storage)
            state_file = 'data/ai_vulnerability_engine_state.json'
            os.makedirs(os.path.dirname(state_file), exist_ok=True)
            
            with open(state_file, 'w') as f:
                json.dump(engine_state, f, indent=2)
                
            # Save ML models if available
            if hasattr(self, 'vulnerability_classifier') and self.vulnerability_classifier:
                try:
                    import joblib
                    model_file = 'data/vulnerability_classifier.pkl'
                    joblib.dump(self.vulnerability_classifier, model_file)
                    logger.info(f"Saved vulnerability classifier to {model_file}")
                except ImportError:
                    logger.warning("joblib not available, cannot save ML models")
                    
            if hasattr(self, 'risk_predictor') and self.risk_predictor:
                try:
                    import joblib
                    model_file = 'data/risk_predictor.pkl'
                    joblib.dump(self.risk_predictor, model_file)
                    logger.info(f"Saved risk predictor to {model_file}")
                except ImportError:
                    pass
                    
            logger.info(f"Engine state saved successfully to {state_file}")
            
        except Exception as e:
            logger.error(f"Error saving engine state: {e}")
            
    async def _load_engine_state(self):
        """Load engine state and ML models from persistent storage"""
        try:
            state_file = 'data/ai_vulnerability_engine_state.json'
            
            if os.path.exists(state_file):
                with open(state_file, 'r') as f:
                    engine_state = json.load(f)
                    
                # Restore statistics
                stats = engine_state.get('statistics', {})
                self._vulns_processed = stats.get('vulnerabilities_processed', 0)
                self._assessments_completed = stats.get('assessments_completed', 0)
                self._models_trained = stats.get('models_trained', 0)
                self._latest_accuracy = stats.get('accuracy_score', 0.0)
                
                # Restore configuration
                config = engine_state.get('configuration', {})
                self.ml_enabled = config.get('ml_enabled', True)
                self.threat_intel_enabled = config.get('threat_intel_enabled', True)
                self.feed_monitoring_enabled = config.get('feed_monitoring_enabled', True)
                
                # Load ML models if available
                try:
                    import joblib
                    
                    classifier_file = 'data/vulnerability_classifier.pkl'
                    if os.path.exists(classifier_file):
                        self.vulnerability_classifier = joblib.load(classifier_file)
                        logger.info("Loaded vulnerability classifier from storage")
                        
                    predictor_file = 'data/risk_predictor.pkl'
                    if os.path.exists(predictor_file):
                        self.risk_predictor = joblib.load(predictor_file)
                        logger.info("Loaded risk predictor from storage")
                        
                except ImportError:
                    logger.warning("joblib not available, cannot load ML models")
                    
                logger.info("Engine state loaded successfully")
            else:
                logger.info("No saved engine state found, starting fresh")
                
        except Exception as e:
            logger.error(f"Error loading engine state: {e}")


# Global service instance
_vulnerability_engine: Optional[ProductionAIVulnerabilityEngine] = None

async def get_production_ai_vulnerability_engine() -> ProductionAIVulnerabilityEngine:
    """Get global Production AI Vulnerability Engine instance"""
    global _vulnerability_engine
    
    if _vulnerability_engine is None:
        _vulnerability_engine = ProductionAIVulnerabilityEngine()
        await _vulnerability_engine.initialize()
        
        # Register with service registry
        from .base_service import service_registry
        service_registry.register(_vulnerability_engine)
    
    return _vulnerability_engine