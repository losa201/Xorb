version: '3.8'

x-common-variables: &common-env
  NVIDIA_API_KEY: ${NVIDIA_API_KEY}
  DATABASE_URL: ${DATABASE_URL:-postgresql://xorb:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}}
  REDIS_URL: ${REDIS_URL:-redis://:${REDIS_PASSWORD}@redis:6379/0}
  ENVIRONMENT: ${ENVIRONMENT:-production}
  LOG_LEVEL: ${LOG_LEVEL:-INFO}
  JWT_SECRET: ${JWT_SECRET}
  XORB_API_KEY: ${XORB_API_KEY}
  ALLOWED_ORIGINS: ${ALLOWED_ORIGINS:-https://xorb.security,https://api.xorb.security}
  ENABLE_RATE_LIMITING: ${ENABLE_RATE_LIMITING:-true}
  ENABLE_AUDIT_LOGGING: ${ENABLE_AUDIT_LOGGING:-true}
  ENABLE_MFA: ${ENABLE_MFA:-true}

x-security-config: &security
  security_opt:
    - no-new-privileges:true
  read_only: true
  tmpfs:
    - /tmp:rw,noexec,nosuid,size=100m

x-resource-limits: &resources
  deploy:
    resources:
      limits:
        cpus: '2.0'
        memory: 2G
      reservations:
        cpus: '0.5'
        memory: 512M

services:
  # XORB API Service
  api:
    build:
      context: .
      dockerfile: services/api/Dockerfile
      target: production
    image: ghcr.io/losa201/xorb-api:latest
    ports:
      - "8000:8000"
    environment:
      <<: *common-env
      SERVICE_NAME: xorb-api
      ENABLE_METRICS: "true"
      NVIDIA_BASE_URL: https://integrate.api.nvidia.com/v1
    volumes:
      - api-logs:/app/logs:rw
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    <<: *security
    <<: *resources
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # XORB Worker Service
  worker:
    build:
      context: .
      dockerfile: services/worker/Dockerfile
      target: production
    image: ghcr.io/losa201/xorb-worker:latest
    ports:
      - "9001:9000"  # Metrics port
    environment:
      <<: *common-env
      SERVICE_NAME: xorb-worker
      TEMPORAL_HOST: temporal:7233
      TASK_QUEUE: xorb-task-queue
      METRICS_PORT: 9000
      WORKER_POOL_SIZE: ${WORKER_POOL_SIZE:-16}
    volumes:
      - worker-logs:/app/logs:rw
    depends_on:
      temporal:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    <<: *security
    <<: *resources
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # XORB Orchestrator Service
  orchestrator:
    build:
      context: .
      dockerfile: services/orchestrator/Dockerfile
      target: production
    image: ghcr.io/losa201/xorb-orchestrator:latest
    ports:
      - "8080:8080"
    environment:
      <<: *common-env
      SERVICE_NAME: xorb-orchestrator
      MAX_CONCURRENT_AGENTS: ${MAX_CONCURRENT_AGENTS:-32}
      ENABLE_METRICS: "true"
    volumes:
      - orchestrator-logs:/app/logs:rw
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    <<: *security
    <<: *resources
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # PostgreSQL Database
  postgres:
    image: ankane/pgvector:v0.5.1
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-xorb}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB:-xorb_db}
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256 --auth-local=scram-sha-256"
    volumes:
      - postgres-data:/var/lib/postgresql/data:rw
      - postgres-logs:/var/log/postgresql:rw
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 1G
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-xorb} -d ${POSTGRES_DB:-xorb_db}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # Redis Cache
  redis:
    image: redis:7.2-alpine
    ports:
      - "6379:6379"
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD}
    command: >
      sh -c "redis-server 
      --requirepass $$REDIS_PASSWORD
      --appendonly yes
      --appendfsync everysec
      --maxmemory 1gb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000"
    volumes:
      - redis-data:/data:rw
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test: ["CMD", "redis-cli", "--no-auth-warning", "-a", "$REDIS_PASSWORD", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Temporal Server
  temporal:
    image: temporalio/auto-setup:1.22.0
    ports:
      - "7233:7233"  # gRPC
      - "8233:8233"  # Web UI
    environment:
      DB: postgresql
      DB_PORT: 5432
      POSTGRES_USER: ${POSTGRES_USER:-xorb}
      POSTGRES_PWD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB:-xorb_db}
      POSTGRES_SEEDS: postgres
      DYNAMIC_CONFIG_FILE_PATH: config/dynamicconfig/development-sql.yaml
    volumes:
      - temporal-data:/etc/temporal/config/dynamicconfig:rw
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "tctl", "cluster", "health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Prometheus Monitoring
  prometheus:
    image: prom/prometheus:v2.47.0
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus:rw
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # Grafana Dashboard
  grafana:
    image: grafana/grafana:10.1.0
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_SECURITY_DISABLE_GRAVATAR: "true"
      GF_SECURITY_COOKIE_SECURE: "true"
      GF_SECURITY_COOKIE_SAMESITE: strict
      GF_SECURITY_CONTENT_TYPE_PROTECTION: "true"
      GF_SECURITY_X_CONTENT_TYPE_OPTIONS: nosniff
      GF_SECURITY_X_XSS_PROTECTION: "true"
    volumes:
      - grafana-data:/var/lib/grafana:rw
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

volumes:
  postgres-data:
    driver: local
  postgres-logs:
    driver: local
  redis-data:
    driver: local
  temporal-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  api-logs:
    driver: local
  worker-logs:
    driver: local
  orchestrator-logs:
    driver: local

networks:
  default:
    driver: bridge
    driver_opts:
      com.docker.network.driver.mtu: 1450
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16