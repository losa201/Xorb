#!/usr/bin/env python3
"""
XORB Episodic Memory Integration for Autonomous Health Management

Advanced episodic memory system integration that learns from failure patterns,
remediation success rates, and environmental conditions to improve autonomous
decision making over time.

Features:
- Pattern recognition for recurring failures
- Success rate tracking for different remediation strategies
- Environmental context correlation
- Adaptive learning for autonomous optimization
- Temporal failure pattern analysis

Author: XORB Autonomous Systems
Version: 2.0.0
"""

import asyncio
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from enum import Enum
import json
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import redis.asyncio as redis

from xorb_core.autonomous.autonomous_health_manager import (
    FailureEvent, ServiceState, RemediationType, ErrorCategory, HealthMetrics
)

logger = logging.getLogger(__name__)


@dataclass
class EpisodicFailureMemory:
    """Episodic memory entry for failure events"""
    id: str
    timestamp: datetime
    service_name: str
    error_category: ErrorCategory
    error_message: str
    failure_context: Dict[str, Any]
    remediation_attempts: List[RemediationType]
    successful_remediation: Optional[RemediationType]
    resolution_time: Optional[float]
    environmental_factors: Dict[str, Any]
    system_state_snapshot: Dict[str, Any]
    success: bool = False
    confidence_score: float = 0.0
    learned_patterns: List[str] = field(default_factory=list)


@dataclass
class RemediationOutcome:
    """Outcome tracking for remediation strategies"""
    remediation_type: RemediationType
    service_name: str
    success_count: int = 0
    failure_count: int = 0
    avg_resolution_time: float = 0.0
    success_rate: float = 0.0
    last_used: Optional[datetime] = None
    environmental_context: Dict[str, Any] = field(default_factory=dict)


class EpisodicMemorySystem:
    """
    Advanced episodic memory system for autonomous health management.
    
    Learns from historical failures and successes to improve autonomous
    decision making and remediation strategies.
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.redis_client = None
        
        # Memory storage
        self.failure_memories: Dict[str, EpisodicFailureMemory] = {}
        self.remediation_outcomes: Dict[str, RemediationOutcome] = {}
        self.pattern_clusters: Dict[str, List[str]] = {}
        
        # Learning components
        self.vectorizer = TfidfVectorizer(max_features=500, stop_words='english')
        self.pattern_classifier = DBSCAN(eps=0.3, min_samples=2)\n        \n        # Configuration\n        self.memory_retention_days = config.get('memory_retention_days', 90)\n        self.pattern_similarity_threshold = config.get('pattern_similarity_threshold', 0.7)\n        self.learning_rate = config.get('learning_rate', 0.1)\n        self.min_pattern_frequency = config.get('min_pattern_frequency', 3)\n        \n        logger.info(\"üß† Episodic Memory System initialized\")\n    \n    async def initialize(self):\n        \"\"\"Initialize async components\"\"\"\n        try:\n            # Initialize Redis connection for persistent storage\n            self.redis_client = redis.Redis(\n                host=self.config.get('redis_host', 'localhost'),\n                port=self.config.get('redis_port', 6379),\n                db=self.config.get('redis_db', 2),  # Use separate DB for episodic memory\n                decode_responses=True\n            )\n            \n            # Load existing memories from persistent storage\n            await self._load_memories_from_storage()\n            \n            # Start background learning tasks\n            asyncio.create_task(self._pattern_learning_loop())\n            asyncio.create_task(self._memory_consolidation_loop())\n            \n            logger.info(\"‚úÖ Episodic Memory System initialized successfully\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to initialize episodic memory system: {e}\")\n            raise\n    \n    async def store_failure_event(self, failure_event: FailureEvent) -> str:\n        \"\"\"Store a failure event in episodic memory\"\"\"\n        try:\n            # Generate unique memory ID\n            memory_id = f\"failure_{failure_event.service_name}_{int(failure_event.timestamp.timestamp())}\"\n            \n            # Extract environmental context\n            environmental_factors = await self._extract_environmental_context(failure_event)\n            \n            # Create episodic memory entry\n            episodic_memory = EpisodicFailureMemory(\n                id=memory_id,\n                timestamp=failure_event.timestamp,\n                service_name=failure_event.service_name,\n                error_category=failure_event.error_category,\n                error_message=failure_event.error_message,\n                failure_context={\n                    'stack_trace': failure_event.stack_trace,\n                    'metrics_snapshot': failure_event.metrics_snapshot.__dict__\n                },\n                remediation_attempts=failure_event.remediation_attempted,\n                successful_remediation=None,\n                resolution_time=failure_event.resolution_time,\n                environmental_factors=environmental_factors,\n                system_state_snapshot=await self._capture_system_state(),\n                success=failure_event.success\n            )\n            \n            # Store in memory\n            self.failure_memories[memory_id] = episodic_memory\n            \n            # Persist to Redis\n            await self._persist_memory(episodic_memory)\n            \n            # Update learning patterns\n            await self._update_learning_patterns(episodic_memory)\n            \n            logger.info(f\"üìù Stored failure memory: {memory_id}\")\n            return memory_id\n            \n        except Exception as e:\n            logger.error(f\"Failed to store failure event: {e}\")\n            return \"\"\n    \n    async def store_failure_resolution(self, failure_event: FailureEvent):\n        \"\"\"Store successful failure resolution for learning\"\"\"\n        try:\n            # Find existing memory entry\n            memory_id = f\"failure_{failure_event.service_name}_{int(failure_event.timestamp.timestamp())}\"\n            \n            if memory_id in self.failure_memories:\n                memory = self.failure_memories[memory_id]\n                \n                # Update with resolution information\n                memory.success = failure_event.success\n                memory.resolution_time = failure_event.resolution_time\n                \n                if failure_event.remediation_attempted:\n                    memory.successful_remediation = failure_event.remediation_attempted[-1]\n                \n                # Update remediation outcome statistics\n                await self._update_remediation_outcomes(memory)\n                \n                # Re-persist updated memory\n                await self._persist_memory(memory)\n                \n                logger.info(f\"‚úÖ Updated failure resolution: {memory_id}\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to store failure resolution: {e}\")\n    \n    async def get_similar_failures(self, current_failure: FailureEvent, \n                                 similarity_threshold: float = 0.7) -> List[Dict[str, Any]]:\n        \"\"\"Get similar historical failures for learning-based remediation\"\"\"\n        try:\n            similar_failures = []\n            \n            # Vectorize current failure\n            current_text = f\"{current_failure.error_message} {current_failure.error_category.value}\"\n            \n            for memory_id, memory in self.failure_memories.items():\n                # Skip if same service and recent (likely same failure)\n                if (memory.service_name == current_failure.service_name and \n                    (current_failure.timestamp - memory.timestamp).total_seconds() < 300):\n                    continue\n                \n                # Calculate similarity\n                similarity = await self._calculate_failure_similarity(current_failure, memory)\n                \n                if similarity >= similarity_threshold:\n                    similar_failures.append({\n                        'memory_id': memory_id,\n                        'memory': memory,\n                        'similarity': similarity,\n                        'successful_remediation': memory.successful_remediation.value if memory.successful_remediation else None,\n                        'success_rate': self._get_remediation_success_rate(\n                            memory.service_name, memory.successful_remediation\n                        ),\n                        'resolution_time': memory.resolution_time\n                    })\n            \n            # Sort by similarity and success rate\n            similar_failures.sort(key=lambda x: (x['similarity'], x['success_rate']), reverse=True)\n            \n            logger.info(f\"üîç Found {len(similar_failures)} similar failures for {current_failure.service_name}\")\n            return similar_failures[:10]  # Return top 10 most similar\n            \n        except Exception as e:\n            logger.error(f\"Failed to get similar failures: {e}\")\n            return []\n    \n    async def get_failure_patterns(self, limit: int = 1000) -> List[Dict[str, Any]]:\n        \"\"\"Get failure patterns for ML model training\"\"\"\n        try:\n            patterns = []\n            \n            for memory in list(self.failure_memories.values())[-limit:]:\n                pattern = {\n                    'error_message': memory.error_message,\n                    'category': memory.error_category.value,\n                    'service': memory.service_name,\n                    'successful_remediation': memory.successful_remediation.value if memory.successful_remediation else 'none',\n                    'severity': self._calculate_failure_severity(memory),\n                    'environmental_load': memory.environmental_factors.get('system_load', 0.5),\n                    'time_of_day': memory.timestamp.hour,\n                    'day_of_week': memory.timestamp.weekday(),\n                    'resolution_time': memory.resolution_time or 0.0\n                }\n                patterns.append(pattern)\n            \n            return patterns\n            \n        except Exception as e:\n            logger.error(f\"Failed to get failure patterns: {e}\")\n            return []\n    \n    async def get_service_failures(self, service_name: str, time_window: str) -> List[Dict[str, Any]]:\n        \"\"\"Get service failures within a time window\"\"\"\n        try:\n            # Parse time window\n            if time_window == '1h':\n                cutoff_time = datetime.now() - timedelta(hours=1)\n            elif time_window == '24h':\n                cutoff_time = datetime.now() - timedelta(hours=24)\n            elif time_window == '7d':\n                cutoff_time = datetime.now() - timedelta(days=7)\n            else:\n                cutoff_time = datetime.now() - timedelta(hours=24)  # Default\n            \n            service_failures = []\n            \n            for memory in self.failure_memories.values():\n                if (memory.service_name == service_name and \n                    memory.timestamp >= cutoff_time):\n                    \n                    service_failures.append({\n                        'timestamp': memory.timestamp.isoformat(),\n                        'resolved': memory.success,\n                        'resolution_time': memory.resolution_time,\n                        'remediation_type': memory.successful_remediation.value if memory.successful_remediation else None,\n                        'error_category': memory.error_category.value\n                    })\n            \n            return service_failures\n            \n        except Exception as e:\n            logger.error(f\"Failed to get service failures: {e}\")\n            return []\n    \n    async def get_optimal_remediation(self, service_name: str, \n                                    error_category: ErrorCategory) -> Optional[RemediationType]:\n        \"\"\"Get optimal remediation based on historical success patterns\"\"\"\n        try:\n            # Get remediation statistics for this service and error category\n            best_remediation = None\n            best_score = 0.0\n            \n            for outcome_key, outcome in self.remediation_outcomes.items():\n                if (service_name in outcome_key and \n                    error_category.value in outcome_key):\n                    \n                    # Calculate composite score (success rate + recency + frequency)\n                    score = (\n                        outcome.success_rate * 0.6 +  # 60% weight on success rate\n                        (1.0 / max(1, (datetime.now() - (outcome.last_used or datetime.now())).days)) * 0.2 +  # 20% recency\n                        min(1.0, outcome.success_count / 10) * 0.2  # 20% frequency (capped)\n                    )\n                    \n                    if score > best_score:\n                        best_score = score\n                        best_remediation = outcome.remediation_type\n            \n            logger.info(f\"üéØ Optimal remediation for {service_name}/{error_category.value}: {best_remediation}\")\n            return best_remediation\n            \n        except Exception as e:\n            logger.error(f\"Failed to get optimal remediation: {e}\")\n            return None\n    \n    async def _extract_environmental_context(self, failure_event: FailureEvent) -> Dict[str, Any]:\n        \"\"\"Extract environmental context at time of failure\"\"\"\n        try:\n            # Get system metrics snapshot\n            context = {\n                'system_load': failure_event.metrics_snapshot.cpu_usage / 100.0,\n                'memory_pressure': failure_event.metrics_snapshot.memory_usage / 100.0,\n                'error_rate': failure_event.metrics_snapshot.error_rate,\n                'response_time': failure_event.metrics_snapshot.response_time,\n                'time_of_day': failure_event.timestamp.hour,\n                'day_of_week': failure_event.timestamp.weekday(),\n                'is_weekend': failure_event.timestamp.weekday() >= 5,\n                'is_business_hours': 9 <= failure_event.timestamp.hour <= 17\n            }\n            \n            # Add service-specific context if available\n            if hasattr(failure_event.metrics_snapshot, 'custom_metrics'):\n                context.update(failure_event.metrics_snapshot.custom_metrics)\n            \n            return context\n            \n        except Exception as e:\n            logger.warning(f\"Failed to extract environmental context: {e}\")\n            return {}\n    \n    async def _capture_system_state(self) -> Dict[str, Any]:\n        \"\"\"Capture current system state snapshot\"\"\"\n        try:\n            # This would typically capture metrics from Prometheus or system monitoring\n            # For now, return a placeholder\n            return {\n                'timestamp': datetime.now().isoformat(),\n                'active_services': 'unknown',\n                'system_resources': 'unknown'\n            }\n        except Exception:\n            return {}\n    \n    async def _calculate_failure_similarity(self, current_failure: FailureEvent, \n                                          memory: EpisodicFailureMemory) -> float:\n        \"\"\"Calculate similarity between current failure and historical memory\"\"\"\n        try:\n            # Text similarity\n            current_text = f\"{current_failure.error_message} {current_failure.error_category.value}\"\n            memory_text = f\"{memory.error_message} {memory.error_category.value}\"\n            \n            # Use cosine similarity for text\n            text_vectors = self.vectorizer.fit_transform([current_text, memory_text])\n            text_similarity = cosine_similarity(text_vectors[0:1], text_vectors[1:2])[0][0]\n            \n            # Service similarity (exact match = 1.0, different = 0.0)\n            service_similarity = 1.0 if current_failure.service_name == memory.service_name else 0.0\n            \n            # Error category similarity\n            category_similarity = 1.0 if current_failure.error_category == memory.error_category else 0.3\n            \n            # Environmental similarity\n            env_similarity = self._calculate_environmental_similarity(\n                await self._extract_environmental_context(current_failure),\n                memory.environmental_factors\n            )\n            \n            # Weighted combination\n            total_similarity = (\n                text_similarity * 0.4 +\n                service_similarity * 0.2 +\n                category_similarity * 0.2 +\n                env_similarity * 0.2\n            )\n            \n            return total_similarity\n            \n        except Exception as e:\n            logger.warning(f\"Failed to calculate failure similarity: {e}\")\n            return 0.0\n    \n    def _calculate_environmental_similarity(self, env1: Dict[str, Any], env2: Dict[str, Any]) -> float:\n        \"\"\"Calculate environmental context similarity\"\"\"\n        try:\n            if not env1 or not env2:\n                return 0.0\n            \n            # Compare numerical factors\n            numerical_factors = ['system_load', 'memory_pressure', 'error_rate']\n            similarities = []\n            \n            for factor in numerical_factors:\n                if factor in env1 and factor in env2:\n                    # Normalized difference (closer values = higher similarity)\n                    diff = abs(env1[factor] - env2[factor])\n                    similarity = max(0.0, 1.0 - diff)\n                    similarities.append(similarity)\n            \n            # Compare categorical factors\n            categorical_factors = ['time_of_day', 'day_of_week', 'is_weekend', 'is_business_hours']\n            for factor in categorical_factors:\n                if factor in env1 and factor in env2:\n                    similarity = 1.0 if env1[factor] == env2[factor] else 0.0\n                    similarities.append(similarity)\n            \n            return np.mean(similarities) if similarities else 0.0\n            \n        except Exception:\n            return 0.0\n    \n    def _calculate_failure_severity(self, memory: EpisodicFailureMemory) -> float:\n        \"\"\"Calculate failure severity score\"\"\"\n        try:\n            # Base severity on error category\n            category_severity = {\n                ErrorCategory.NETWORK: 0.6,\n                ErrorCategory.MEMORY: 0.9,\n                ErrorCategory.DEPENDENCY: 0.7,\n                ErrorCategory.LOGIC: 0.4,\n                ErrorCategory.RESOURCE: 0.8,\n                ErrorCategory.CONFIGURATION: 0.5,\n                ErrorCategory.UNKNOWN: 0.5\n            }\n            \n            base_severity = category_severity.get(memory.error_category, 0.5)\n            \n            # Adjust based on resolution time\n            if memory.resolution_time:\n                time_factor = min(1.0, memory.resolution_time / 300.0)  # Normalize to 5 minutes\n                base_severity += time_factor * 0.2\n            \n            # Adjust based on environmental factors\n            if memory.environmental_factors:\n                load_factor = memory.environmental_factors.get('system_load', 0.5)\n                base_severity += load_factor * 0.1\n            \n            return min(1.0, base_severity)\n            \n        except Exception:\n            return 0.5\n    \n    def _get_remediation_success_rate(self, service_name: str, \n                                    remediation_type: Optional[RemediationType]) -> float:\n        \"\"\"Get success rate for a specific remediation type\"\"\"\n        if not remediation_type:\n            return 0.0\n        \n        outcome_key = f\"{service_name}_{remediation_type.value}\"\n        \n        if outcome_key in self.remediation_outcomes:\n            return self.remediation_outcomes[outcome_key].success_rate\n        \n        return 0.0\n    \n    async def _update_remediation_outcomes(self, memory: EpisodicFailureMemory):\n        \"\"\"Update remediation outcome statistics\"\"\"\n        try:\n            if not memory.successful_remediation:\n                return\n            \n            outcome_key = f\"{memory.service_name}_{memory.successful_remediation.value}\"\n            \n            if outcome_key not in self.remediation_outcomes:\n                self.remediation_outcomes[outcome_key] = RemediationOutcome(\n                    remediation_type=memory.successful_remediation,\n                    service_name=memory.service_name\n                )\n            \n            outcome = self.remediation_outcomes[outcome_key]\n            \n            # Update statistics\n            if memory.success:\n                outcome.success_count += 1\n            else:\n                outcome.failure_count += 1\n            \n            # Update success rate\n            total_attempts = outcome.success_count + outcome.failure_count\n            outcome.success_rate = outcome.success_count / total_attempts if total_attempts > 0 else 0.0\n            \n            # Update average resolution time\n            if memory.resolution_time and memory.success:\n                if outcome.avg_resolution_time == 0.0:\n                    outcome.avg_resolution_time = memory.resolution_time\n                else:\n                    # Running average\n                    outcome.avg_resolution_time = (\n                        outcome.avg_resolution_time * 0.8 + memory.resolution_time * 0.2\n                    )\n            \n            outcome.last_used = memory.timestamp\n            outcome.environmental_context = memory.environmental_factors\n            \n            # Persist outcome update\n            await self._persist_remediation_outcome(outcome_key, outcome)\n            \n        except Exception as e:\n            logger.error(f\"Failed to update remediation outcomes: {e}\")\n    \n    async def _persist_memory(self, memory: EpisodicFailureMemory):\n        \"\"\"Persist memory to Redis storage\"\"\"\n        try:\n            if self.redis_client:\n                memory_data = {\n                    'id': memory.id,\n                    'timestamp': memory.timestamp.isoformat(),\n                    'service_name': memory.service_name,\n                    'error_category': memory.error_category.value,\n                    'error_message': memory.error_message,\n                    'failure_context': json.dumps(memory.failure_context),\n                    'remediation_attempts': [r.value for r in memory.remediation_attempts],\n                    'successful_remediation': memory.successful_remediation.value if memory.successful_remediation else None,\n                    'resolution_time': memory.resolution_time,\n                    'environmental_factors': json.dumps(memory.environmental_factors),\n                    'system_state_snapshot': json.dumps(memory.system_state_snapshot),\n                    'success': memory.success\n                }\n                \n                await self.redis_client.hset(f\"episodic_memory:{memory.id}\", mapping=memory_data)\n                \n                # Set expiration based on retention policy\n                expire_seconds = self.memory_retention_days * 24 * 3600\n                await self.redis_client.expire(f\"episodic_memory:{memory.id}\", expire_seconds)\n        \n        except Exception as e:\n            logger.warning(f\"Failed to persist memory: {e}\")\n    \n    async def _persist_remediation_outcome(self, outcome_key: str, outcome: RemediationOutcome):\n        \"\"\"Persist remediation outcome to Redis\"\"\"\n        try:\n            if self.redis_client:\n                outcome_data = {\n                    'remediation_type': outcome.remediation_type.value,\n                    'service_name': outcome.service_name,\n                    'success_count': outcome.success_count,\n                    'failure_count': outcome.failure_count,\n                    'avg_resolution_time': outcome.avg_resolution_time,\n                    'success_rate': outcome.success_rate,\n                    'last_used': outcome.last_used.isoformat() if outcome.last_used else None,\n                    'environmental_context': json.dumps(outcome.environmental_context)\n                }\n                \n                await self.redis_client.hset(f\"remediation_outcome:{outcome_key}\", mapping=outcome_data)\n        \n        except Exception as e:\n            logger.warning(f\"Failed to persist remediation outcome: {e}\")\n    \n    async def _load_memories_from_storage(self):\n        \"\"\"Load existing memories from Redis storage\"\"\"\n        try:\n            if not self.redis_client:\n                return\n            \n            # Load episodic memories\n            memory_keys = await self.redis_client.keys(\"episodic_memory:*\")\n            \n            for key in memory_keys:\n                memory_data = await self.redis_client.hgetall(key)\n                if memory_data:\n                    # Reconstruct memory object\n                    memory = EpisodicFailureMemory(\n                        id=memory_data['id'],\n                        timestamp=datetime.fromisoformat(memory_data['timestamp']),\n                        service_name=memory_data['service_name'],\n                        error_category=ErrorCategory(memory_data['error_category']),\n                        error_message=memory_data['error_message'],\n                        failure_context=json.loads(memory_data.get('failure_context', '{}')),\n                        remediation_attempts=[RemediationType(r) for r in memory_data.get('remediation_attempts', [])],\n                        successful_remediation=RemediationType(memory_data['successful_remediation']) \n                                            if memory_data.get('successful_remediation') else None,\n                        resolution_time=float(memory_data['resolution_time']) if memory_data.get('resolution_time') else None,\n                        environmental_factors=json.loads(memory_data.get('environmental_factors', '{}')),\n                        system_state_snapshot=json.loads(memory_data.get('system_state_snapshot', '{}')),\n                        success=memory_data.get('success', 'false').lower() == 'true'\n                    )\n                    \n                    self.failure_memories[memory.id] = memory\n            \n            # Load remediation outcomes\n            outcome_keys = await self.redis_client.keys(\"remediation_outcome:*\")\n            \n            for key in outcome_keys:\n                outcome_data = await self.redis_client.hgetall(key)\n                if outcome_data:\n                    outcome_key = key.replace(\"remediation_outcome:\", \"\")\n                    \n                    outcome = RemediationOutcome(\n                        remediation_type=RemediationType(outcome_data['remediation_type']),\n                        service_name=outcome_data['service_name'],\n                        success_count=int(outcome_data.get('success_count', 0)),\n                        failure_count=int(outcome_data.get('failure_count', 0)),\n                        avg_resolution_time=float(outcome_data.get('avg_resolution_time', 0.0)),\n                        success_rate=float(outcome_data.get('success_rate', 0.0)),\n                        last_used=datetime.fromisoformat(outcome_data['last_used']) \n                                 if outcome_data.get('last_used') else None,\n                        environmental_context=json.loads(outcome_data.get('environmental_context', '{}'))\n                    )\n                    \n                    self.remediation_outcomes[outcome_key] = outcome\n            \n            logger.info(\n                f\"üìö Loaded {len(self.failure_memories)} memories and \"\n                f\"{len(self.remediation_outcomes)} remediation outcomes from storage\"\n            )\n        \n        except Exception as e:\n            logger.error(f\"Failed to load memories from storage: {e}\")\n    \n    async def _update_learning_patterns(self, memory: EpisodicFailureMemory):\n        \"\"\"Update learning patterns based on new memory\"\"\"\n        try:\n            # Pattern learning will be implemented here\n            # For now, just log the memory addition\n            logger.debug(f\"üìñ Learning pattern update for: {memory.id}\")\n        \n        except Exception as e:\n            logger.warning(f\"Failed to update learning patterns: {e}\")\n    \n    async def _pattern_learning_loop(self):\n        \"\"\"Background task for pattern learning and clustering\"\"\"\n        while True:\n            try:\n                await asyncio.sleep(3600)  # Run every hour\n                \n                if len(self.failure_memories) < self.min_pattern_frequency:\n                    continue\n                \n                # Perform pattern clustering on recent memories\n                recent_memories = [\n                    memory for memory in self.failure_memories.values()\n                    if (datetime.now() - memory.timestamp).days <= 7\n                ]\n                \n                if len(recent_memories) >= self.min_pattern_frequency:\n                    await self._cluster_failure_patterns(recent_memories)\n                \n            except Exception as e:\n                logger.error(f\"Pattern learning loop error: {e}\")\n                await asyncio.sleep(300)  # Retry after 5 minutes\n    \n    async def _cluster_failure_patterns(self, memories: List[EpisodicFailureMemory]):\n        \"\"\"Cluster failure patterns for learning\"\"\"\n        try:\n            # Extract text features for clustering\n            texts = [f\"{m.error_message} {m.error_category.value}\" for m in memories]\n            \n            if len(set(texts)) < 2:  # Need at least 2 unique patterns\n                return\n            \n            # Vectorize and cluster\n            text_vectors = self.vectorizer.fit_transform(texts)\n            clusters = self.pattern_classifier.fit_predict(text_vectors.toarray())\n            \n            # Group memories by cluster\n            clustered_patterns = {}\n            for i, cluster_id in enumerate(clusters):\n                if cluster_id == -1:  # Noise cluster\n                    continue\n                \n                if cluster_id not in clustered_patterns:\n                    clustered_patterns[cluster_id] = []\n                \n                clustered_patterns[cluster_id].append(memories[i])\n            \n            # Analyze successful patterns\n            for cluster_id, cluster_memories in clustered_patterns.items():\n                if len(cluster_memories) >= self.min_pattern_frequency:\n                    await self._analyze_cluster_patterns(cluster_id, cluster_memories)\n            \n            logger.info(f\"üîç Clustered {len(memories)} memories into {len(clustered_patterns)} patterns\")\n        \n        except Exception as e:\n            logger.error(f\"Failed to cluster failure patterns: {e}\")\n    \n    async def _analyze_cluster_patterns(self, cluster_id: int, memories: List[EpisodicFailureMemory]):\n        \"\"\"Analyze patterns within a cluster\"\"\"\n        try:\n            # Calculate success rates for different remediation strategies\n            remediation_stats = {}\n            \n            for memory in memories:\n                if memory.successful_remediation and memory.success:\n                    remediation_type = memory.successful_remediation.value\n                    \n                    if remediation_type not in remediation_stats:\n                        remediation_stats[remediation_type] = {'success': 0, 'total': 0}\n                    \n                    remediation_stats[remediation_type]['success'] += 1\n                    remediation_stats[remediation_type]['total'] += 1\n                elif memory.remediation_attempts:\n                    for attempt in memory.remediation_attempts:\n                        if attempt.value not in remediation_stats:\n                            remediation_stats[attempt.value] = {'success': 0, 'total': 0}\n                        remediation_stats[attempt.value]['total'] += 1\n            \n            # Log pattern insights\n            logger.info(f\"üìä Cluster {cluster_id} pattern analysis: {remediation_stats}\")\n            \n            # Store pattern insights (could be used for ML model improvement)\n            pattern_key = f\"pattern_cluster_{cluster_id}\"\n            self.pattern_clusters[pattern_key] = [m.id for m in memories]\n        \n        except Exception as e:\n            logger.error(f\"Failed to analyze cluster patterns: {e}\")\n    \n    async def _memory_consolidation_loop(self):\n        \"\"\"Background task for memory consolidation and cleanup\"\"\"\n        while True:\n            try:\n                await asyncio.sleep(86400)  # Run daily\n                \n                # Clean up old memories beyond retention period\n                cutoff_date = datetime.now() - timedelta(days=self.memory_retention_days)\n                \n                expired_memories = [\n                    memory_id for memory_id, memory in self.failure_memories.items()\n                    if memory.timestamp < cutoff_date\n                ]\n                \n                # Remove expired memories\n                for memory_id in expired_memories:\n                    self.failure_memories.pop(memory_id, None)\n                    \n                    # Also remove from Redis\n                    if self.redis_client:\n                        await self.redis_client.delete(f\"episodic_memory:{memory_id}\")\n                \n                if expired_memories:\n                    logger.info(f\"üßπ Cleaned up {len(expired_memories)} expired memories\")\n            \n            except Exception as e:\n                logger.error(f\"Memory consolidation error: {e}\")\n                await asyncio.sleep(3600)  # Retry after 1 hour\n\n\n# Factory function\nasync def create_episodic_memory_system(config: Dict[str, Any]) -> EpisodicMemorySystem:\n    \"\"\"Factory function to create and initialize episodic memory system\"\"\"\n    system = EpisodicMemorySystem(config)\n    await system.initialize()\n    return system