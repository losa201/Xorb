#!/usr/bin/env python3
"""
Sophisticated Red Team Agent Validation Script
Comprehensive testing and validation of the advanced red team AI capabilities

This script validates:
- Red team agent initialization and functionality
- AI-powered attack planning and execution
- Purple team collaboration features
- Defensive insight generation
- Integration with existing XORB infrastructure
"""

import asyncio
import json
import logging
import sys
import time
from pathlib import Path
from typing import Dict, Any, List, Optional

# Add the project root to the Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "src" / "api"))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SophisticatedRedTeamValidator:
    """Comprehensive validator for sophisticated red team agent"""
    
    def __init__(self):
        self.validation_id = f"redteam_validation_{int(time.time())}"
        self.results = {
            "validation_id": self.validation_id,
            "timestamp": time.time(),
            "tests": {},
            "summary": {},
            "recommendations": []
        }
        
        # Service instances
        self.red_team_agent = None
        self.red_team_utilities = None
        
        # Test data
        self.test_objectives = []
        self.test_operations = []

    async def run_comprehensive_validation(self) -> Dict[str, Any]:
        """Run comprehensive validation of red team agent"""
        logger.info("ðŸš€ Starting Sophisticated Red Team Agent Validation")
        logger.info(f"ðŸ“‹ Validation ID: {self.validation_id}")
        
        try:
            # Initialize services
            await self._initialize_services()
            
            # Core functionality tests
            await self._test_agent_initialization()
            await self._test_threat_actor_modeling()
            await self._test_ai_decision_making()
            await self._test_attack_planning()
            await self._test_operation_execution()
            await self._test_defensive_insights()
            await self._test_purple_team_collaboration()
            await self._test_utilities_integration()
            await self._test_mitre_integration()
            await self._test_performance_metrics()
            
            # Integration tests
            await self._test_api_integration()
            await self._test_health_monitoring()
            
            # Generate final summary
            await self._generate_validation_summary()
            
            return self.results
            
        except Exception as e:
            logger.error(f"âŒ Validation failed: {e}")
            self.results["error"] = str(e)
            return self.results

    async def _initialize_services(self):
        """Initialize red team services"""
        test_name = "Service Initialization"
        logger.info(f"ðŸ”§ {test_name}")
        
        try:
            # Import and initialize red team agent
            from app.services.sophisticated_red_team_agent import get_sophisticated_red_team_agent
            from app.services.red_team_utilities import get_red_team_utilities
            
            self.red_team_agent = await get_sophisticated_red_team_agent()
            self.red_team_utilities = await get_red_team_utilities()
            
            self.results["tests"][test_name] = {
                "status": "passed",
                "details": {
                    "red_team_agent_initialized": self.red_team_agent is not None,
                    "utilities_initialized": self.red_team_utilities is not None,
                    "agent_type": type(self.red_team_agent).__name__,
                    "service_name": self.red_team_agent.service_name
                }
            }
            
            logger.info(f"âœ… {test_name} - PASSED")
            
        except Exception as e:
            logger.error(f"âŒ {test_name} - FAILED: {e}")
            self.results["tests"][test_name] = {
                "status": "failed",
                "error": str(e)
            }
            raise

    async def _test_agent_initialization(self):
        """Test red team agent initialization"""
        test_name = "Agent Initialization"
        logger.info(f"ðŸ§ª Testing: {test_name}")
        
        try:
            # Test agent properties
            test_results = {
                "service_name": self.red_team_agent.service_name,
                "service_type": self.red_team_agent.service_type,
                "dependencies": self.red_team_agent.dependencies,
                "max_concurrent_operations": self.red_team_agent.max_concurrent_operations,
                "safety_constraints": self.red_team_agent.safety_constraints,
                "purple_team_mode": self.red_team_agent.purple_team_mode
            }
            
            # Test AI engines availability
            ai_engines = {
                "decision_engine": self.red_team_agent.decision_engine is not None,
                "behavior_models": len(self.red_team_agent.behavior_models) > 0,
                "neural_networks": len(self.red_team_agent.neural_networks) > 0,
                "evasion_engine": self.red_team_agent.evasion_engine is not None
            }
            test_results["ai_engines"] = ai_engines
            
            # Test knowledge bases
            knowledge_bases = {
                "threat_actor_models": len(self.red_team_agent.threat_actor_models),
                "technique_database": len(self.red_team_agent.technique_database),
                "exploit_database": len(self.red_team_agent.exploit_database),
                "defense_database": len(self.red_team_agent.defense_database)
            }
            test_results["knowledge_bases"] = knowledge_bases
            
            # Validate safety constraints
            safety_checks = [
                self.red_team_agent.safety_constraints.get("defensive_purpose_only", False),
                self.red_team_agent.safety_constraints.get("purple_team_collaboration", False),
                self.red_team_agent.safety_constraints.get("require_authorization", False)
            ]
            
            self.results["tests"][test_name] = {
                "status": "passed" if all(safety_checks) else "warning",
                "details": test_results,
                "safety_validation": all(safety_checks),
                "warnings": [] if all(safety_checks) else ["Some safety constraints may not be properly configured"]
            }
            
            logger.info(f"âœ… {test_name} - PASSED")
            logger.info(f"   ðŸ“Š Threat actors loaded: {knowledge_bases['threat_actor_models']}")
            logger.info(f"   ðŸ“Š Techniques available: {knowledge_bases['technique_database']}")
            logger.info(f"   ðŸ›¡ï¸ Safety constraints: {'âœ…' if all(safety_checks) else 'âš ï¸'}")
            
        except Exception as e:
            logger.error(f"âŒ {test_name} - FAILED: {e}")
            self.results["tests"][test_name] = {
                "status": "failed",
                "error": str(e)
            }

    async def _test_threat_actor_modeling(self):
        """Test threat actor modeling capabilities"""
        test_name = "Threat Actor Modeling"
        logger.info(f"ðŸ§ª Testing: {test_name}")
        
        try:
            # Get available threat actors
            threat_actors = self.red_team_agent.threat_actor_models
            
            test_results = {
                "total_actors": len(threat_actors),
                "actor_profiles": []
            }
            
            # Test each threat actor profile
            for actor_id, actor in list(threat_actors.items())[:3]:  # Test first 3
                profile_test = {
                    "actor_id": actor.actor_id,
                    "name": actor.name,
                    "sophistication_level": actor.sophistication_level.value,
                    "techniques_count": len(actor.preferred_techniques),
                    "targeting_sectors": len(actor.targeting_criteria),
                    "attribution_confidence": actor.attribution_confidence
                }
                test_results["actor_profiles"].append(profile_test)
                
                # Test threat actor intelligence generation
                try:
                    intelligence = await self.red_team_agent.generate_threat_actor_intelligence(actor_id)
                    profile_test["intelligence_generation"] = True
                    profile_test["intelligence_components"] = list(intelligence.keys())
                except Exception as e:
                    profile_test["intelligence_generation"] = False
                    profile_test["intelligence_error"] = str(e)
            
            # Validate actor quality
            quality_checks = []
            for actor in threat_actors.values():
                checks = [
                    len(actor.preferred_techniques) > 0,
                    len(actor.operational_patterns) > 0,
                    len(actor.targeting_criteria) > 0,
                    actor.attribution_confidence > 0,
                    len(actor.defensive_countermeasures) > 0
                ]
                quality_checks.append(all(checks))
            
            overall_quality = sum(quality_checks) / len(quality_checks) if quality_checks else 0
            
            self.results["tests"][test_name] = {
                "status": "passed" if overall_quality > 0.8 else "warning",
                "details": test_results,
                "quality_score": overall_quality,
                "metrics": {
                    "avg_techniques_per_actor": sum(len(a.preferred_techniques) for a in threat_actors.values()) / len(threat_actors),
                    "avg_attribution_confidence": sum(a.attribution_confidence for a in threat_actors.values()) / len(threat_actors)
                }
            }
            
            logger.info(f"âœ… {test_name} - PASSED")
            logger.info(f"   ðŸ“Š Threat actors: {len(threat_actors)}")
            logger.info(f"   ðŸ“Š Quality score: {overall_quality:.2%}")
            
        except Exception as e:
            logger.error(f"âŒ {test_name} - FAILED: {e}")
            self.results["tests"][test_name] = {
                "status": "failed",
                "error": str(e)
            }

    async def _test_ai_decision_making(self):
        """Test AI decision-making capabilities"""
        test_name = "AI Decision Making"
        logger.info(f"ðŸ§ª Testing: {test_name}")
        
        try:
            # Test AI model availability
            ai_models = {
                "behavior_models": len(self.red_team_agent.behavior_models),
                "decision_trees": len(self.red_team_agent.decision_trees),
                "neural_networks": len(self.red_team_agent.neural_networks)
            }
            
            # Test technique selection AI
            test_techniques = [
                {"technique_id": "T1566.001", "name": "Spearphishing Attachment"},
                {"technique_id": "T1059.001", "name": "PowerShell"},
                {"technique_id": "T1055", "name": "Process Injection"}
            ]
            
            # Create mock objective and environment
            from app.services.sophisticated_red_team_agent import RedTeamObjective, SophisticationLevel
            from datetime import timedelta
            
            mock_objective = RedTeamObjective(
                objective_id="test_obj_001",
                name="AI Decision Test",
                description="Test AI decision-making capabilities",
                target_assets=["test.example.com"],
                success_criteria=["Successful technique selection"],
                mitre_tactics=["TA0001"],
                mitre_techniques=["T1566.001"],
                sophistication_level=SophisticationLevel.ADVANCED,
                estimated_duration=timedelta(hours=1),
                stealth_requirements=True,
                defensive_learning_goals=["Test AI decisions"]
            )
            
            mock_environment = {"test_environment": True}
            
            # Test technique selection
            selection_results = []
            
            # Get a threat actor for testing
            threat_actor = list(self.red_team_agent.threat_actor_models.values())[0]
            
            try:
                # Test AI technique selection (simplified test)
                selected_technique = await self.red_team_agent._ai_select_technique(
                    test_techniques, mock_objective, threat_actor, mock_environment
                )
                selection_results.append({
                    "selected_technique": selected_technique.get("technique_id") if selected_technique else None,
                    "selection_successful": selected_technique is not None
                })
            except Exception as e:
                selection_results.append({
                    "selection_error": str(e),
                    "selection_successful": False
                })
            
            self.results["tests"][test_name] = {
                "status": "passed",
                "details": {
                    "ai_models": ai_models,
                    "technique_selection": selection_results,
                    "ai_availability": {
                        "sklearn": hasattr(self.red_team_agent, '_sklearn_available'),
                        "torch": hasattr(self.red_team_agent, '_torch_available')
                    }
                }
            }
            
            logger.info(f"âœ… {test_name} - PASSED")
            logger.info(f"   ðŸ¤– AI models available: {sum(ai_models.values())}")
            
        except Exception as e:
            logger.error(f"âŒ {test_name} - FAILED: {e}")
            self.results["tests"][test_name] = {
                "status": "failed",
                "error": str(e)
            }

    async def _test_attack_planning(self):
        """Test attack planning capabilities"""
        test_name = "Attack Planning"
        logger.info(f"ðŸ§ª Testing: {test_name}")
        
        try:
            # Create test objective
            from app.services.sophisticated_red_team_agent import RedTeamObjective, SophisticationLevel
            from datetime import timedelta
            
            test_objective = RedTeamObjective(
                objective_id="test_planning_001",
                name="Test Attack Planning",
                description="Comprehensive attack planning test",
                target_assets=["test1.example.com", "test2.example.com"],
                success_criteria=[
                    "Generate comprehensive attack chain",
                    "Include multiple attack phases",
                    "Provide defensive insights"
                ],
                mitre_tactics=["TA0001", "TA0002", "TA0003"],
                mitre_techniques=["T1566.001", "T1059.001", "T1055"],
                sophistication_level=SophisticationLevel.ADVANCED,
                estimated_duration=timedelta(hours=6),
                stealth_requirements=True,
                defensive_learning_goals=[
                    "Improve phishing detection",
                    "Enhance PowerShell monitoring",
                    "Strengthen process injection defenses"
                ]
            )
            
            # Test environment
            test_environment = {
                "environment_type": "test",
                "security_controls": ["email_security", "endpoint_detection", "network_monitoring"],
                "target_systems": ["windows_workstations", "linux_servers"],
                "network_segmentation": "basic"
            }
            
            # Plan red team operation
            operation = await self.red_team_agent.plan_red_team_operation(
                test_objective, test_environment
            )
            
            # Validate operation plan
            plan_validation = {
                "operation_created": operation is not None,
                "operation_id": operation.operation_id if operation else None,
                "attack_chain_length": len(operation.attack_chain) if operation else 0,
                "timeline_generated": len(operation.timeline) > 0 if operation else False,
                "success_metrics": len(operation.success_metrics) > 0 if operation else False,
                "defensive_insights": len(operation.defensive_insights) > 0 if operation else False,
                "purple_team_integration": operation.purple_team_integration if operation else False
            }
            
            # Store operation for later tests
            if operation:
                self.test_operations.append(operation)
            
            # Analyze attack chain quality
            attack_chain_quality = {}
            if operation and operation.attack_chain:
                attack_chain_quality = {
                    "total_vectors": len(operation.attack_chain),
                    "avg_success_probability": sum(v.success_probability for v in operation.attack_chain) / len(operation.attack_chain),
                    "avg_detection_probability": sum(v.detection_probability for v in operation.attack_chain) / len(operation.attack_chain),
                    "avg_defensive_value": sum(v.defensive_value for v in operation.attack_chain) / len(operation.attack_chain),
                    "technique_coverage": len(set(v.technique_id for v in operation.attack_chain))
                }
            
            self.results["tests"][test_name] = {
                "status": "passed" if plan_validation["operation_created"] else "failed",
                "details": {
                    "plan_validation": plan_validation,
                    "attack_chain_quality": attack_chain_quality,
                    "objective_mapping": {
                        "tactics_covered": len(test_objective.mitre_tactics),
                        "techniques_covered": len(test_objective.mitre_techniques),
                        "learning_goals": len(test_objective.defensive_learning_goals)
                    }
                }
            }
            
            logger.info(f"âœ… {test_name} - PASSED")
            if operation:
                logger.info(f"   ðŸ“Š Attack vectors: {len(operation.attack_chain)}")
                logger.info(f"   ðŸ“Š Purple team integration: {'âœ…' if operation.purple_team_integration else 'âŒ'}")
            
        except Exception as e:
            logger.error(f"âŒ {test_name} - FAILED: {e}")
            self.results["tests"][test_name] = {
                "status": "failed",
                "error": str(e)
            }

    async def _test_operation_execution(self):
        """Test operation execution (simulation mode)"""
        test_name = "Operation Execution"
        logger.info(f"ðŸ§ª Testing: {test_name}")
        
        try:
            if not self.test_operations:
                logger.warning(f"âš ï¸ {test_name} - SKIPPED: No operations available for testing")
                self.results["tests"][test_name] = {
                    "status": "skipped",
                    "reason": "No operations available for testing"
                }
                return
            
            # Execute first test operation
            operation = self.test_operations[0]
            
            # Execute operation (simulation mode)
            execution_results = await self.red_team_agent.execute_red_team_operation(operation.operation_id)
            
            # Validate execution results
            execution_validation = {
                "execution_completed": execution_results is not None,
                "phases_executed": len(execution_results.get("execution_results", {}).get("phases_executed", [])),
                "techniques_used": len(execution_results.get("execution_results", {}).get("techniques_used", [])),
                "detection_events": len(execution_results.get("execution_results", {}).get("detection_events", [])),
                "defensive_insights_generated": len(execution_results.get("defensive_insights", [])),
                "purple_team_feedback": "purple_team_feedback" in execution_results,
                "success_rate": execution_results.get("success_rate", 0),
                "detection_rate": execution_results.get("detection_rate", 0)
            }
            
            # Analyze execution quality
            execution_quality = {}
            if execution_results:
                execution_quality = {
                    "simulation_safety": execution_results.get("execution_results", {}).get("simulation_only", True),
                    "defensive_value": len(execution_results.get("defensive_improvements", [])),
                    "learning_outcomes": len(execution_results.get("defensive_insights", [])),
                    "purple_team_collaboration": execution_results.get("purple_team_feedback") is not None
                }
            
            self.results["tests"][test_name] = {
                "status": "passed" if execution_validation["execution_completed"] else "failed",
                "details": {
                    "execution_validation": execution_validation,
                    "execution_quality": execution_quality,
                    "safety_verification": execution_results.get("execution_results", {}).get("simulation_only", True)
                }
            }
            
            logger.info(f"âœ… {test_name} - PASSED")
            logger.info(f"   ðŸ“Š Phases executed: {execution_validation['phases_executed']}")
            logger.info(f"   ðŸ“Š Detection events: {execution_validation['detection_events']}")
            logger.info(f"   ðŸ›¡ï¸ Simulation mode: {'âœ…' if execution_quality.get('simulation_safety') else 'âŒ'}")
            
        except Exception as e:
            logger.error(f"âŒ {test_name} - FAILED: {e}")
            self.results["tests"][test_name] = {
                "status": "failed",
                "error": str(e)
            }

    async def _test_defensive_insights(self):
        """Test defensive insight generation"""
        test_name = "Defensive Insights"
        logger.info(f"ðŸ§ª Testing: {test_name}")
        
        try:
            # Create mock execution results
            mock_execution_results = {
                "techniques_used": [
                    {"technique_id": "T1566.001", "name": "Spearphishing Attachment", "success": True, "detection_triggered": False},
                    {"technique_id": "T1059.001", "name": "PowerShell", "success": True, "detection_triggered": True},
                    {"technique_id": "T1055", "name": "Process Injection", "success": False, "detection_triggered": False}
                ],
                "detection_events": [
                    {"technique": "T1059.001", "detection_method": "PowerShell logging", "timestamp": "2025-01-01T12:00:00"}
                ]
            }
            
            # Generate defensive insights
            insights = await self.red_team_agent._generate_defensive_insights(mock_execution_results)
            
            # Analyze insights quality
            insight_analysis = {
                "total_insights": len(insights),
                "insight_types": list(set(insight.get("type") for insight in insights)),
                "high_priority_insights": len([i for i in insights if i.get("priority") == "high"]),
                "detection_gaps_identified": len([i for i in insights if i.get("type") == "detection_gap"]),
                "response_improvements": len([i for i in insights if i.get("type") == "response_improvement"])
            }
            
            # Validate insight structure
            insight_structure_valid = []
            for insight in insights[:3]:  # Check first 3 insights
                required_fields = ["type", "description", "recommendations", "priority"]
                structure_valid = all(field in insight for field in required_fields)
                insight_structure_valid.append(structure_valid)
            
            structure_quality = sum(insight_structure_valid) / len(insight_structure_valid) if insight_structure_valid else 0
            
            self.results["tests"][test_name] = {
                "status": "passed" if len(insights) > 0 and structure_quality > 0.8 else "warning",
                "details": {
                    "insight_analysis": insight_analysis,
                    "structure_quality": structure_quality,
                    "sample_insights": insights[:2] if insights else []
                }
            }
            
            logger.info(f"âœ… {test_name} - PASSED")
            logger.info(f"   ðŸ“Š Insights generated: {len(insights)}")
            logger.info(f"   ðŸ“Š Detection gaps: {insight_analysis['detection_gaps_identified']}")
            logger.info(f"   ðŸ“Š Structure quality: {structure_quality:.2%}")
            
        except Exception as e:
            logger.error(f"âŒ {test_name} - FAILED: {e}")
            self.results["tests"][test_name] = {
                "status": "failed",
                "error": str(e)
            }

    async def _test_purple_team_collaboration(self):
        """Test purple team collaboration features"""
        test_name = "Purple Team Collaboration"
        logger.info(f"ðŸ§ª Testing: {test_name}")
        
        try:
            # Test purple team integration status
            purple_team_config = {
                "collaborative_mode": getattr(self.red_team_agent, 'purple_team_integration', {}).get("collaborative_mode", False),
                "real_time_sharing": getattr(self.red_team_agent, 'purple_team_integration', {}).get("real_time_sharing", False),
                "defensive_feedback_loop": getattr(self.red_team_agent, 'purple_team_integration', {}).get("defensive_feedback_loop", False),
                "training_integration": getattr(self.red_team_agent, 'purple_team_integration', {}).get("training_integration", False)
            }
            
            # Test purple team report generation
            mock_operation_results = {
                "techniques_used": [
                    {"technique_id": "T1566.001", "success": True, "detection_triggered": False},
                    {"technique_id": "T1059.001", "success": True, "detection_triggered": True}
                ],
                "detection_events": [{"technique": "T1059.001", "detection_method": "PowerShell logging"}],
                "purple_team_observations": [{"feedback": "Good detection coverage for PowerShell"}]
            }
            
            # Generate purple team report using utilities
            purple_report = await self.red_team_utilities.generate_purple_team_report(mock_operation_results)
            
            # Validate report structure
            report_validation = {
                "report_generated": purple_report is not None,
                "has_executive_summary": "executive_summary" in purple_report,
                "has_technical_analysis": "technical_analysis" in purple_report,
                "has_defensive_improvements": "defensive_improvements" in purple_report,
                "has_training_recommendations": "training_recommendations" in purple_report,
                "has_metrics": "metrics" in purple_report,
                "has_next_steps": "next_steps" in purple_report
            }
            
            collaboration_score = sum(purple_team_config.values()) / len(purple_team_config)
            report_completeness = sum(report_validation.values()) / len(report_validation)
            
            self.results["tests"][test_name] = {
                "status": "passed" if collaboration_score > 0.5 and report_completeness > 0.8 else "warning",
                "details": {
                    "purple_team_config": purple_team_config,
                    "report_validation": report_validation,
                    "collaboration_score": collaboration_score,
                    "report_completeness": report_completeness,
                    "sample_report_sections": list(purple_report.keys()) if purple_report else []
                }
            }
            
            logger.info(f"âœ… {test_name} - PASSED")
            logger.info(f"   ðŸ“Š Collaboration score: {collaboration_score:.2%}")
            logger.info(f"   ðŸ“Š Report completeness: {report_completeness:.2%}")
            
        except Exception as e:
            logger.error(f"âŒ {test_name} - FAILED: {e}")
            self.results["tests"][test_name] = {
                "status": "failed",
                "error": str(e)
            }

    async def _test_utilities_integration(self):
        """Test red team utilities integration"""
        test_name = "Utilities Integration"
        logger.info(f"ðŸ§ª Testing: {test_name}")
        
        try:
            # Test detection rule generation
            test_technique = "T1566.001"
            detection_rule = await self.red_team_utilities.generate_detection_rule(test_technique)
            
            # Test training exercise generation
            training_exercise = await self.red_team_utilities.generate_training_exercise(
                [test_technique, "T1059.001"]
            )
            
            # Test technique statistics
            technique_stats = await self.red_team_utilities.get_technique_statistics()
            
            # Validate utilities functionality
            utilities_validation = {
                "detection_rule_generation": detection_rule is not None,
                "training_exercise_generation": training_exercise is not None,
                "technique_statistics": technique_stats is not None,
                "detection_rules_available": len(self.red_team_utilities.detection_rules_db),
                "training_exercises_available": len(self.red_team_utilities.training_exercises_db),
                "hunting_queries_available": len(self.red_team_utilities.hunting_queries_db),
                "technique_mappings": len(self.red_team_utilities.technique_mappings)
            }
            
            # Test detection rule quality
            rule_quality = {}
            if detection_rule:
                rule_quality = {
                    "has_rule_content": len(detection_rule.rule_content) > 0,
                    "has_description": len(detection_rule.description) > 0,
                    "confidence_level": detection_rule.confidence_level,
                    "false_positive_rate": detection_rule.false_positive_rate,
                    "data_sources": len(detection_rule.data_sources)
                }
            
            # Test training exercise quality
            exercise_quality = {}
            if training_exercise:
                exercise_quality = {
                    "has_steps": len(training_exercise.exercise_steps) > 0,
                    "has_objectives": len(training_exercise.learning_objectives) > 0,
                    "duration_reasonable": training_exercise.duration_minutes > 0,
                    "techniques_covered": len(training_exercise.techniques_covered),
                    "defensive_focus": len(training_exercise.defensive_focus_areas)
                }
            
            overall_quality = sum(utilities_validation.values()) / len(utilities_validation)
            
            self.results["tests"][test_name] = {
                "status": "passed" if overall_quality > 0.7 else "warning",
                "details": {
                    "utilities_validation": utilities_validation,
                    "rule_quality": rule_quality,
                    "exercise_quality": exercise_quality,
                    "overall_quality": overall_quality
                }
            }
            
            logger.info(f"âœ… {test_name} - PASSED")
            logger.info(f"   ðŸ“Š Detection rules: {utilities_validation['detection_rules_available']}")
            logger.info(f"   ðŸ“Š Training exercises: {utilities_validation['training_exercises_available']}")
            logger.info(f"   ðŸ“Š Overall quality: {overall_quality:.2%}")
            
        except Exception as e:
            logger.error(f"âŒ {test_name} - FAILED: {e}")
            self.results["tests"][test_name] = {
                "status": "failed",
                "error": str(e)
            }

    async def _test_mitre_integration(self):
        """Test MITRE ATT&CK integration"""
        test_name = "MITRE Integration"
        logger.info(f"ðŸ§ª Testing: {test_name}")
        
        try:
            # Test MITRE engine availability
            mitre_engine = self.red_team_agent.mitre_engine
            
            mitre_validation = {
                "mitre_engine_available": mitre_engine is not None,
                "techniques_loaded": len(self.red_team_agent.technique_database),
                "technique_mappings": len(self.red_team_utilities.technique_mappings)
            }
            
            # Test technique database quality
            technique_quality = {}
            if self.red_team_agent.technique_database:
                sample_techniques = list(self.red_team_agent.technique_database.items())[:5]
                quality_scores = []
                
                for technique_id, technique_data in sample_techniques:
                    required_fields = ["name", "description", "red_team_value", "detection_difficulty", "defensive_learning_value"]
                    field_completeness = sum(1 for field in required_fields if field in technique_data) / len(required_fields)
                    quality_scores.append(field_completeness)
                
                technique_quality = {
                    "avg_completeness": sum(quality_scores) / len(quality_scores) if quality_scores else 0,
                    "sample_techniques": [tid for tid, _ in sample_techniques],
                    "data_richness": sum(len(data) for _, data in sample_techniques) / len(sample_techniques) if sample_techniques else 0
                }
            
            # Test technique mapping coverage
            mapping_coverage = {}
            if self.red_team_utilities.technique_mappings:
                mappings = self.red_team_utilities.technique_mappings
                mapping_coverage = {
                    "total_mappings": len(mappings),
                    "with_detection_rules": sum(1 for m in mappings.values() if m.get("detection_rules")),
                    "with_hunting_queries": sum(1 for m in mappings.values() if m.get("hunting_queries")),
                    "with_training_exercises": sum(1 for m in mappings.values() if m.get("training_exercises")),
                    "with_countermeasures": sum(1 for m in mappings.values() if m.get("countermeasures"))
                }
            
            overall_integration = sum(mitre_validation.values()) / len(mitre_validation)
            
            self.results["tests"][test_name] = {
                "status": "passed" if overall_integration > 0.8 else "warning",
                "details": {
                    "mitre_validation": mitre_validation,
                    "technique_quality": technique_quality,
                    "mapping_coverage": mapping_coverage,
                    "integration_score": overall_integration
                }
            }
            
            logger.info(f"âœ… {test_name} - PASSED")
            logger.info(f"   ðŸ“Š Techniques: {mitre_validation['techniques_loaded']}")
            logger.info(f"   ðŸ“Š Mappings: {mitre_validation['technique_mappings']}")
            logger.info(f"   ðŸ“Š Integration score: {overall_integration:.2%}")
            
        except Exception as e:
            logger.error(f"âŒ {test_name} - FAILED: {e}")
            self.results["tests"][test_name] = {
                "status": "failed",
                "error": str(e)
            }

    async def _test_performance_metrics(self):
        """Test performance metrics and monitoring"""
        test_name = "Performance Metrics"
        logger.info(f"ðŸ§ª Testing: {test_name}")
        
        try:
            # Get operation metrics
            metrics = await self.red_team_agent.get_operation_metrics()
            
            # Test metrics availability
            metrics_validation = {
                "metrics_available": metrics is not None,
                "has_operation_metrics": "operation_metrics" in metrics,
                "has_active_operations": "active_operations" in metrics,
                "has_success_rate": "success_rate" in metrics,
                "has_detection_improvement_rate": "detection_improvement_rate" in metrics
            }
            
            # Validate metric values
            metric_values = {}
            if metrics:
                metric_values = {
                    "total_operations": metrics.get("operation_metrics", {}).get("total_operations", 0),
                    "successful_operations": metrics.get("operation_metrics", {}).get("successful_operations", 0),
                    "defensive_insights_generated": metrics.get("operation_metrics", {}).get("defensive_insights_generated", 0),
                    "active_operations": metrics.get("active_operations", 0),
                    "success_rate": metrics.get("success_rate", 0),
                    "detection_improvement_rate": metrics.get("detection_improvement_rate", 0)
                }
            
            # Test performance characteristics
            performance_validation = {
                "metrics_response_time": "fast",  # Placeholder - would measure actual time
                "data_consistency": True,  # Placeholder - would validate data consistency
                "memory_efficiency": True  # Placeholder - would check memory usage
            }
            
            metrics_completeness = sum(metrics_validation.values()) / len(metrics_validation)
            
            self.results["tests"][test_name] = {
                "status": "passed" if metrics_completeness > 0.8 else "warning",
                "details": {
                    "metrics_validation": metrics_validation,
                    "metric_values": metric_values,
                    "performance_validation": performance_validation,
                    "completeness_score": metrics_completeness
                }
            }
            
            logger.info(f"âœ… {test_name} - PASSED")
            logger.info(f"   ðŸ“Š Metrics completeness: {metrics_completeness:.2%}")
            
        except Exception as e:
            logger.error(f"âŒ {test_name} - FAILED: {e}")
            self.results["tests"][test_name] = {
                "status": "failed",
                "error": str(e)
            }

    async def _test_api_integration(self):
        """Test API integration capabilities"""
        test_name = "API Integration"
        logger.info(f"ðŸ§ª Testing: {test_name}")
        
        try:
            # Test router module availability
            try:
                from app.routers.sophisticated_red_team import router
                router_available = True
                router_routes = len(router.routes)
            except ImportError:
                router_available = False
                router_routes = 0
            
            # Test API endpoint structure
            api_validation = {
                "router_available": router_available,
                "router_routes": router_routes,
                "endpoints_expected": [
                    "/sophisticated-red-team/objectives",
                    "/sophisticated-red-team/operations",
                    "/sophisticated-red-team/threat-actors",
                    "/sophisticated-red-team/defensive-insights",
                    "/sophisticated-red-team/metrics",
                    "/sophisticated-red-team/health"
                ]
            }
            
            # Test service integration points
            integration_points = {
                "mitre_attack_integration": hasattr(self.red_team_agent, 'mitre_engine'),
                "ptaas_integration": 'ptaas_scanner' in self.red_team_agent.dependencies,
                "threat_intelligence_integration": 'threat_intelligence' in self.red_team_agent.dependencies,
                "utilities_integration": self.red_team_utilities is not None
            }
            
            integration_score = sum(integration_points.values()) / len(integration_points)
            
            self.results["tests"][test_name] = {
                "status": "passed" if router_available and integration_score > 0.7 else "warning",
                "details": {
                    "api_validation": api_validation,
                    "integration_points": integration_points,
                    "integration_score": integration_score
                }
            }
            
            logger.info(f"âœ… {test_name} - PASSED")
            logger.info(f"   ðŸ“Š Router available: {'âœ…' if router_available else 'âŒ'}")
            logger.info(f"   ðŸ“Š Integration score: {integration_score:.2%}")
            
        except Exception as e:
            logger.error(f"âŒ {test_name} - FAILED: {e}")
            self.results["tests"][test_name] = {
                "status": "failed",
                "error": str(e)
            }

    async def _test_health_monitoring(self):
        """Test health monitoring capabilities"""
        test_name = "Health Monitoring"
        logger.info(f"ðŸ§ª Testing: {test_name}")
        
        try:
            # Perform health check
            health_status = await self.red_team_agent.health_check()
            
            # Validate health check response
            health_validation = {
                "health_check_available": health_status is not None,
                "has_service_name": hasattr(health_status, 'service_name'),
                "has_status": hasattr(health_status, 'status'),
                "has_timestamp": hasattr(health_status, 'timestamp'),
                "has_details": hasattr(health_status, 'details')
            }
            
            # Check health status details
            health_details = {}
            if health_status and hasattr(health_status, 'details'):
                details = health_status.details
                health_details = {
                    "component_status_available": "component_status" in details,
                    "operation_metrics_available": "operation_metrics" in details,
                    "ai_availability_info": "ai_availability" in details,
                    "service_status": health_status.status.value if hasattr(health_status, 'status') else "unknown"
                }
            
            # Test utilities health
            utilities_health = {
                "detection_rules_loaded": len(self.red_team_utilities.detection_rules_db) > 0,
                "training_exercises_loaded": len(self.red_team_utilities.training_exercises_db) > 0,
                "hunting_queries_loaded": len(self.red_team_utilities.hunting_queries_db) > 0,
                "technique_mappings_loaded": len(self.red_team_utilities.technique_mappings) > 0
            }
            
            health_completeness = sum(health_validation.values()) / len(health_validation)
            utilities_completeness = sum(utilities_health.values()) / len(utilities_health)
            overall_health = (health_completeness + utilities_completeness) / 2
            
            self.results["tests"][test_name] = {
                "status": "passed" if overall_health > 0.8 else "warning",
                "details": {
                    "health_validation": health_validation,
                    "health_details": health_details,
                    "utilities_health": utilities_health,
                    "health_completeness": health_completeness,
                    "utilities_completeness": utilities_completeness,
                    "overall_health_score": overall_health
                }
            }
            
            logger.info(f"âœ… {test_name} - PASSED")
            logger.info(f"   ðŸ“Š Health score: {overall_health:.2%}")
            if health_status:
                logger.info(f"   ðŸ“Š Service status: {health_details.get('service_status', 'unknown')}")
            
        except Exception as e:
            logger.error(f"âŒ {test_name} - FAILED: {e}")
            self.results["tests"][test_name] = {
                "status": "failed",
                "error": str(e)
            }

    async def _generate_validation_summary(self):
        """Generate comprehensive validation summary"""
        logger.info("ðŸ“‹ Generating Validation Summary")
        
        try:
            # Calculate overall statistics
            total_tests = len(self.results["tests"])
            passed_tests = len([t for t in self.results["tests"].values() if t["status"] == "passed"])
            failed_tests = len([t for t in self.results["tests"].values() if t["status"] == "failed"])
            warning_tests = len([t for t in self.results["tests"].values() if t["status"] == "warning"])
            skipped_tests = len([t for t in self.results["tests"].values() if t["status"] == "skipped"])
            
            # Calculate success rate
            success_rate = passed_tests / total_tests if total_tests > 0 else 0
            
            # Determine overall status
            if failed_tests > 0:
                overall_status = "NEEDS_ATTENTION"
            elif warning_tests > 2:
                overall_status = "GOOD_WITH_WARNINGS"
            elif success_rate >= 0.9:
                overall_status = "EXCELLENT"
            elif success_rate >= 0.8:
                overall_status = "GOOD"
            else:
                overall_status = "NEEDS_IMPROVEMENT"
            
            # Generate recommendations
            recommendations = []
            
            # Check for failed tests
            for test_name, test_result in self.results["tests"].items():
                if test_result["status"] == "failed":
                    recommendations.append({
                        "priority": "high",
                        "category": "bug_fix",
                        "test": test_name,
                        "issue": test_result.get("error", "Test failed"),
                        "recommendation": f"Fix critical issue in {test_name} functionality"
                    })
                elif test_result["status"] == "warning":
                    recommendations.append({
                        "priority": "medium",
                        "category": "improvement",
                        "test": test_name,
                        "recommendation": f"Address warnings in {test_name} to improve reliability"
                    })
            
            # Add general recommendations
            if success_rate < 1.0:
                recommendations.append({
                    "priority": "medium",
                    "category": "testing",
                    "recommendation": "Increase test coverage and add more comprehensive validation"
                })
            
            # Identify strengths
            strengths = []
            if passed_tests >= 8:
                strengths.append("Strong core functionality implementation")
            if any("AI" in test_name for test_name in self.results["tests"].keys() if self.results["tests"][test_name]["status"] == "passed"):
                strengths.append("Robust AI integration and decision-making capabilities")
            if any("Purple Team" in test_name for test_name in self.results["tests"].keys() if self.results["tests"][test_name]["status"] == "passed"):
                strengths.append("Effective purple team collaboration features")
            if any("MITRE" in test_name for test_name in self.results["tests"].keys() if self.results["tests"][test_name]["status"] == "passed"):
                strengths.append("Comprehensive MITRE ATT&CK framework integration")
            
            # Generate capability assessment
            capabilities = {
                "red_team_operations": any("Attack Planning" in t or "Operation Execution" in t for t in self.results["tests"].keys()),
                "ai_decision_making": "AI Decision Making" in self.results["tests"] and self.results["tests"]["AI Decision Making"]["status"] == "passed",
                "threat_actor_modeling": "Threat Actor Modeling" in self.results["tests"] and self.results["tests"]["Threat Actor Modeling"]["status"] == "passed",
                "defensive_insights": "Defensive Insights" in self.results["tests"] and self.results["tests"]["Defensive Insights"]["status"] == "passed",
                "purple_team_collaboration": "Purple Team Collaboration" in self.results["tests"] and self.results["tests"]["Purple Team Collaboration"]["status"] == "passed",
                "mitre_integration": "MITRE Integration" in self.results["tests"] and self.results["tests"]["MITRE Integration"]["status"] == "passed",
                "api_integration": "API Integration" in self.results["tests"] and self.results["tests"]["API Integration"]["status"] == "passed"
            }
            
            capability_score = sum(capabilities.values()) / len(capabilities)
            
            # Create summary
            self.results["summary"] = {
                "overall_status": overall_status,
                "test_statistics": {
                    "total_tests": total_tests,
                    "passed": passed_tests,
                    "failed": failed_tests,
                    "warnings": warning_tests,
                    "skipped": skipped_tests,
                    "success_rate": success_rate
                },
                "capability_assessment": {
                    "capabilities": capabilities,
                    "capability_score": capability_score,
                    "strengths": strengths
                },
                "validation_outcome": self._generate_validation_outcome(overall_status, success_rate, capability_score)
            }
            
            self.results["recommendations"] = recommendations
            
            # Log summary
            logger.info("ðŸ“Š VALIDATION SUMMARY")
            logger.info(f"   Overall Status: {overall_status}")
            logger.info(f"   Success Rate: {success_rate:.1%}")
            logger.info(f"   Tests Passed: {passed_tests}/{total_tests}")
            logger.info(f"   Capability Score: {capability_score:.1%}")
            logger.info(f"   Recommendations: {len(recommendations)}")
            
        except Exception as e:
            logger.error(f"Failed to generate validation summary: {e}")
            self.results["summary"] = {"error": str(e)}

    def _generate_validation_outcome(self, overall_status: str, success_rate: float, capability_score: float) -> str:
        """Generate validation outcome message"""
        if overall_status == "EXCELLENT":
            return "ðŸŽ‰ SOPHISTICATED RED TEAM AGENT: PRODUCTION READY! All core capabilities validated and operational."
        elif overall_status == "GOOD":
            return "âœ… SOPHISTICATED RED TEAM AGENT: READY FOR DEPLOYMENT with minor optimizations recommended."
        elif overall_status == "GOOD_WITH_WARNINGS":
            return "âš ï¸ SOPHISTICATED RED TEAM AGENT: FUNCTIONAL but address warnings before production deployment."
        elif overall_status == "NEEDS_IMPROVEMENT":
            return "ðŸ”§ SOPHISTICATED RED TEAM AGENT: NEEDS IMPROVEMENT - address failed tests before deployment."
        else:
            return "âŒ SOPHISTICATED RED TEAM AGENT: NEEDS ATTENTION - critical issues must be resolved."


async def main():
    """Main validation function"""
    print("ðŸŽ¯ SOPHISTICATED RED TEAM AGENT VALIDATION")
    print("=" * 60)
    
    validator = SophisticatedRedTeamValidator()
    results = await validator.run_comprehensive_validation()
    
    # Save results
    output_file = f"sophisticated_redteam_validation_{int(time.time())}.json"
    try:
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        print(f"\nðŸ“„ Detailed results saved to: {output_file}")
    except Exception as e:
        print(f"\nâš ï¸ Could not save results file: {e}")
    
    # Print final outcome
    print("\n" + "=" * 60)
    if "summary" in results and "validation_outcome" in results["summary"]:
        print(results["summary"]["validation_outcome"])
    else:
        print("âŒ VALIDATION INCOMPLETE - Check detailed results for errors")
    
    print("=" * 60)
    
    return results


if __name__ == "__main__":
    asyncio.run(main())