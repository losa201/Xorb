#!/usr/bin/env python3
"""
XORB Infrastructure Automation Module
Comprehensive infrastructure provisioning and management for PostgreSQL, Redis, Neo4j, and monitoring stack
"""

import asyncio
import json
import logging
import os
import subprocess
import tempfile
import time
import yaml
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, field
from enum import Enum

logger = logging.getLogger('XORBInfrastructure')

class InfrastructureStatus(Enum):
    """Infrastructure component status"""
    PENDING = "pending"
    PROVISIONING = "provisioning"
    RUNNING = "running"
    FAILED = "failed"
    UPDATING = "updating"
    STOPPED = "stopped"

class DatabaseType(Enum):
    """Database types"""
    POSTGRESQL = "postgresql"
    REDIS = "redis"
    NEO4J = "neo4j"
    ELASTICSEARCH = "elasticsearch"

@dataclass
class PostgreSQLConfig:
    """PostgreSQL configuration"""
    version: str = "15"
    port: int = 5432
    database: str = "xorb"
    username: str = "xorb_user"
    password: str = "xorb_secure_password_123!"
    extensions: List[str] = field(default_factory=lambda: ["pgvector", "uuid-ossp", "pg_stat_statements"])
    max_connections: int = 200
    shared_buffers: str = "256MB"
    work_mem: str = "4MB"
    maintenance_work_mem: str = "64MB"
    effective_cache_size: str = "1GB"
    enable_ssl: bool = True
    enable_replication: bool = True
    replica_count: int = 1
    backup_schedule: str = "0 2 * * *"
    storage_size: str = "20Gi"
    performance_tuning: Dict[str, Any] = field(default_factory=lambda: {
        "checkpoint_completion_target": "0.9",
        "wal_buffers": "16MB",
        "default_statistics_target": "100",
        "random_page_cost": "1.1",
        "effective_io_concurrency": "200"
    })

@dataclass
class RedisConfig:
    """Redis configuration"""
    version: str = "7"
    port: int = 6379
    max_memory: str = "512mb"
    max_memory_policy: str = "allkeys-lru"
    enable_cluster: bool = False
    enable_sentinel: bool = True
    sentinel_quorum: int = 2
    cluster_nodes: int = 6
    enable_ssl: bool = True
    enable_auth: bool = True
    persistence: str = "aof"
    backup_schedule: str = "0 3 * * *"
    appendonly: bool = True
    appendfsync: str = "everysec"
    storage_size: str = "10Gi"

@dataclass
class Neo4jConfig:
    """Neo4j configuration"""
    version: str = "5"
    port: int = 7687
    http_port: int = 7474
    heap_size: str = "512M"
    page_cache: str = "512M"
    enable_ssl: bool = True
    enable_auth: bool = True
    enable_clustering: bool = False
    cluster_nodes: int = 3
    backup_schedule: str = "0 4 * * *"
    storage_size: str = "10Gi"
    username: str = "neo4j"
    password: str = "neo4j_secure_password_123!"

@dataclass
class ElasticsearchConfig:
    """Elasticsearch configuration"""
    version: str = "8.9.0"
    port: int = 9200
    heap_size: str = "1g"
    enable_security: bool = True
    enable_ssl: bool = True
    replica_count: int = 1
    shard_count: int = 1
    storage_size: str = "20Gi"
    cluster_name: str = "xorb-elasticsearch"

@dataclass
class InfrastructureComponent:
    """Infrastructure component"""
    name: str
    type: DatabaseType
    status: InfrastructureStatus
    config: Union[PostgreSQLConfig, RedisConfig, Neo4jConfig, ElasticsearchConfig]
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    error_message: Optional[str] = None
    endpoints: Dict[str, str] = field(default_factory=dict)
    health_status: bool = False
    metrics: Dict[str, Any] = field(default_factory=dict)

class XORBInfrastructureAutomation:
    """Main infrastructure automation system"""
    
    def __init__(self, namespace: str = "xorb-infra", platform: str = "kubernetes"):
        self.namespace = namespace
        self.platform = platform
        self.components: Dict[str, InfrastructureComponent] = {}
        self.deployment_id = f"infra_{int(time.time())}"
        
        # Initialize configurations
        self._initialize_infrastructure_configs()
        
        logger.info(f"üèóÔ∏è  Infrastructure automation initialized")
        logger.info(f"üìã Namespace: {self.namespace}")
        logger.info(f"üéØ Platform: {self.platform}")
        logger.info(f"üÜî Deployment ID: {self.deployment_id}")
    
    def _initialize_infrastructure_configs(self):
        """Initialize infrastructure component configurations"""
        
        # PostgreSQL Primary
        postgres_config = PostgreSQLConfig()
        self.components["postgresql-primary"] = InfrastructureComponent(
            name="postgresql-primary",
            type=DatabaseType.POSTGRESQL,
            status=InfrastructureStatus.PENDING,
            config=postgres_config
        )
        
        # PostgreSQL Replica
        postgres_replica_config = PostgreSQLConfig()
        postgres_replica_config.port = 5433
        self.components["postgresql-replica"] = InfrastructureComponent(
            name="postgresql-replica",
            type=DatabaseType.POSTGRESQL,
            status=InfrastructureStatus.PENDING,
            config=postgres_replica_config
        )
        
        # Redis Cluster
        redis_config = RedisConfig()
        redis_config.enable_cluster = True
        self.components["redis-cluster"] = InfrastructureComponent(
            name="redis-cluster",
            type=DatabaseType.REDIS,
            status=InfrastructureStatus.PENDING,
            config=redis_config
        )
        
        # Neo4j Cluster
        neo4j_config = Neo4jConfig()
        self.components["neo4j-cluster"] = InfrastructureComponent(
            name="neo4j-cluster",
            type=DatabaseType.NEO4J,
            status=InfrastructureStatus.PENDING,
            config=neo4j_config
        )
        
        # Elasticsearch Cluster
        elasticsearch_config = ElasticsearchConfig()
        self.components["elasticsearch-cluster"] = InfrastructureComponent(
            name="elasticsearch-cluster",
            type=DatabaseType.ELASTICSEARCH,
            status=InfrastructureStatus.PENDING,
            config=elasticsearch_config
        )
        
        logger.info(f"üîß Initialized {len(self.components)} infrastructure components")
    
    async def provision_infrastructure(self) -> bool:
        """Provision all infrastructure components"""
        try:
            logger.info("üöÄ Starting infrastructure provisioning")
            
            # Create namespace
            if self.platform == "kubernetes":
                await self._create_kubernetes_namespace()
            
            # Provision components in dependency order
            provisioning_order = [
                "postgresql-primary",
                "postgresql-replica", 
                "redis-cluster",
                "neo4j-cluster",
                "elasticsearch-cluster"
            ]
            
            for component_name in provisioning_order:
                component = self.components[component_name]
                logger.info(f"üîß Provisioning {component_name}")
                
                success = await self._provision_component(component)
                
                if success:
                    logger.info(f"‚úÖ {component_name} provisioned successfully")
                    # Wait for health check
                    await self._wait_for_component_health(component)
                else:
                    logger.error(f"‚ùå Failed to provision {component_name}")
                    return False
            
            # Setup monitoring and backups
            await self._setup_infrastructure_monitoring()
            await self._setup_automated_backups()
            
            logger.info("üéâ Infrastructure provisioning completed successfully!")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Infrastructure provisioning failed: {e}")
            return False
    
    async def _create_kubernetes_namespace(self) -> bool:
        """Create Kubernetes namespace for infrastructure"""
        try:
            namespace_yaml = f"""
apiVersion: v1
kind: Namespace
metadata:
  name: {self.namespace}
  labels:
    name: {self.namespace}
    component: infrastructure
    managed-by: xorb-automation
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: {self.namespace}-quota
  namespace: {self.namespace}
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi
    persistentvolumeclaims: "10"
    services: "10"
---
apiVersion: v1
kind: LimitRange
metadata:
  name: {self.namespace}-limits
  namespace: {self.namespace}
spec:
  limits:
  - default:
      cpu: "2"
      memory: "2Gi"
    defaultRequest:
      cpu: "500m"
      memory: "512Mi"
    type: Container
"""
            
            return await self._kubectl_apply(namespace_yaml)
            
        except Exception as e:
            logger.error(f"‚ùå Failed to create namespace: {e}")
            return False
    
    async def _provision_component(self, component: InfrastructureComponent) -> bool:
        """Provision individual infrastructure component"""
        try:
            component.status = InfrastructureStatus.PROVISIONING
            component.start_time = datetime.now()
            
            if component.type == DatabaseType.POSTGRESQL:
                success = await self._provision_postgresql(component)
            elif component.type == DatabaseType.REDIS:
                success = await self._provision_redis(component)
            elif component.type == DatabaseType.NEO4J:
                success = await self._provision_neo4j(component)
            elif component.type == DatabaseType.ELASTICSEARCH:
                success = await self._provision_elasticsearch(component)
            else:
                logger.warning(f"‚ö†Ô∏è  Unknown component type: {component.type}")
                success = False
            
            component.end_time = datetime.now()
            
            if success:
                component.status = InfrastructureStatus.RUNNING
            else:
                component.status = InfrastructureStatus.FAILED
            
            return success
            
        except Exception as e:
            component.status = InfrastructureStatus.FAILED
            component.error_message = str(e)
            component.end_time = datetime.now()
            logger.error(f"‚ùå Component provisioning failed: {e}")
            return False
    
    async def _provision_postgresql(self, component: InfrastructureComponent) -> bool:
        """Provision PostgreSQL with high availability"""
        try:
            config = component.config
            is_replica = "replica" in component.name
            
            if self.platform == "kubernetes":
                return await self._provision_postgresql_kubernetes(component, config, is_replica)
            else:
                return await self._provision_postgresql_docker(component, config, is_replica)
                
        except Exception as e:
            logger.error(f"‚ùå PostgreSQL provisioning failed: {e}")
            return False
    
    async def _provision_postgresql_kubernetes(self, component: InfrastructureComponent, 
                                            config: PostgreSQLConfig, is_replica: bool) -> bool:
        """Provision PostgreSQL on Kubernetes"""
        try:
            # Create PostgreSQL secret
            secret_yaml = f"""
apiVersion: v1
kind: Secret
metadata:
  name: {component.name}-secret
  namespace: {self.namespace}
type: Opaque
data:
  username: {self._base64_encode(config.username)}
  password: {self._base64_encode(config.password)}
  database: {self._base64_encode(config.database)}
"""
            
            await self._kubectl_apply(secret_yaml)
            
            # Create ConfigMap for PostgreSQL configuration
            postgres_conf = f"""
# PostgreSQL configuration optimized for XORB
max_connections = {config.max_connections}
shared_buffers = {config.shared_buffers}
work_mem = {config.work_mem}
maintenance_work_mem = {config.maintenance_work_mem}
effective_cache_size = {config.effective_cache_size}

# Performance tuning
checkpoint_completion_target = {config.performance_tuning.get('checkpoint_completion_target', '0.9')}
wal_buffers = {config.performance_tuning.get('wal_buffers', '16MB')}
default_statistics_target = {config.performance_tuning.get('default_statistics_target', '100')}
random_page_cost = {config.performance_tuning.get('random_page_cost', '1.1')}
effective_io_concurrency = {config.performance_tuning.get('effective_io_concurrency', '200')}

# Logging
log_destination = 'stderr'
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
log_checkpoints = on
log_connections = on
log_disconnections = on
log_lock_waits = on
log_temp_files = 0

# Replication settings
{"" if not is_replica else "hot_standby = on"}
{"wal_level = replica" if not is_replica else ""}
{"max_wal_senders = 3" if not is_replica else ""}
{"wal_keep_size = 1GB" if not is_replica else ""}
"""
            
            configmap_yaml = f"""
apiVersion: v1
kind: ConfigMap
metadata:
  name: {component.name}-config
  namespace: {self.namespace}
data:
  postgresql.conf: |
{postgres_conf}
  pg_hba.conf: |
    # PostgreSQL Client Authentication Configuration
    local all all trust
    host all all 127.0.0.1/32 trust
    host all all ::1/128 trust
    host replication all 0.0.0.0/0 md5
    host all all 0.0.0.0/0 md5
"""
            
            await self._kubectl_apply(configmap_yaml)
            
            # Create StatefulSet
            statefulset_yaml = f"""
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {component.name}
  namespace: {self.namespace}
  labels:
    app: {component.name}
    component: database
    database: postgresql
spec:
  serviceName: {component.name}
  replicas: 1
  selector:
    matchLabels:
      app: {component.name}
  template:
    metadata:
      labels:
        app: {component.name}
        component: database
        database: postgresql
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9187"
    spec:
      initContainers:
      - name: postgres-init
        image: postgres:{config.version}
        command:
        - sh
        - -c
        - |
          echo "Initializing PostgreSQL..."
          if [ ! -f /var/lib/postgresql/data/postgresql.conf ]; then
            echo "Setting up initial configuration"
            mkdir -p /var/lib/postgresql/data
            chown -R postgres:postgres /var/lib/postgresql/data
          fi
        volumeMounts:
        - name: postgres-data
          mountPath: /var/lib/postgresql/data
        securityContext:
          runAsUser: 0
      containers:
      - name: postgresql
        image: postgres:{config.version}
        ports:
        - containerPort: 5432
          name: postgres
        env:
        - name: POSTGRES_DB
          valueFrom:
            secretKeyRef:
              name: {component.name}-secret
              key: database
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: {component.name}-secret
              key: username
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: {component.name}-secret
              key: password
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        volumeMounts:
        - name: postgres-data
          mountPath: /var/lib/postgresql/data
        - name: postgres-config
          mountPath: /etc/postgresql/postgresql.conf
          subPath: postgresql.conf
        - name: postgres-config
          mountPath: /etc/postgresql/pg_hba.conf
          subPath: pg_hba.conf
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          exec:
            command:
            - pg_isready
            - -U
            - {config.username}
            - -d
            - {config.database}
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          exec:
            command:
            - pg_isready
            - -U
            - {config.username}
            - -d
            - {config.database}
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        securityContext:
          runAsUser: 999
          runAsGroup: 999
          allowPrivilegeEscalation: false
      - name: postgres-exporter
        image: prometheuscommunity/postgres-exporter:v0.12.0
        ports:
        - containerPort: 9187
          name: metrics
        env:
        - name: DATA_SOURCE_NAME
          value: "postgresql://{config.username}:{config.password}@localhost:5432/{config.database}?sslmode=disable"
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
      volumes:
      - name: postgres-config
        configMap:
          name: {component.name}-config
      securityContext:
        fsGroup: 999
  volumeClaimTemplates:
  - metadata:
      name: postgres-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: {config.storage_size}
---
apiVersion: v1
kind: Service
metadata:
  name: {component.name}
  namespace: {self.namespace}
  labels:
    app: {component.name}
    component: database
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9187"
spec:
  selector:
    app: {component.name}
  ports:
  - name: postgres
    port: 5432
    targetPort: 5432
    protocol: TCP
  - name: metrics
    port: 9187
    targetPort: 9187
    protocol: TCP
  type: ClusterIP
"""
            
            success = await self._kubectl_apply(statefulset_yaml)
            
            if success:
                # Set endpoints
                component.endpoints = {
                    "primary": f"{component.name}.{self.namespace}.svc.cluster.local:5432",
                    "metrics": f"{component.name}.{self.namespace}.svc.cluster.local:9187"
                }
            
            return success
            
        except Exception as e:
            logger.error(f"‚ùå PostgreSQL Kubernetes provisioning failed: {e}")
            return False
    
    async def _provision_postgresql_docker(self, component: InfrastructureComponent, 
                                         config: PostgreSQLConfig, is_replica: bool) -> bool:
        """Provision PostgreSQL using Docker Compose"""
        try:
            # This would integrate with existing Docker Compose files
            compose_service = f"""
{component.name}:
  image: postgres:{config.version}
  container_name: xorb-{component.name}
  environment:
    POSTGRES_DB: {config.database}
    POSTGRES_USER: {config.username}
    POSTGRES_PASSWORD: {config.password}
    PGDATA: /var/lib/postgresql/data/pgdata
  ports:
    - "{config.port}:5432"
  volumes:
    - {component.name}_data:/var/lib/postgresql/data
    - ./config/postgresql.conf:/etc/postgresql/postgresql.conf
  command: postgres -c config_file=/etc/postgresql/postgresql.conf
  networks:
    - xorb-backend
  restart: unless-stopped
  healthcheck:
    test: ["CMD-SHELL", "pg_isready -U {config.username} -d {config.database}"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 60s
"""
            
            # In a real implementation, this would update the docker-compose.yml file
            # For now, we'll simulate success
            component.endpoints = {
                "primary": f"localhost:{config.port}",
                "connection_string": f"postgresql://{config.username}:{config.password}@localhost:{config.port}/{config.database}"
            }
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå PostgreSQL Docker provisioning failed: {e}")
            return False
    
    async def _provision_redis(self, component: InfrastructureComponent) -> bool:
        """Provision Redis cluster"""
        try:
            config = component.config
            
            if self.platform == "kubernetes":
                return await self._provision_redis_kubernetes(component, config)
            else:
                return await self._provision_redis_docker(component, config)
                
        except Exception as e:
            logger.error(f"‚ùå Redis provisioning failed: {e}")
            return False
    
    async def _provision_redis_kubernetes(self, component: InfrastructureComponent, config: RedisConfig) -> bool:
        """Provision Redis on Kubernetes"""
        try:
            # Redis ConfigMap
            redis_conf = f"""
# Redis configuration for XORB
port {config.port}
tcp-backlog 511
timeout 0
tcp-keepalive 300

# Memory management
maxmemory {config.max_memory}
maxmemory-policy {config.max_memory_policy}

# Persistence
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes

# AOF
appendonly {"yes" if config.appendonly else "no"}
appendfsync {config.appendfsync}
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# Logging
loglevel notice
syslog-enabled no

# Security
requirepass redis_secure_password_123!

# Networking
bind 0.0.0.0
protected-mode yes
"""
            
            configmap_yaml = f"""
apiVersion: v1
kind: ConfigMap
metadata:
  name: {component.name}-config
  namespace: {self.namespace}
data:
  redis.conf: |
{redis_conf}
"""
            
            await self._kubectl_apply(configmap_yaml)
            
            # Redis StatefulSet
            statefulset_yaml = f"""
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {component.name}
  namespace: {self.namespace}
  labels:
    app: {component.name}
    component: cache
    database: redis
spec:
  serviceName: {component.name}
  replicas: {"6" if config.enable_cluster else "1"}
  selector:
    matchLabels:
      app: {component.name}
  template:
    metadata:
      labels:
        app: {component.name}
        component: cache
        database: redis
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9121"
    spec:
      containers:
      - name: redis
        image: redis:{config.version}
        ports:
        - containerPort: 6379
          name: redis
        command:
        - redis-server
        - /etc/redis/redis.conf
        volumeMounts:
        - name: redis-data
          mountPath: /data
        - name: redis-config
          mountPath: /etc/redis/redis.conf
          subPath: redis.conf
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          exec:
            command:
            - redis-cli
            - ping
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          exec:
            command:
            - redis-cli
            - ping
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
      - name: redis-exporter
        image: oliver006/redis_exporter:v1.45.0
        ports:
        - containerPort: 9121
          name: metrics
        env:
        - name: REDIS_ADDR
          value: "redis://localhost:6379"
        - name: REDIS_PASSWORD
          value: "redis_secure_password_123!"
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
      volumes:
      - name: redis-config
        configMap:
          name: {component.name}-config
  volumeClaimTemplates:
  - metadata:
      name: redis-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: {config.storage_size}
---
apiVersion: v1
kind: Service
metadata:
  name: {component.name}
  namespace: {self.namespace}
  labels:
    app: {component.name}
    component: cache
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9121"
spec:
  selector:
    app: {component.name}
  ports:
  - name: redis
    port: 6379
    targetPort: 6379
    protocol: TCP
  - name: metrics
    port: 9121
    targetPort: 9121
    protocol: TCP
  type: ClusterIP
"""
            
            success = await self._kubectl_apply(statefulset_yaml)
            
            if success:
                component.endpoints = {
                    "primary": f"{component.name}.{self.namespace}.svc.cluster.local:6379",
                    "metrics": f"{component.name}.{self.namespace}.svc.cluster.local:9121"
                }
            
            return success
            
        except Exception as e:
            logger.error(f"‚ùå Redis Kubernetes provisioning failed: {e}")
            return False
    
    async def _provision_redis_docker(self, component: InfrastructureComponent, config: RedisConfig) -> bool:
        """Provision Redis using Docker"""
        try:
            component.endpoints = {
                "primary": f"localhost:{config.port}",
                "connection_string": f"redis://localhost:{config.port}"
            }
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Redis Docker provisioning failed: {e}")
            return False
    
    async def _provision_neo4j(self, component: InfrastructureComponent) -> bool:
        """Provision Neo4j graph database"""
        try:
            config = component.config
            
            if self.platform == "kubernetes":
                return await self._provision_neo4j_kubernetes(component, config)
            else:
                return await self._provision_neo4j_docker(component, config)
                
        except Exception as e:
            logger.error(f"‚ùå Neo4j provisioning failed: {e}")
            return False
    
    async def _provision_neo4j_kubernetes(self, component: InfrastructureComponent, config: Neo4jConfig) -> bool:
        """Provision Neo4j on Kubernetes"""
        try:
            # Neo4j StatefulSet
            statefulset_yaml = f"""
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {component.name}
  namespace: {self.namespace}
  labels:
    app: {component.name}
    component: database
    database: neo4j
spec:
  serviceName: {component.name}
  replicas: 1
  selector:
    matchLabels:
      app: {component.name}
  template:
    metadata:
      labels:
        app: {component.name}
        component: database
        database: neo4j
    spec:
      containers:
      - name: neo4j
        image: neo4j:{config.version}
        ports:
        - containerPort: 7474
          name: http
        - containerPort: 7687
          name: bolt
        env:
        - name: NEO4J_AUTH
          value: "{config.username}/{config.password}"
        - name: NEO4J_dbms_memory_heap_initial__size
          value: "{config.heap_size}"
        - name: NEO4J_dbms_memory_heap_max__size
          value: "{config.heap_size}"
        - name: NEO4J_dbms_memory_pagecache_size
          value: "{config.page_cache}"
        - name: NEO4J_dbms_default__listen__address
          value: "0.0.0.0"
        - name: NEO4J_dbms_connector_bolt_listen__address
          value: "0.0.0.0:7687"
        - name: NEO4J_dbms_connector_http_listen__address
          value: "0.0.0.0:7474"
        volumeMounts:
        - name: neo4j-data
          mountPath: /data
        - name: neo4j-logs
          mountPath: /logs
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /
            port: 7474
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /
            port: 7474
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
      securityContext:
        fsGroup: 7474
  volumeClaimTemplates:
  - metadata:
      name: neo4j-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: {config.storage_size}
  - metadata:
      name: neo4j-logs
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: {component.name}
  namespace: {self.namespace}
  labels:
    app: {component.name}
    component: database
spec:
  selector:
    app: {component.name}
  ports:
  - name: http
    port: 7474
    targetPort: 7474
    protocol: TCP
  - name: bolt
    port: 7687
    targetPort: 7687
    protocol: TCP
  type: ClusterIP
"""
            
            success = await self._kubectl_apply(statefulset_yaml)
            
            if success:
                component.endpoints = {
                    "http": f"{component.name}.{self.namespace}.svc.cluster.local:7474",
                    "bolt": f"{component.name}.{self.namespace}.svc.cluster.local:7687"
                }
            
            return success
            
        except Exception as e:
            logger.error(f"‚ùå Neo4j Kubernetes provisioning failed: {e}")
            return False
    
    async def _provision_neo4j_docker(self, component: InfrastructureComponent, config: Neo4jConfig) -> bool:
        """Provision Neo4j using Docker"""
        try:
            component.endpoints = {
                "http": f"localhost:{config.http_port}",
                "bolt": f"localhost:{config.port}"
            }
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Neo4j Docker provisioning failed: {e}")
            return False
    
    async def _provision_elasticsearch(self, component: InfrastructureComponent) -> bool:
        """Provision Elasticsearch cluster"""
        try:
            config = component.config
            
            if self.platform == "kubernetes":
                return await self._provision_elasticsearch_kubernetes(component, config)
            else:
                return await self._provision_elasticsearch_docker(component, config)
                
        except Exception as e:
            logger.error(f"‚ùå Elasticsearch provisioning failed: {e}")
            return False
    
    async def _provision_elasticsearch_kubernetes(self, component: InfrastructureComponent, 
                                               config: ElasticsearchConfig) -> bool:
        """Provision Elasticsearch on Kubernetes"""
        try:
            # Elasticsearch StatefulSet
            statefulset_yaml = f"""
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {component.name}
  namespace: {self.namespace}
  labels:
    app: {component.name}
    component: search
    database: elasticsearch
spec:
  serviceName: {component.name}
  replicas: {config.replica_count}
  selector:
    matchLabels:
      app: {component.name}
  template:
    metadata:
      labels:
        app: {component.name}
        component: search
        database: elasticsearch
    spec:
      initContainers:
      - name: increase-vm-max-map
        image: busybox:1.35
        command:
        - sysctl
        - -w
        - vm.max_map_count=262144
        securityContext:
          privileged: true
      containers:
      - name: elasticsearch
        image: elasticsearch:{config.version}
        ports:
        - containerPort: 9200
          name: http
        - containerPort: 9300
          name: transport
        env:
        - name: cluster.name
          value: "{config.cluster_name}"
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: discovery.type
          value: "single-node"
        - name: ES_JAVA_OPTS
          value: "-Xms{config.heap_size} -Xmx{config.heap_size}"
        - name: xpack.security.enabled
          value: "{"true" if config.enable_security else "false"}"
        - name: xpack.security.transport.ssl.enabled
          value: "{"true" if config.enable_ssl else "false"}"
        - name: xpack.security.http.ssl.enabled
          value: "{"true" if config.enable_ssl else "false"}"
        volumeMounts:
        - name: elasticsearch-data
          mountPath: /usr/share/elasticsearch/data
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /_cluster/health
            port: 9200
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /_cluster/health?wait_for_status=yellow&timeout=5s
            port: 9200
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        securityContext:
          runAsUser: 1000
          runAsGroup: 1000
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: {config.storage_size}
---
apiVersion: v1
kind: Service
metadata:
  name: {component.name}
  namespace: {self.namespace}
  labels:
    app: {component.name}
    component: search
spec:
  selector:
    app: {component.name}
  ports:
  - name: http
    port: 9200
    targetPort: 9200
    protocol: TCP
  - name: transport
    port: 9300
    targetPort: 9300
    protocol: TCP
  type: ClusterIP
"""
            
            success = await self._kubectl_apply(statefulset_yaml)
            
            if success:
                component.endpoints = {
                    "http": f"{component.name}.{self.namespace}.svc.cluster.local:9200",
                    "transport": f"{component.name}.{self.namespace}.svc.cluster.local:9300"
                }
            
            return success
            
        except Exception as e:
            logger.error(f"‚ùå Elasticsearch Kubernetes provisioning failed: {e}")
            return False
    
    async def _provision_elasticsearch_docker(self, component: InfrastructureComponent, 
                                            config: ElasticsearchConfig) -> bool:
        """Provision Elasticsearch using Docker"""
        try:
            component.endpoints = {
                "http": f"localhost:{config.port}"
            }
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Elasticsearch Docker provisioning failed: {e}")
            return False
    
    async def _wait_for_component_health(self, component: InfrastructureComponent, timeout: int = 300) -> bool:
        """Wait for component to become healthy"""
        try:
            logger.info(f"‚è≥ Waiting for {component.name} to become healthy...")
            
            start_time = time.time()
            
            while time.time() - start_time < timeout:
                health_status = await self._check_component_health(component)
                
                if health_status:
                    component.health_status = True
                    logger.info(f"‚úÖ {component.name} is healthy")
                    return True
                
                logger.info(f"‚è≥ {component.name} not ready yet, waiting...")
                await asyncio.sleep(10)
            
            logger.error(f"‚ùå Timeout waiting for {component.name} to become healthy")
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Health check failed for {component.name}: {e}")
            return False
    
    async def _check_component_health(self, component: InfrastructureComponent) -> bool:
        """Check component health status"""
        try:
            if self.platform == "kubernetes":
                # Check pod status
                result = subprocess.run([
                    "kubectl", "get", "pods",
                    "-l", f"app={component.name}",
                    "-n", self.namespace,
                    "--field-selector=status.phase=Running",
                    "--no-headers"
                ], capture_output=True, text=True, timeout=30)
                
                if result.returncode == 0 and result.stdout.strip():
                    # At least one pod is running
                    return True
            
            # Add Docker health check logic here for Docker platform
            
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Component health check failed: {e}")
            return False
    
    async def _setup_infrastructure_monitoring(self):
        """Setup monitoring for infrastructure components"""
        logger.info("üìä Setting up infrastructure monitoring")
        
        # This would setup Prometheus monitoring for all infrastructure components
        # ServiceMonitor resources would be created for each component
        
        for component_name, component in self.components.items():
            if component.status == InfrastructureStatus.RUNNING:
                await self._setup_component_monitoring(component)
    
    async def _setup_component_monitoring(self, component: InfrastructureComponent):
        """Setup monitoring for individual component"""
        try:
            if self.platform == "kubernetes":
                # Create ServiceMonitor for Prometheus scraping
                service_monitor_yaml = f"""
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: {component.name}-monitor
  namespace: {self.namespace}
  labels:
    app: {component.name}
    component: monitoring
spec:
  selector:
    matchLabels:
      app: {component.name}
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
"""
                
                await self._kubectl_apply(service_monitor_yaml)
                logger.info(f"üìä Monitoring setup completed for {component.name}")
            
        except Exception as e:
            logger.error(f"‚ùå Monitoring setup failed for {component.name}: {e}")
    
    async def _setup_automated_backups(self):
        """Setup automated backup jobs"""
        logger.info("üíæ Setting up automated backups")
        
        for component_name, component in self.components.items():
            if component.status == InfrastructureStatus.RUNNING:
                await self._setup_component_backup(component)
    
    async def _setup_component_backup(self, component: InfrastructureComponent):
        """Setup backup for individual component"""
        try:
            if self.platform == "kubernetes":
                backup_schedule = "0 2 * * *"  # Default schedule
                
                if component.type == DatabaseType.POSTGRESQL:
                    backup_schedule = component.config.backup_schedule
                elif component.type == DatabaseType.REDIS:
                    backup_schedule = component.config.backup_schedule
                elif component.type == DatabaseType.NEO4J:
                    backup_schedule = component.config.backup_schedule
                
                # Create CronJob for backup
                cronjob_yaml = f"""
apiVersion: batch/v1
kind: CronJob
metadata:
  name: {component.name}-backup
  namespace: {self.namespace}
  labels:
    app: {component.name}
    component: backup
spec:
  schedule: "{backup_schedule}"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: {self._get_backup_image(component.type)}
            command: {self._get_backup_command(component)}
            env:
            - name: BACKUP_TARGET
              value: "{component.name}"
            - name: BACKUP_TYPE
              value: "{component.type.value}"
            volumeMounts:
            - name: backup-storage
              mountPath: /backups
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-storage-pvc
          restartPolicy: OnFailure
"""
                
                await self._kubectl_apply(cronjob_yaml)
                logger.info(f"üíæ Backup setup completed for {component.name}")
            
        except Exception as e:
            logger.error(f"‚ùå Backup setup failed for {component.name}: {e}")
    
    def _get_backup_image(self, db_type: DatabaseType) -> str:
        """Get appropriate backup image for database type"""
        images = {
            DatabaseType.POSTGRESQL: "postgres:15",
            DatabaseType.REDIS: "redis:7",
            DatabaseType.NEO4J: "neo4j:5",
            DatabaseType.ELASTICSEARCH: "elasticsearch:8.9.0"
        }
        return images.get(db_type, "busybox:1.35")
    
    def _get_backup_command(self, component: InfrastructureComponent) -> List[str]:
        """Get backup command for component"""
        if component.type == DatabaseType.POSTGRESQL:
            config = component.config
            return [
                "sh", "-c",
                f"pg_dump -h {component.name} -U {config.username} -d {config.database} > /backups/{component.name}_$(date +%Y%m%d_%H%M%S).sql"
            ]
        elif component.type == DatabaseType.REDIS:
            return [
                "sh", "-c",
                f"redis-cli -h {component.name} --rdb /backups/{component.name}_$(date +%Y%m%d_%H%M%S).rdb"
            ]
        elif component.type == DatabaseType.NEO4J:
            return [
                "sh", "-c",
                f"neo4j-admin backup --from={component.name}:6362 --backup-dir=/backups --name={component.name}_$(date +%Y%m%d_%H%M%S)"
            ]
        else:
            return ["echo", "Backup not implemented for this type"]
    
    async def _kubectl_apply(self, yaml_content: str) -> bool:
        """Apply Kubernetes YAML configuration"""
        try:
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yml', delete=False) as f:
                f.write(yaml_content)
                temp_file = f.name
            
            cmd = ["kubectl", "apply", "-f", temp_file]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            
            os.unlink(temp_file)
            
            if result.returncode == 0:
                return True
            else:
                logger.error(f"‚ùå kubectl apply failed: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå kubectl apply failed: {e}")
            return False
    
    def _base64_encode(self, data: str) -> str:
        """Base64 encode string"""
        import base64
        return base64.b64encode(data.encode()).decode()
    
    def get_infrastructure_status(self) -> Dict[str, Any]:
        """Get comprehensive infrastructure status"""
        status = {
            "deployment_id": self.deployment_id,
            "namespace": self.namespace,
            "platform": self.platform,
            "timestamp": datetime.now().isoformat(),
            "components": {},
            "summary": {
                "total": len(self.components),
                "running": 0,
                "failed": 0,
                "provisioning": 0
            }
        }
        
        for name, component in self.components.items():
            status["components"][name] = {
                "type": component.type.value,
                "status": component.status.value,
                "health_status": component.health_status,
                "endpoints": component.endpoints,
                "start_time": component.start_time.isoformat() if component.start_time else None,
                "end_time": component.end_time.isoformat() if component.end_time else None,
                "error_message": component.error_message
            }
            
            if component.status == InfrastructureStatus.RUNNING:
                status["summary"]["running"] += 1
            elif component.status == InfrastructureStatus.FAILED:
                status["summary"]["failed"] += 1
            elif component.status == InfrastructureStatus.PROVISIONING:
                status["summary"]["provisioning"] += 1
        
        return status
    
    async def scale_component(self, component_name: str, replicas: int) -> bool:
        """Scale infrastructure component"""
        try:
            if component_name not in self.components:
                logger.error(f"‚ùå Component {component_name} not found")
                return False
            
            component = self.components[component_name]
            
            if self.platform == "kubernetes":
                result = subprocess.run([
                    "kubectl", "scale", "statefulset", component_name,
                    f"--replicas={replicas}",
                    f"--namespace={self.namespace}"
                ], capture_output=True, text=True, timeout=60)
                
                if result.returncode == 0:
                    logger.info(f"‚úÖ Scaled {component_name} to {replicas} replicas")
                    return True
                else:
                    logger.error(f"‚ùå Failed to scale {component_name}: {result.stderr}")
                    return False
            
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Component scaling failed: {e}")
            return False
    
    async def backup_component(self, component_name: str) -> bool:
        """Trigger manual backup for component"""
        try:
            if component_name not in self.components:
                logger.error(f"‚ùå Component {component_name} not found")
                return False
            
            component = self.components[component_name]
            
            if self.platform == "kubernetes":
                # Create a one-time backup job
                job_yaml = f"""
apiVersion: batch/v1
kind: Job
metadata:
  name: {component_name}-manual-backup-{int(time.time())}
  namespace: {self.namespace}
spec:
  template:
    spec:
      containers:
      - name: backup
        image: {self._get_backup_image(component.type)}
        command: {self._get_backup_command(component)}
        env:
        - name: BACKUP_TARGET
          value: "{component_name}"
        - name: BACKUP_TYPE
          value: "{component.type.value}"
        volumeMounts:
        - name: backup-storage
          mountPath: /backups
      volumes:
      - name: backup-storage
        persistentVolumeClaim:
          claimName: backup-storage-pvc
      restartPolicy: Never
"""
                
                success = await self._kubectl_apply(job_yaml)
                
                if success:
                    logger.info(f"‚úÖ Manual backup triggered for {component_name}")
                    return True
                else:
                    logger.error(f"‚ùå Failed to trigger backup for {component_name}")
                    return False
            
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Component backup failed: {e}")
            return False

async def main():
    """Main function for testing infrastructure automation"""
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Initialize infrastructure automation
    infra = XORBInfrastructureAutomation(
        namespace="xorb-infra",
        platform="kubernetes"
    )
    
    # Provision infrastructure
    success = await infra.provision_infrastructure()
    
    if success:
        print("üéâ Infrastructure provisioning completed successfully!")
        
        # Get status
        status = infra.get_infrastructure_status()
        print(f"üìä Infrastructure Status:")
        print(f"  Total Components: {status['summary']['total']}")
        print(f"  Running: {status['summary']['running']}")
        print(f"  Failed: {status['summary']['failed']}")
        
        # Print endpoints
        print(f"\nüîó Service Endpoints:")
        for name, component in status['components'].items():
            if component['endpoints']:
                print(f"  {name}:")
                for endpoint_type, endpoint_url in component['endpoints'].items():
                    print(f"    {endpoint_type}: {endpoint_url}")
    else:
        print("‚ùå Infrastructure provisioning failed!")
    
    return success

if __name__ == "__main__":
    result = asyncio.run(main())
    exit(0 if result else 1)